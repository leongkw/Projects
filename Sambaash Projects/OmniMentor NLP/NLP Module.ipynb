{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import fasttext\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nlp_module():\n",
    "    \n",
    "    def stopwords():\n",
    "        stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
    "                     'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
    "                     \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
    "                     'he', 'him', 'his', 'himself', 'she', \"she's\", 'her',\n",
    "                     'hers', 'herself', 'it', \"it's\", 'its', 'itself',\n",
    "                     'they', 'them', 'their', 'theirs', 'themselves', 'what',\n",
    "                     'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these',\n",
    "                     'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been',\n",
    "                     'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
    "                     'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if',\n",
    "                     'or', 'because', 'as', 'until', 'while', 'of', 'at',\n",
    "                     'by', 'for', 'with', 'about', 'against', 'between',\n",
    "                     'into', 'through', 'during', 'before', 'after', 'above',\n",
    "                     'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on',\n",
    "                     'off', 'over', 'under', 'again', 'further', 'then', 'once',\n",
    "                     'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any',\n",
    "                     'both', 'each', 'few', 'more', 'most', 'other', 'some',\n",
    "                     'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
    "                     'than', 'too', 'very', 's', 't', 'can', 'will', 'just',\n",
    "                     'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll',\n",
    "                     'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn',\n",
    "                     \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\n",
    "                     \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\",\n",
    "                     'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\",\n",
    "                     'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren',\n",
    "                     \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'fuck', 'fucker',\n",
    "                     'lah', 'la', 'leh', 'lor', 'nah', 'ya', 'yah', 'shit', 'ass', 'asshole',\n",
    "                     'le', 'already', 'liao', 'liaoz', 'u', 'cheebye', 'lanjiao',\n",
    "                     'nabei', 'kaopei', 'knnb', 'cb', 'cheebye', 'fucked', 'fucks',\n",
    "                     'bitch', 'bitches', 'scumbag', 'fuckface', 'wtf', 'ffs', 'siao',\n",
    "                     'walao', 'waliao', 'ttyl', 'orhhhh', 'sai']\n",
    "        \n",
    "        return stopwords\n",
    "    \n",
    "    def train_model(input):\n",
    "        model = fasttext.train_supervised(input)\n",
    "        model.save_model(\"ft_model.bin\")\n",
    "    \n",
    "    def TfidfVec(stopwords):\n",
    "    # Initializing tfidf vectorizer model\n",
    "        TfidfVec = TfidfVectorizer(tokenizer=nlp_module.LemNormalize, \n",
    "                                   stop_words=stopwords, \n",
    "                                   ngram_range=(1,2))\n",
    "        \n",
    "        return TfidfVec\n",
    "    \n",
    "    def LemTokens(tokens):\n",
    "        lemmer = nltk.stem.WordNetLemmatizer()\n",
    "        \n",
    "        return [lemmer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    def LemNormalize(text):\n",
    "        remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "        \n",
    "        return nlp_module.LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
    "    \n",
    "    # Function to process stopwords\n",
    "    def stopwords_process(stopwords):\n",
    "        temp_string = ''\n",
    "        for word in stopwords:\n",
    "            temp_string += word\n",
    "            temp_string += ' '\n",
    "        \n",
    "        output = nlp_module.LemNormalize(temp_string)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    # Load fasttext model\n",
    "    def load_model(model):\n",
    "        ft_model = fasttext.load_model(model)\n",
    "        \n",
    "        return ft_model\n",
    "    \n",
    "    def fasttext_output(user_input, model):\n",
    "        return model.predict(user_input, k=1)[0][0]\n",
    "       \n",
    "    def load_response_file(csv_file):\n",
    "        output = pd.read_csv(csv_file)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    # Function to create a prompts corpus based on the classification generated by the fasttext model\\\n",
    "    # Takes in user's input and labelled prompts/response database\n",
    "    def create_prompts_corpus(input_text, csv_file, model):\n",
    "        # Use the predicted label by fasttext model to filter prompts/response database\n",
    "        matched_label = model.predict(input_text, k=1)[0][0]\n",
    "        mask = csv_file.label == matched_label\n",
    "\n",
    "        # Intializing empty string for runtime generated prompts corpus\n",
    "        corp = ''\n",
    "\n",
    "        # Iterates through the filtered prompts database and adds to corpus string\n",
    "        # End result will be string containing only prompts that fit the filtered category\n",
    "        for prompt in csv_file[mask]['prompt'].values:\n",
    "            corp += prompt\n",
    "            corp = corp.replace('.', '. ').replace('?', '? ').replace('!', '! ')\n",
    "        return corp\n",
    "    \n",
    "    # Function to generate the top matched response given user input\n",
    "    def get_response(input_text, csv_file, TfidfVec, model):\n",
    "        user_input = input_text.lower()\n",
    "        response = ''\n",
    "        \n",
    "        response_db = nlp_module.load_response_file(csv_file)\n",
    "        prompts_corpus = nlp_module.create_prompts_corpus(input_text, response_db, model)\n",
    "        \n",
    "      \n",
    "        # Tokenizing the prompts corpus into sentence tokens\n",
    "        # Adding the user's input into the list of sentence tokens,\n",
    "        prompts_sent_token = nltk.sent_tokenize(prompts_corpus)\n",
    "        prompts_sent_token.append(user_input)\n",
    "      \n",
    "        # Fitting the tfidf model with the sentence tokens\n",
    "        # Calculating the cosine similarities between user input and corpus \n",
    "        tfidf = TfidfVec.fit_transform(prompts_sent_token)\n",
    "        cosine_vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "\n",
    "        flat = cosine_vals.flatten()\n",
    "\n",
    "        score = 0\n",
    "        for i in range(0, len(flat)-1, 1):\n",
    "            if flat[i] >= score:\n",
    "                score = flat[i]\n",
    "                matched_score = score\n",
    "        \n",
    "        matching_list = list(flat)\n",
    "        index = matching_list.index(matched_score)\n",
    "       \n",
    "        if matched_score == 0:\n",
    "            response = response + \"I'm sorry, I do not understand you\"\n",
    "        \n",
    "        else:\n",
    "            try:\n",
    "                response = response + response_db[response_db['prompt'] == prompts_sent_token[index]]['response'].values[0]\n",
    "                prompts_sent_token.remove(user_input)\n",
    "            except:\n",
    "                response = response + response_db[(response_db['prompt'].str.contains(prompts_sent_token[index]))]\n",
    "                prompts_sent_token.remove(user_input)\n",
    "       \n",
    "        return str(response)\n",
    "    \n",
    "    # Function to return fasttext matched label and matched score\n",
    "    # Format: [('label', score)]\n",
    "    def predict_label(input_text, model):\n",
    "        output = []\n",
    "        user_input = input_text.lower()        \n",
    "        matched_label = model.predict(input_text, k=1)[0][0]\n",
    "        matched_score = model.predict(input_text, k=1)[1][0]\n",
    "        output.append((matched_label, matched_score))\n",
    "        \n",
    "        return output\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
