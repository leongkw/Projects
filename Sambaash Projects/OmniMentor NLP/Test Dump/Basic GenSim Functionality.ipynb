{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from pprint import pprint\n",
    "from gensim import models\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\"The Saudis are preparing a report that will acknowledge that\", \n",
    "             \"Saudi journalist Jamal Khashoggi's death was the result of an\", \n",
    "             \"interrogation that went wrong, one that was intended to lead\", \n",
    "             \"to his abduction from Turkey, according to two sources.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits up the different sentences into individual words\n",
    "# Words in individual sentences will still be enclosed within a list\n",
    "# Meaning ['word1', 'word2'], ['word1', 'word2']\n",
    "texts = [[word for word in sentence.split()] for sentence in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(33 unique tokens: ['Saudis', 'The', 'a', 'acknowledge', 'are']...)\n"
     ]
    }
   ],
   "source": [
    "# Creates a GenSim dictionary\n",
    "# A dictionary is a collection of words (or bag of words)\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "# Shows how many unique words there are in the dictionary\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Saudis': 0, 'The': 1, 'a': 2, 'acknowledge': 3, 'are': 4, 'preparing': 5, 'report': 6, 'that': 7, 'will': 8, 'Jamal': 9, \"Khashoggi's\": 10, 'Saudi': 11, 'an': 12, 'death': 13, 'journalist': 14, 'of': 15, 'result': 16, 'the': 17, 'was': 18, 'intended': 19, 'interrogation': 20, 'lead': 21, 'one': 22, 'to': 23, 'went': 24, 'wrong,': 25, 'Turkey,': 26, 'abduction': 27, 'according': 28, 'from': 29, 'his': 30, 'sources.': 31, 'two': 32}\n"
     ]
    }
   ],
   "source": [
    "# In a GenSim dictionary, individual words are assigned unique numerical ids\n",
    "# .token2id returns a dictionary of the unique ids attached to each word\n",
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a new 'document' into the dictionary\n",
    "documents2 = [\"The intersection graph of paths in trees\",\n",
    "               \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "               \"Graph minors A survey\"]\n",
    "texts2 = [[word for word in sentence.split()] for sentence in documents2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The', 'intersection', 'graph', 'of', 'paths', 'in', 'trees'],\n",
       " ['Graph',\n",
       "  'minors',\n",
       "  'IV',\n",
       "  'Widths',\n",
       "  'of',\n",
       "  'trees',\n",
       "  'and',\n",
       "  'well',\n",
       "  'quasi',\n",
       "  'ordering'],\n",
       " ['Graph', 'minors', 'A', 'survey']]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(48 unique tokens: ['Saudis', 'The', 'a', 'acknowledge', 'are']...)\n"
     ]
    }
   ],
   "source": [
    "dictionary.add_documents(texts2)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the dictionary into a bag-of-words corpus format\n",
    "\n",
    "# Combining the list of tokenized words together\n",
    "combined_text = texts + texts2\n",
    "\n",
    "# Creating a corpus\n",
    "# Use .doc2bow() to convert tokenized words into a bag-of-words corpus\n",
    "corpus = [dictionary.doc2bow(doc, allow_update=True) for doc in combined_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 2), (8, 1)],\n",
      " [(9, 1),\n",
      "  (10, 1),\n",
      "  (11, 1),\n",
      "  (12, 1),\n",
      "  (13, 1),\n",
      "  (14, 1),\n",
      "  (15, 1),\n",
      "  (16, 1),\n",
      "  (17, 1),\n",
      "  (18, 1)],\n",
      " [(7, 2),\n",
      "  (18, 1),\n",
      "  (19, 1),\n",
      "  (20, 1),\n",
      "  (21, 1),\n",
      "  (22, 1),\n",
      "  (23, 1),\n",
      "  (24, 1),\n",
      "  (25, 1)],\n",
      " [(23, 2), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1)],\n",
      " [(1, 1), (15, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1)],\n",
      " [(15, 1),\n",
      "  (37, 1),\n",
      "  (38, 1),\n",
      "  (39, 1),\n",
      "  (40, 1),\n",
      "  (41, 1),\n",
      "  (42, 1),\n",
      "  (43, 1),\n",
      "  (44, 1),\n",
      "  (45, 1)],\n",
      " [(38, 1), (42, 1), (46, 1), (47, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# Interpreting the corpus\n",
    "# [] Encloses each sentence/document\n",
    "# (n1, n2) n1 is the unique numerical id for the word\n",
    "# n2 is the number of times it appears in the sentence/document\n",
    "pprint(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Saudis', 1),\n",
      "  ('The', 1),\n",
      "  ('a', 1),\n",
      "  ('acknowledge', 1),\n",
      "  ('are', 1),\n",
      "  ('preparing', 1),\n",
      "  ('report', 1),\n",
      "  ('that', 2),\n",
      "  ('will', 1)],\n",
      " [('Jamal', 1),\n",
      "  (\"Khashoggi's\", 1),\n",
      "  ('Saudi', 1),\n",
      "  ('an', 1),\n",
      "  ('death', 1),\n",
      "  ('journalist', 1),\n",
      "  ('of', 1),\n",
      "  ('result', 1),\n",
      "  ('the', 1),\n",
      "  ('was', 1)],\n",
      " [('that', 2),\n",
      "  ('was', 1),\n",
      "  ('intended', 1),\n",
      "  ('interrogation', 1),\n",
      "  ('lead', 1),\n",
      "  ('one', 1),\n",
      "  ('to', 1),\n",
      "  ('went', 1),\n",
      "  ('wrong,', 1)],\n",
      " [('to', 2),\n",
      "  ('Turkey,', 1),\n",
      "  ('abduction', 1),\n",
      "  ('according', 1),\n",
      "  ('from', 1),\n",
      "  ('his', 1),\n",
      "  ('sources.', 1),\n",
      "  ('two', 1)],\n",
      " [('The', 1),\n",
      "  ('of', 1),\n",
      "  ('graph', 1),\n",
      "  ('in', 1),\n",
      "  ('intersection', 1),\n",
      "  ('paths', 1),\n",
      "  ('trees', 1)],\n",
      " [('of', 1),\n",
      "  ('trees', 1),\n",
      "  ('Graph', 1),\n",
      "  ('IV', 1),\n",
      "  ('Widths', 1),\n",
      "  ('and', 1),\n",
      "  ('minors', 1),\n",
      "  ('ordering', 1),\n",
      "  ('quasi', 1),\n",
      "  ('well', 1)],\n",
      " [('Graph', 1), ('minors', 1), ('A', 1), ('survey', 1)]]\n"
     ]
    }
   ],
   "source": [
    "# Displaying \n",
    "count = [[(dictionary[id], count) for id, count in line] for line in corpus]\n",
    "pprint(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the tfidf model\n",
    "# Need to go look up what the hyperparameters mean\n",
    "# Use corpus to train the tfidf model\n",
    "tfidf = models.TfidfModel(corpus, smartirs='ntc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Saudis', 0.33],\n",
      " ['The', 0.22],\n",
      " ['a', 0.33],\n",
      " ['acknowledge', 0.33],\n",
      " ['are', 0.33],\n",
      " ['preparing', 0.33],\n",
      " ['report', 0.33],\n",
      " ['that', 0.44],\n",
      " ['will', 0.33]]\n",
      "[['Jamal', 0.34],\n",
      " [\"Khashoggi's\", 0.34],\n",
      " ['Saudi', 0.34],\n",
      " ['an', 0.34],\n",
      " ['death', 0.34],\n",
      " ['journalist', 0.34],\n",
      " ['of', 0.16],\n",
      " ['result', 0.34],\n",
      " ['the', 0.34],\n",
      " ['was', 0.23]]\n",
      "[['that', 0.45],\n",
      " ['was', 0.23],\n",
      " ['intended', 0.34],\n",
      " ['interrogation', 0.34],\n",
      " ['lead', 0.34],\n",
      " ['one', 0.34],\n",
      " ['to', 0.23],\n",
      " ['went', 0.34],\n",
      " ['wrong,', 0.34]]\n",
      "[['to', 0.45],\n",
      " ['Turkey,', 0.34],\n",
      " ['abduction', 0.34],\n",
      " ['according', 0.34],\n",
      " ['from', 0.34],\n",
      " ['his', 0.34],\n",
      " ['sources.', 0.34],\n",
      " ['two', 0.34]]\n",
      "[['The', 0.29],\n",
      " ['of', 0.21],\n",
      " ['graph', 0.44],\n",
      " ['in', 0.44],\n",
      " ['intersection', 0.44],\n",
      " ['paths', 0.44],\n",
      " ['trees', 0.29]]\n",
      "[['of', 0.17],\n",
      " ['trees', 0.24],\n",
      " ['Graph', 0.24],\n",
      " ['IV', 0.36],\n",
      " ['Widths', 0.36],\n",
      " ['and', 0.36],\n",
      " ['minors', 0.24],\n",
      " ['ordering', 0.36],\n",
      " ['quasi', 0.36],\n",
      " ['well', 0.36]]\n",
      "[['Graph', 0.39], ['minors', 0.39], ['A', 0.59], ['survey', 0.59]]\n"
     ]
    }
   ],
   "source": [
    "# Print out the weights for each word in the tfidf model\n",
    "# The more a word appears, the smaller the value of its weight\n",
    "# If word appears in all sentences/documents, its removed altogether\n",
    "for sentence in tfidf[corpus]:\n",
    "    pprint([[dictionary[id], \n",
    "            np.around(freq, decimals=2)] \n",
    "            for id, freq in sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
