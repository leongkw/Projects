{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import fasttext\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "import pyemd\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "import string\n",
    "from gensim.similarities import WmdSimilarity\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "# Unused code\n",
    "\n",
    "# feed = []\n",
    "# for sentence in processed:\n",
    "#    feed.append([sentence])\n",
    "    \n",
    "# def preprocess(doc):\n",
    "#    for sentence in doc:\n",
    "#        doc = doc.lower()  # Lower the text.\n",
    "#        doc = word_tokenize(doc)  # Split into words.\n",
    "#        doc = [w for w in doc if not w in stopwords]  # Remove stopwords.\n",
    "#        doc = [w for w in doc if w.isalpha()]  # Remove numbers and punctuation.\n",
    "#        return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "qasmall = pd.read_excel('tagging.xlsx')\n",
    "qabig = pd.read_excel('q&adatabaseexpanded.xlsx')\n",
    "\n",
    "qasmall['prompt'] = qasmall['prompt'].map(lambda x: x.lower())\n",
    "qabig['prompt'] = qabig['prompt'].map(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [\n",
    "             'i', 'me', 'my', 'myself', 'we', 'our', 'ours', \n",
    "             'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \n",
    "             \"you'd\", 'your', 'yours', 'yourself', 'yourselves', \n",
    "             'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', \n",
    "             'hers', 'herself', 'it', \"it's\", 'its', 'itself', \n",
    "             'they', 'them', 'their', 'theirs', 'themselves', 'what', \n",
    "             'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', \n",
    "             'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', \n",
    "             'being', 'have', 'has', 'had', 'having', 'do', 'does', \n",
    "             'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', \n",
    "             'or', 'because', 'as', 'until', 'while', 'of', 'at', \n",
    "             'by', 'for', 'with', 'about', 'against', 'between', \n",
    "             'into', 'through', 'during', 'before', 'after', 'above', \n",
    "             'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', \n",
    "             'off', 'over', 'under', 'again', 'further', 'then', 'once', \n",
    "             'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', \n",
    "             'both', 'each', 'few', 'more', 'most', 'other', 'some', \n",
    "             'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', \n",
    "             'than', 'too', 'very', 's', 't', 'can', 'will', 'just', \n",
    "             'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', \n",
    "             'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \n",
    "             \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \n",
    "             \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", \n",
    "             'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", \n",
    "             'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren',\n",
    "             \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'fuck', 'fucker',\n",
    "             'lah', 'la', 'leh', 'lor', 'nah', 'ya', 'yah', 'shit', 'ass', 'asshole',\n",
    "             'le', 'already', 'liao', 'liaoz', 'u', 'cheebye', 'lanjiao',\n",
    "             'nabei', 'kaopei', 'knnb', 'cb', 'cheebye', 'fucked', 'fucks',\n",
    "             'bitch', 'bitches', 'scumbag', 'fuckface', 'wtf', 'ffs', 'siao',\n",
    "             'walao', 'waliao', 'ttyl', 'orhhhh', 'sai'\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = []\n",
    "for i in qabig['prompt'].values:\n",
    "    prompts.append(i)\n",
    "\n",
    "documents = []\n",
    "for i in prompts:\n",
    "    documents.append(i.replace('.', '').replace('?', '').replace('!', '').replace(',', ''))\n",
    "    \n",
    "processed = [' '.join(w for w in doc.split() if w.lower() not in stopwords)\n",
    "         for doc in documents\n",
    "         ]\n",
    "\n",
    "w2v_corpus = processed\n",
    "wmd_corpus = processed\n",
    "documents = documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasttext_output(userinput):\n",
    "    return str(ft_model.predict(userinput, k=1)[0]).replace('(', '').replace(')', '').replace(',', '').replace(\"'\", '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model = fasttext.train_supervised('fasttext_input.txt', \n",
    "                                  epoch=100,\n",
    "                                  lr=0.1,\n",
    "                                  wordNgrams=2,\n",
    "                                  loss='softmax',\n",
    "                                  ws=10\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(w2v_corpus, workers=3, size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_best = 10\n",
    "instance = WmdSimilarity(wmd_corpus, model, num_best=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sent = \"yo bot, how do i make payment?\"\n",
    "# sent = preprocess(sent)\n",
    "query = sent\n",
    "\n",
    "sims = instance[query]  # A query is simply a \"look-up\" in the similarity class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i want to postpone my course fees date'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "yo bot, how do i make payment?\n",
      "sim = 0.991249685035521\n",
      "where do i find a hard copy of ebook\n",
      "sim = 0.9905710267940252\n",
      "do you have a hard copy of the ebook\n",
      "sim = 0.990378174832725\n",
      "can i download or print the ebook\n",
      "sim = 0.9895845633421072\n",
      "how does the attendance system work\n",
      "sim = 0.9895189599364259\n",
      "yes i want to meet the mentor\n",
      "sim = 0.9890769318558675\n",
      "is it possible to download the ebook\n",
      "sim = 0.988984530350853\n",
      "why canâ€™t i see the upload button in the lms\n",
      "sim = 0.9888201231552632\n",
      "i want to pay with skills future credits how can i do it\n",
      "sim = 0.9887989329487767\n",
      "is it mandatory to complete the elearning\n",
      "sim = 0.9887698460515151\n",
      "how does the attendance work for mentoring session\n"
     ]
    }
   ],
   "source": [
    "print('Query:')\n",
    "print(sent)\n",
    "for i in range(num_best):\n",
    "    print('sim =', sims[i][1])\n",
    "    print(documents[sims[i][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, please ask me a question!\n",
      "What is my mentor's phone number?\n",
      "Matched Label:  __label__adminconsultation\n",
      "Matched Label Score:  [0.96773034]\n",
      "what is my mentor's handphone number\n",
      "sim = 0.8234466769506992\n",
      "Can i pay with skills future credit?\n",
      "Matched Label:  __label__adminpayment\n",
      "Matched Label Score:  [0.99706018]\n",
      "what date to submit sfc by\n",
      "sim = 0.7596996877297869\n",
      "How to pay with sfc?\n",
      "Matched Label:  __label__adminpayment\n",
      "Matched Label Score:  [0.96084219]\n",
      "what date to submit sfc by\n",
      "sim = 0.6754750832898203\n"
     ]
    }
   ],
   "source": [
    "flag=True\n",
    "print(\"Hello, please ask me a question!\")\n",
    "while(flag == True):\n",
    "    user_input = input()\n",
    "    user_input = user_input.lower()\n",
    "    bot_response = ''\n",
    "    \n",
    "    matched_label = fasttext_output(user_input)\n",
    "    mask = qabig.label == matched_label\n",
    "    \n",
    "    prompts_corpus = []\n",
    "    for i in qabig[mask]['prompt'].values:\n",
    "        prompts_corpus.append(i)\n",
    "    \n",
    "    documents = []\n",
    "    for j in prompts_corpus:\n",
    "        documents.append(i.replace('.', '').replace('?', '').replace('!', '').replace(',', ''))\n",
    "    \n",
    "    processed = [' '.join(w for w in doc.split() if w.lower() not in stopwords)\n",
    "                 for doc in documents]\n",
    "    \n",
    "    w2v_corpus = processed\n",
    "    wmd_corpus = processed\n",
    "    documents = documents\n",
    "    \n",
    "    model = Word2Vec(w2v_corpus, workers=3, size=100)\n",
    "    instance = WmdSimilarity(wmd_corpus, model, num_best=10)\n",
    "    \n",
    "    sims = instance[user_input]\n",
    "    \n",
    "    print('Matched Label: ', matched_label)\n",
    "    print('Matched Label Score: ', ft_model.predict(user_input, k=1)[1])\n",
    "    print(documents[sims[0][0]])\n",
    "    print('sim =', sims[0][1])    \n",
    " \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
