{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import fasttext\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "\n",
    "import string\n",
    "import time\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "import telepot\n",
    "from telepot.loop import MessageLoop\n",
    "from telepot.delegate import pave_event_space, per_chat_id, create_open\n",
    "\n",
    "\n",
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Lemmatizer function\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "\n",
    "# Function to remove punctuation then convert into word tokens\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
    "\n",
    "\n",
    "# Function to create a prompts corpus based on the classification generated by the fasttext model\\\n",
    "# Takes in user's input and labelled prompts/response database\n",
    "def create_prompts_corpus(input_text, prompts):\n",
    "    # Use the predicted label by fasttext model to filter prompts/response database\n",
    "    matched_label = fasttext_output(input_text)\n",
    "    mask = qadatabase.label == matched_label\n",
    "\n",
    "    # Intializing empty string for runtime generated prompts corpus\n",
    "    corp = ''\n",
    "\n",
    "    # Iterates through the filtered prompts database and adds to corpus string\n",
    "    # End result will be string containing only prompts that fit the filtered category\n",
    "    for prompt in qadatabase[mask]['prompt'].values:\n",
    "        corp += prompt\n",
    "        corp = corp.replace('.', '. ').replace('?', '? ').replace('!', '! ')\n",
    "    return corp\n",
    "\n",
    "# Function to get predicted label by fasttext model\n",
    "def fasttext_output(user_input):\n",
    "    return ft_model.predict(user_input, k=1)[0][0]\n",
    "\n",
    "# Load fasttext model from local drive\n",
    "ft_model = fasttext.load_model('ft_model.bin')\n",
    "\n",
    "# Labelled prompts/response database\n",
    "qadatabase = pd.read_excel('q&adatabaseexpanded.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
    "             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
    "             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
    "             'he', 'him', 'his', 'himself', 'she', \"she's\", 'her',\n",
    "             'hers', 'herself', 'it', \"it's\", 'its', 'itself',\n",
    "             'they', 'them', 'their', 'theirs', 'themselves', 'what',\n",
    "             'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these',\n",
    "             'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been',\n",
    "             'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
    "             'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if',\n",
    "             'or', 'because', 'as', 'until', 'while', 'of', 'at',\n",
    "             'by', 'for', 'with', 'about', 'against', 'between',\n",
    "             'into', 'through', 'during', 'before', 'after', 'above',\n",
    "             'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on',\n",
    "             'off', 'over', 'under', 'again', 'further', 'then', 'once',\n",
    "             'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any',\n",
    "             'both', 'each', 'few', 'more', 'most', 'other', 'some',\n",
    "             'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
    "             'than', 'too', 'very', 's', 't', 'can', 'will', 'just',\n",
    "             'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll',\n",
    "             'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn',\n",
    "             \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\n",
    "             \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\",\n",
    "             'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\",\n",
    "             'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren',\n",
    "             \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'fuck', 'fucker',\n",
    "             'lah', 'la', 'leh', 'lor', 'nah', 'ya', 'yah', 'shit', 'ass', 'asshole',\n",
    "             'le', 'already', 'liao', 'liaoz', 'u', 'cheebye', 'lanjiao',\n",
    "             'nabei', 'kaopei', 'knnb', 'cb', 'cheebye', 'fucked', 'fucks',\n",
    "             'bitch', 'bitches', 'scumbag', 'fuckface', 'wtf', 'ffs', 'siao',\n",
    "             'walao', 'waliao', 'ttyl', 'orhhhh', 'sai'\n",
    "             ]\n",
    "\n",
    "temp_string = ''\n",
    "for word in stopwords:\n",
    "    temp_string += word\n",
    "    temp_string += ' '\n",
    "stopwords_processed = LemNormalize(temp_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing tfidf vectorizer model\n",
    "TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, \n",
    "                            stop_words=stopwords_processed, \n",
    "                            ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nlp_module():\n",
    "    # Function to generate the top matched response given user input\n",
    "    def get_response(input_text):\n",
    "        user_input = input_text.lower()\n",
    "        response = ''\n",
    "        \n",
    "        prompts_corpus = create_prompts_corpus(input_text, qadatabase)\n",
    "      \n",
    "        # Tokenizing the prompts corpus into sentence tokens\n",
    "        # Adding the user's input into the list of sentence tokens,\n",
    "        prompts_sent_token = nltk.sent_tokenize(prompts_corpus)\n",
    "        prompts_sent_token.append(user_input)\n",
    "      \n",
    "        # Fitting the tfidf model with the sentence tokens\n",
    "        # Calculating the cosine similarities between user input and corpus \n",
    "        tfidf = TfidfVec.fit_transform(prompts_sent_token)\n",
    "        cosine_vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "\n",
    "        flat = cosine_vals.flatten()\n",
    "\n",
    "        score = 0\n",
    "        for i in range(0, len(flat)-1, 1):\n",
    "            if flat[i] >= score:\n",
    "                score = flat[i]\n",
    "                matched_score = score\n",
    "        \n",
    "        matching_list = list(flat)\n",
    "        index = matching_list.index(matched_score)\n",
    "       \n",
    "        if matched_score == 0:\n",
    "            response = response + \"I'm sorry, I do not understand you\"\n",
    "        \n",
    "        else:\n",
    "            try:\n",
    "            # bot_response = bot_response + qabig[(qabig['prompt'] == prompts_sent_token[index])]['resp'][qabig[qabig['prompt'] == prompts_sent_token[index]]['resp'].index[0]]\n",
    "            # bot_response = bot_response + qabig[(qabig['prompt'].str.contains(prompts_sent_token[index]))]['resp'][qabig[qabig['prompt'] == prompts_sent_token[index]]['resp'].index[0]\n",
    "                response = response + qadatabase[qadatabase['prompt'] == prompts_sent_token[index]]['response'].values[0]\n",
    "                prompts_sent_token.remove(user_input)\n",
    "            except:\n",
    "                response = response + qadatabase[(qadatabase['prompt'].str.contains(prompts_sent_token[index]))]\n",
    "                prompts_sent_token.remove(user_input)\n",
    "       \n",
    "        return str(response)\n",
    "    \n",
    "    # Function to return fasttext matched label and matched score\n",
    "    # Format: [('label', score)]\n",
    "    def predict_label(input_text):\n",
    "        output = []\n",
    "        user_input = input_text.lower()        \n",
    "        matched_label = ft_model.predict(input_text, k=1)[0][0]\n",
    "        matched_score = ft_model.predict(input_text, k=1)[1][0]\n",
    "        output.append((matched_label, matched_score))\n",
    "        \n",
    "        return output\n",
    "    \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "-- Function that broke with the rewrite of the matching algorithm\n",
    "-- To be debugged in future\n",
    "\n",
    "    # Function to return top 5 matches based on tfidf and cosine similarity\n",
    "    # Format: [('prompt1', score1), ('prompt2', score2), ... ('prompt5', score5)]\n",
    "    def cosine_matches(input_text):\n",
    "        user_input = input_text.lower()\n",
    "        response = ''\n",
    "        \n",
    "        prompts_corpus = create_prompts_corpus(input_text, qadatabase)\n",
    "        \n",
    "        prompts_sent_token = nltk.sent_tokenize(prompts_corpus)\n",
    "        prompts_sent_token.append(user_input)\n",
    "       \n",
    "        tfidf = TfidfVec.fit_transform(prompts_sent_token)\n",
    "        cosine_vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "\n",
    "        flat = cosine_vals.flatten()\n",
    "\n",
    "        score = 0\n",
    "        for i in range(0, len(flat)-1, 1):\n",
    "            if flat[i] > score:\n",
    "                score = flat[i]\n",
    "                matched_score = score\n",
    "        print(matched_score)\n",
    "        \n",
    "        matching_list = list(flat)\n",
    "        index = matching_list.index(matched_score)\n",
    "\n",
    "        \n",
    "      \n",
    "        response = [(prompts_sent_token[index], flat[-2]), \n",
    "                    (prompts_sent_token[index-1], flat[-3]),\n",
    "                    (prompts_sent_token[index-2], flat[-3]),\n",
    "                    (prompts_sent_token[index-3], flat[-4]),\n",
    "                    (prompts_sent_token[index-4], flat[-5]),\n",
    "                    (prompts_sent_token[index-4], flat[-6]),\n",
    "                    (prompts_sent_token[index-4], flat[-7]),\n",
    "                    prompts_sent_token,\n",
    "                    flat]\n",
    "       \n",
    "        return response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
