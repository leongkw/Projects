{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import accuracy_score, SCORERS\n",
    "import matplotlib as plt\n",
    "%matplotlib inline\n",
    "from matplotlib import rcParams\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pprint as pp\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Player</th>\n",
       "      <th>MVP</th>\n",
       "      <th>Year</th>\n",
       "      <th>Age</th>\n",
       "      <th>TmWin</th>\n",
       "      <th>G</th>\n",
       "      <th>GS</th>\n",
       "      <th>MP</th>\n",
       "      <th>PER</th>\n",
       "      <th>TS%</th>\n",
       "      <th>...</th>\n",
       "      <th>ORB/G</th>\n",
       "      <th>DRB/G</th>\n",
       "      <th>TRB/G</th>\n",
       "      <th>AST/G</th>\n",
       "      <th>STL/G</th>\n",
       "      <th>BLK/G</th>\n",
       "      <th>TOV/G</th>\n",
       "      <th>PF/G</th>\n",
       "      <th>PPG</th>\n",
       "      <th>Impact</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A.C. Green</td>\n",
       "      <td>0</td>\n",
       "      <td>1986</td>\n",
       "      <td>22.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1542.0</td>\n",
       "      <td>11.8</td>\n",
       "      <td>0.564</td>\n",
       "      <td>...</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.70</td>\n",
       "      <td>4.65</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.21</td>\n",
       "      <td>2.79</td>\n",
       "      <td>6.35</td>\n",
       "      <td>292.787250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A.C. Green</td>\n",
       "      <td>0</td>\n",
       "      <td>1987</td>\n",
       "      <td>23.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>2240.0</td>\n",
       "      <td>15.7</td>\n",
       "      <td>0.599</td>\n",
       "      <td>...</td>\n",
       "      <td>2.66</td>\n",
       "      <td>5.13</td>\n",
       "      <td>7.78</td>\n",
       "      <td>1.06</td>\n",
       "      <td>0.89</td>\n",
       "      <td>1.01</td>\n",
       "      <td>1.29</td>\n",
       "      <td>2.16</td>\n",
       "      <td>10.78</td>\n",
       "      <td>429.586585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A.C. Green</td>\n",
       "      <td>0</td>\n",
       "      <td>1988</td>\n",
       "      <td>24.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>2636.0</td>\n",
       "      <td>14.5</td>\n",
       "      <td>0.581</td>\n",
       "      <td>...</td>\n",
       "      <td>2.99</td>\n",
       "      <td>5.67</td>\n",
       "      <td>8.66</td>\n",
       "      <td>1.13</td>\n",
       "      <td>1.06</td>\n",
       "      <td>0.55</td>\n",
       "      <td>1.46</td>\n",
       "      <td>2.49</td>\n",
       "      <td>11.43</td>\n",
       "      <td>500.510500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A.C. Green</td>\n",
       "      <td>0</td>\n",
       "      <td>1989</td>\n",
       "      <td>25.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>2510.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>0.594</td>\n",
       "      <td>...</td>\n",
       "      <td>3.15</td>\n",
       "      <td>5.87</td>\n",
       "      <td>9.01</td>\n",
       "      <td>1.26</td>\n",
       "      <td>1.15</td>\n",
       "      <td>0.67</td>\n",
       "      <td>1.45</td>\n",
       "      <td>2.10</td>\n",
       "      <td>13.27</td>\n",
       "      <td>506.706250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A.C. Green</td>\n",
       "      <td>0</td>\n",
       "      <td>1990</td>\n",
       "      <td>26.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>2709.0</td>\n",
       "      <td>14.7</td>\n",
       "      <td>0.548</td>\n",
       "      <td>...</td>\n",
       "      <td>3.20</td>\n",
       "      <td>5.49</td>\n",
       "      <td>8.68</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.41</td>\n",
       "      <td>2.52</td>\n",
       "      <td>12.94</td>\n",
       "      <td>608.001188</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Player  MVP  Year   Age  TmWin     G    GS      MP   PER    TS%  \\\n",
       "0  A.C. Green    0  1986  22.0   62.0  82.0   1.0  1542.0  11.8  0.564   \n",
       "1  A.C. Green    0  1987  23.0   65.0  79.0  72.0  2240.0  15.7  0.599   \n",
       "2  A.C. Green    0  1988  24.0   62.0  82.0  64.0  2636.0  14.5  0.581   \n",
       "3  A.C. Green    0  1989  25.0   57.0  82.0  82.0  2510.0  17.8  0.594   \n",
       "4  A.C. Green    0  1990  26.0   63.0  82.0  82.0  2709.0  14.7  0.548   \n",
       "\n",
       "      ...      ORB/G  DRB/G  TRB/G  AST/G  STL/G  BLK/G  TOV/G  PF/G    PPG  \\\n",
       "0     ...       1.95   2.70   4.65   0.66   0.60   0.60   1.21  2.79   6.35   \n",
       "1     ...       2.66   5.13   7.78   1.06   0.89   1.01   1.29  2.16  10.78   \n",
       "2     ...       2.99   5.67   8.66   1.13   1.06   0.55   1.46  2.49  11.43   \n",
       "3     ...       3.15   5.87   9.01   1.26   1.15   0.67   1.45  2.10  13.27   \n",
       "4     ...       3.20   5.49   8.68   1.10   0.80   0.61   1.41  2.52  12.94   \n",
       "\n",
       "       Impact  \n",
       "0  292.787250  \n",
       "1  429.586585  \n",
       "2  500.510500  \n",
       "3  506.706250  \n",
       "4  608.001188  \n",
       "\n",
       "[5 rows x 51 columns]"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading in the dataframe, then removing the useless column\n",
    "stats = pd.read_csv('stats_1.6.csv')\n",
    "stats.drop(labels='Unnamed: 0', axis=1, inplace=True)\n",
    "stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the predictors for Logistic Regression\n",
    "# Naming the predictors and target variables X1, y1\n",
    "X1 = stats[['WS', 'TOV/G', 'PF/G', 'TS%', 'AST/G', 'VORP', 'BLK/G', 'PPG', 'TRB/G', 'Impact', 'PER']]\n",
    "y1 = stats['MVP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the predictors using StandardScaler()\n",
    "ss = StandardScaler()\n",
    "ss.fit_transform(X1)\n",
    "\n",
    "# Creating a training and testing set\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1a. Testing with base Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores: [0.97740525 0.97849854 0.97485423 0.97885527 0.9795844 ]\n",
      "Mean Cross-Validation Score: 0.9786410731158772\n",
      "[[3344   15]\n",
      " [  54   17]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      3359\n",
      "           1       0.53      0.24      0.33        71\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      3430\n",
      "   macro avg       0.76      0.62      0.66      3430\n",
      "weighted avg       0.97      0.98      0.98      3430\n",
      "\n",
      "AUC Score 0.9026516946274251\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "\n",
    "# Fitting the model\n",
    "rfc1 = rfc.fit(X1_train, y1_train)\n",
    "\n",
    "\n",
    "# Checking cross-validation values\n",
    "print('Cross-Validation Scores:', cross_val_score(rfc1, X1_train, y1_train, cv=5))\n",
    "print('Mean Cross-Validation Score:', np.mean(cross_val_score(rfc1, X1_train, y1_train, cv=5)))\n",
    "\n",
    "# Constructing the confusion matrix\n",
    "predictions_rfc1 = rfc1.predict(X1_test)\n",
    "predictions_proba_rfc1 = rfc1.predict_proba(X1_test)\n",
    "print(confusion_matrix(y1_test, predictions_rfc1))\n",
    "print(classification_report(y1_test, predictions_rfc1))\n",
    "print('AUC Score', roc_auc_score(y1_test, predictions_proba_rfc1[:,1]))\n",
    "\n",
    "# Results are pretty awful for predicting the minority class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1b. Testing with some manually inputed parameters for Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores: [0.9803207  0.97849854 0.97886297 0.98031353 0.9795844 ]\n",
      "Mean Cross-Validation Score: 0.9795160275453341\n",
      "[[3348   11]\n",
      " [  53   18]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      3359\n",
      "           1       0.62      0.25      0.36        71\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      3430\n",
      "   macro avg       0.80      0.63      0.68      3430\n",
      "weighted avg       0.98      0.98      0.98      3430\n",
      "\n",
      "AUC Score 0.912314614091216\n"
     ]
    }
   ],
   "source": [
    "rfc2 = RandomForestClassifier(n_estimators=10, criterion='entropy', max_features=2, n_jobs=-1, random_state=0)\n",
    "\n",
    "# Fitting the model\n",
    "rfct2 = rfc2.fit(X1_train, y1_train)\n",
    "\n",
    "\n",
    "# Checking cross-validation values\n",
    "print('Cross-Validation Scores:', cross_val_score(rfct2, X1_train, y1_train, cv=5))\n",
    "print('Mean Cross-Validation Score:', np.mean(cross_val_score(rfct2, X1_train, y1_train, cv=5)))\n",
    "\n",
    "# Constructing the confusion matrix\n",
    "predictions_rfct2 = rfct2.predict(X1_test)\n",
    "predictions_proba_rfct2 = rfct2.predict_proba(X1_test)\n",
    "print(confusion_matrix(y1_test, predictions_rfct2))\n",
    "print(classification_report(y1_test, predictions_rfct2))\n",
    "print('AUC Score', roc_auc_score(y1_test, predictions_proba_rfct2[:,1]))\n",
    "\n",
    "# Results are still pretty awful for predicting the minority class\n",
    "# Tested both 'gini'/'entropy' entropy can detect 1 more minority class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1c. Trying SMOTE with Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversampling the data with SMOTE\n",
    "sm = SMOTE(sampling_strategy=0.6, random_state=7, k_neighbors=9)\n",
    "smote_X1, smote_y1 = sm.fit_sample(X1_train, y1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores: [0.96731263 0.97104833 0.96777959 0.96870621 0.96986685]\n",
      "Mean Cross-Validation Score: 0.9689427242549208\n",
      "[[3281   78]\n",
      " [  20   51]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99      3359\n",
      "           1       0.40      0.72      0.51        71\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      3430\n",
      "   macro avg       0.69      0.85      0.75      3430\n",
      "weighted avg       0.98      0.97      0.98      3430\n",
      "\n",
      "AUC Score 0.9532095819932995\n"
     ]
    }
   ],
   "source": [
    "rfc3 = RandomForestClassifier(n_estimators=10, criterion='entropy', max_features=2, n_jobs=-1, random_state=0)\n",
    "\n",
    "# Fitting the model\n",
    "rfct3 = rfc3.fit(smote_X1, smote_y1)\n",
    "\n",
    "\n",
    "# Checking cross-validation values\n",
    "print('Cross-Validation Scores:', cross_val_score(rfct3, smote_X1, smote_y1, cv=5))\n",
    "print('Mean Cross-Validation Score:', np.mean(cross_val_score(rfct3, smote_X1, smote_y1, cv=5)))\n",
    "\n",
    "# Constructing the confusion matrix\n",
    "predictions_rfct3 = rfct3.predict(X1_test)\n",
    "predictions_proba_rfct3 = rfct3.predict_proba(X1_test)\n",
    "print(confusion_matrix(y1_test, predictions_rfct3))\n",
    "print(classification_report(y1_test, predictions_rfct3))\n",
    "print('AUC Score', roc_auc_score(y1_test, predictions_proba_rfct3[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done 313 tasks      | elapsed:    9.6s\n",
      "[Parallel(n_jobs=-1)]: Done 662 tasks      | elapsed:   27.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1012 tasks      | elapsed:   42.7s\n",
      "[Parallel(n_jobs=-1)]: Done 1462 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1500 out of 1500 | elapsed:  1.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'entropy', 'max_depth': 11, 'max_features': 7, 'n_estimators': 11}\n",
      "0.9865488128648268\n",
      "{'mean_fit_time': array([0.02792511, 0.0482707 , 0.06343184, 0.0710084 , 0.07340474,\n",
      "       0.03610487, 0.06343174, 0.08956051, 0.09773893, 0.11449313,\n",
      "       0.05126333, 0.08397527, 0.11050491, 0.12326994, 0.13543787,\n",
      "       0.05126305, 0.10013213, 0.13503895, 0.1490015 , 0.16894755,\n",
      "       0.05944142, 0.11050553, 0.16416311, 0.18191428, 0.20026412,\n",
      "       0.03470712, 0.06283121, 0.09155459, 0.10172858, 0.10831099,\n",
      "       0.05904236, 0.11628795, 0.1531899 , 0.17552977, 0.19587545,\n",
      "       0.08098383, 0.15259223, 0.22460003, 0.25850778, 0.28543577,\n",
      "       0.10571694, 0.197472  , 0.30498433, 0.34068837, 0.3528553 ,\n",
      "       0.1308495 , 0.25152688, 0.37499585, 0.40890679, 0.46615334,\n",
      "       0.03969388, 0.07480001, 0.1140944 , 0.12287145, 0.13304467,\n",
      "       0.08856297, 0.15658092, 0.22818961, 0.2463407 , 0.27985172,\n",
      "       0.11688681, 0.22519684, 0.33370624, 0.38297544, 0.40351954,\n",
      "       0.16136794, 0.32533011, 0.45139194, 0.48909259, 0.55371809,\n",
      "       0.19547634, 0.37758937, 0.5547164 , 0.64706893, 0.70112605,\n",
      "       0.04507899, 0.08537049, 0.12227373, 0.14221988, 0.15179434,\n",
      "       0.10152888, 0.18510466, 0.27286916, 0.31276278, 0.33171206,\n",
      "       0.1476047 , 0.28683248, 0.41987634, 0.46754885, 0.52619176,\n",
      "       0.20086236, 0.38297505, 0.57685723, 0.64427667, 0.68616395,\n",
      "       0.24833541, 0.50265484, 0.71987424, 0.80444894, 0.91515212,\n",
      "       0.0522603 , 0.09195371, 0.13882918, 0.1553833 , 0.17313581,\n",
      "       0.11808395, 0.21661968, 0.31455879, 0.34248347, 0.39135275,\n",
      "       0.16834965, 0.33410668, 0.48051429, 0.53656402, 0.59221544,\n",
      "       0.22599497, 0.45199051, 0.66641688, 0.75876994, 0.82479353,\n",
      "       0.2836411 , 0.5664835 , 0.84932728, 0.94666762, 1.03423409,\n",
      "       0.05525231, 0.11210041, 0.16994467, 0.19049015, 0.19088907,\n",
      "       0.12845578, 0.23716459, 0.34767013, 0.40212474, 0.4418191 ,\n",
      "       0.19647422, 0.36881342, 0.55890479, 0.63011317, 0.67579236,\n",
      "       0.26190019, 0.51641746, 0.76195889, 0.86269093, 0.92572384,\n",
      "       0.33350716, 0.6893559 , 1.04440541, 1.18941832, 1.35836806,\n",
      "       0.02712717, 0.04687457, 0.06502552, 0.07081051, 0.07918892,\n",
      "       0.04188776, 0.07759314, 0.11309748, 0.12327085, 0.1380312 ,\n",
      "       0.06362925, 0.10452061, 0.15977283, 0.17174077, 0.1938807 ,\n",
      "       0.07340431, 0.13284531, 0.1942812 , 0.22280383, 0.23916063,\n",
      "       0.08936095, 0.16196666, 0.23716559, 0.2630969 , 0.29481196,\n",
      "       0.04647574, 0.08537226, 0.12366967, 0.12546616, 0.14680691,\n",
      "       0.08577018, 0.16854939, 0.23497076, 0.26469169, 0.29062333,\n",
      "       0.13244519, 0.24514399, 0.35046163, 0.38995643, 0.43004932,\n",
      "       0.16635561, 0.32153926, 0.47074037, 0.51881185, 0.5758594 ,\n",
      "       0.20225897, 0.39733677, 0.58164339, 0.66103158, 0.73682904,\n",
      "       0.05584955, 0.10132818, 0.15518446, 0.17672658, 0.20345683,\n",
      "       0.13065047, 0.22519636, 0.34826632, 0.37001023, 0.40092635,\n",
      "       0.1833086 , 0.3458745 , 0.50564704, 0.56887765, 0.61734977,\n",
      "       0.23656645, 0.45338635, 0.69015307, 0.77532549, 0.83955479,\n",
      "       0.29840155, 0.58303995, 0.86468668, 0.97917976, 1.06495199,\n",
      "       0.06322989, 0.12666168, 0.18610201, 0.21143498, 0.22739153,\n",
      "       0.15099559, 0.28623376, 0.4144907 , 0.45817447, 0.5018569 ,\n",
      "       0.22519674, 0.4250628 , 0.64607124, 0.70531354, 0.77612376,\n",
      "       0.30039701, 0.56967583, 0.86907473, 0.95404816, 1.05039029,\n",
      "       0.37539473, 0.73064518, 1.09686575, 1.23549461, 1.35238333,\n",
      "       0.07639461, 0.14561076, 0.21462545, 0.23756423, 0.25332165,\n",
      "       0.17852182, 0.32572756, 0.48171115, 0.53397136, 0.59959626,\n",
      "       0.25751066, 0.50823956, 0.87326484, 1.04241166, 1.06235862,\n",
      "       0.39454422, 0.68097849, 1.02266369, 1.10743756, 1.25045528,\n",
      "       0.43523564, 0.88642917, 1.27259607, 1.42040033, 1.64061103,\n",
      "       0.07938662, 0.15079675, 0.22739205, 0.25172677, 0.28084841,\n",
      "       0.18410678, 0.37519636, 0.54613919, 0.59101839, 0.67519312,\n",
      "       0.30139375, 0.56269479, 0.85311828, 0.95504494, 1.02246432,\n",
      "       0.40352044, 0.77452807, 1.14134712, 1.31248918, 1.41980205,\n",
      "       0.49966283, 0.97898078, 1.47345819, 1.63961368, 1.44553294]),\n",
      " 'mean_score_time': array([0.00458808, 0.00538492, 0.00598221, 0.00897665, 0.00658207,\n",
      "       0.00319099, 0.00478659, 0.0059844 , 0.00598392, 0.00658216,\n",
      "       0.00458694, 0.00498667, 0.00558543, 0.00618391, 0.00738173,\n",
      "       0.00299211, 0.00458713, 0.00578508, 0.00578365, 0.00658283,\n",
      "       0.00598426, 0.00478716, 0.00558233, 0.0057847 , 0.00598488,\n",
      "       0.00378928, 0.00518646, 0.00738044, 0.00658307, 0.00698037,\n",
      "       0.00398974, 0.00518665, 0.00578451, 0.00618472, 0.00777946,\n",
      "       0.00359006, 0.00478644, 0.00598392, 0.00618377, 0.00638309,\n",
      "       0.00359149, 0.00478668, 0.00578475, 0.00598397, 0.0063839 ,\n",
      "       0.00359035, 0.00458841, 0.0059845 , 0.00598402, 0.00658226,\n",
      "       0.00378985, 0.00538483, 0.00638375, 0.0071816 , 0.00758028,\n",
      "       0.00379009, 0.00538607, 0.00618477, 0.00698237, 0.00718102,\n",
      "       0.00359068, 0.0043889 , 0.00638289, 0.00638251, 0.00638309,\n",
      "       0.0037899 , 0.00458746, 0.0057848 , 0.00618291, 0.00658288,\n",
      "       0.00379057, 0.00498714, 0.00618329, 0.00658283, 0.00698161,\n",
      "       0.00379019, 0.00518675, 0.00638237, 0.00797834, 0.00797877,\n",
      "       0.00378971, 0.00498652, 0.00638299, 0.00718126, 0.0073802 ,\n",
      "       0.00379014, 0.00478706, 0.00638385, 0.00658226, 0.00738125,\n",
      "       0.00378976, 0.00498786, 0.00638251, 0.00678167, 0.00698199,\n",
      "       0.00438843, 0.00498734, 0.00718117, 0.00698104, 0.00777965,\n",
      "       0.00438867, 0.0059845 , 0.00718012, 0.00797925, 0.00877728,\n",
      "       0.00379028, 0.00578442, 0.0069819 , 0.00718136, 0.00777998,\n",
      "       0.00359015, 0.00498633, 0.00678186, 0.00758038, 0.00797882,\n",
      "       0.00359015, 0.00538621, 0.00698214, 0.00738063, 0.00877619,\n",
      "       0.00418887, 0.0053865 , 0.00658255, 0.00738049, 0.00797901,\n",
      "       0.00458751, 0.00638304, 0.00817881, 0.00857692, 0.00957441,\n",
      "       0.00398984, 0.00558538, 0.00698071, 0.00877671, 0.00877824,\n",
      "       0.00398941, 0.00578427, 0.00737972, 0.00777984, 0.00917573,\n",
      "       0.00398917, 0.00578489, 0.00718131, 0.00778003, 0.00817761,\n",
      "       0.00438871, 0.00618348, 0.00817804, 0.00837827, 0.00897598,\n",
      "       0.00438814, 0.0049871 , 0.00658274, 0.00678205, 0.00718045,\n",
      "       0.00398979, 0.00498619, 0.00718117, 0.0069808 , 0.00857749,\n",
      "       0.00418968, 0.00498705, 0.0061832 , 0.00758047, 0.00718069,\n",
      "       0.00418901, 0.00578356, 0.00638289, 0.00698233, 0.00877657,\n",
      "       0.00538583, 0.00518641, 0.00618348, 0.00678196, 0.00837741,\n",
      "       0.0041894 , 0.00538578, 0.00698133, 0.00777831, 0.00817828,\n",
      "       0.00438905, 0.00518599, 0.00658288, 0.00777912, 0.00737996,\n",
      "       0.00398946, 0.00558596, 0.00678234, 0.00718112, 0.00738029,\n",
      "       0.0039887 , 0.00558577, 0.00618339, 0.00718107, 0.00778008,\n",
      "       0.0039887 , 0.00578547, 0.00678267, 0.00738039, 0.00718102,\n",
      "       0.00418987, 0.00558519, 0.00758023, 0.00877652, 0.00937486,\n",
      "       0.00438843, 0.00578451, 0.00718117, 0.00777936, 0.00857754,\n",
      "       0.00538578, 0.00518641, 0.00698175, 0.00737991, 0.00797791,\n",
      "       0.00478735, 0.00538616, 0.00718155, 0.00738101, 0.00777907,\n",
      "       0.00418906, 0.00538692, 0.00698161, 0.00738068, 0.00837765,\n",
      "       0.00418935, 0.00658216, 0.00777993, 0.00817757, 0.00937533,\n",
      "       0.00458827, 0.00738068, 0.00738049, 0.00777922, 0.00837831,\n",
      "       0.00438895, 0.00578504, 0.00797982, 0.00777884, 0.00917602,\n",
      "       0.00418839, 0.00538592, 0.00678248, 0.00738082, 0.00797825,\n",
      "       0.00438876, 0.00578523, 0.00718088, 0.00777965, 0.00877628,\n",
      "       0.00498796, 0.00638313, 0.00797858, 0.00937481, 0.00957484,\n",
      "       0.00618391, 0.00777955, 0.00817852, 0.00877705, 0.00997286,\n",
      "       0.00438886, 0.00598388, 0.00877671, 0.00917583, 0.0091753 ,\n",
      "       0.00518603, 0.006183  , 0.00777988, 0.00797868, 0.00877681,\n",
      "       0.00398936, 0.00618339, 0.00777936, 0.00797896, 0.00937543,\n",
      "       0.00498753, 0.00638342, 0.0089756 , 0.01037216, 0.00997348,\n",
      "       0.00418887, 0.00638337, 0.00797911, 0.00857773, 0.00957489,\n",
      "       0.00478721, 0.00598407, 0.0083777 , 0.00977459, 0.00877705,\n",
      "       0.00498662, 0.00698133, 0.00837836, 0.00837755, 0.00917583,\n",
      "       0.0043889 , 0.00638385, 0.00817885, 0.0083777 , 0.00638309]),\n",
      " 'mean_test_score': array([0.84730641, 0.85564723, 0.85478128, 0.84780038, 0.89264052,\n",
      "       0.91343917, 0.8974972 , 0.88865328, 0.9006102 , 0.89139267,\n",
      "       0.90347504, 0.89139481, 0.89749657, 0.89774463, 0.89600159,\n",
      "       0.89550282, 0.8936362 , 0.89687398, 0.89998772, 0.88553857,\n",
      "       0.90746075, 0.89189055, 0.89264061, 0.8961247 , 0.90596614,\n",
      "       0.90708771, 0.91817136, 0.93299321, 0.91468334, 0.91256728,\n",
      "       0.91219411, 0.92178377, 0.92738804, 0.91543169, 0.92552024,\n",
      "       0.92253156, 0.92664124, 0.92390179, 0.92801164, 0.92066322,\n",
      "       0.92078672, 0.92464869, 0.92327868, 0.92502238, 0.92589354,\n",
      "       0.91854357, 0.93087589, 0.92489717, 0.9261431 , 0.92651721,\n",
      "       0.9285097 , 0.94034172, 0.93697928, 0.94171184, 0.93909621,\n",
      "       0.94607106, 0.94706704, 0.94495031, 0.95441564, 0.94881056,\n",
      "       0.94943349, 0.95155126, 0.95503878, 0.9577786 , 0.95653294,\n",
      "       0.94507451, 0.95752968, 0.9555365 , 0.9546648 , 0.95914837,\n",
      "       0.95080355, 0.9549134 , 0.95254767, 0.95728033, 0.95765334,\n",
      "       0.94806369, 0.95229811, 0.95591027, 0.95591033, 0.95341898,\n",
      "       0.95952224, 0.96525115, 0.96936133, 0.96911217, 0.9721013 ,\n",
      "       0.96761767, 0.96961077, 0.97210134, 0.97459218, 0.97322228,\n",
      "       0.96824057, 0.97085587, 0.97185205, 0.97284891, 0.97409445,\n",
      "       0.96786639, 0.97085576, 0.97085583, 0.97384514, 0.97770593,\n",
      "       0.95902441, 0.96412983, 0.96811571, 0.96886329, 0.9706069 ,\n",
      "       0.96936101, 0.97558841, 0.97895163, 0.98144264, 0.98044597,\n",
      "       0.97210117, 0.97583761, 0.98019704, 0.98343531, 0.98231439,\n",
      "       0.97434318, 0.97832853, 0.98144244, 0.98231405, 0.98169157,\n",
      "       0.9725996 , 0.98019674, 0.98131788, 0.98368408, 0.9821895 ,\n",
      "       0.96699546, 0.96861377, 0.97334679, 0.97434296, 0.97683404,\n",
      "       0.97259953, 0.97695861, 0.98343509, 0.98306158, 0.98293691,\n",
      "       0.97484118, 0.97646053, 0.9851789 , 0.98380877, 0.98517877,\n",
      "       0.97359579, 0.97758156, 0.98343521, 0.98206499, 0.98567707,\n",
      "       0.97670964, 0.9787024 , 0.98393327, 0.98156691, 0.98542788,\n",
      "       0.90359574, 0.91007797, 0.91978943, 0.89226416, 0.90197909,\n",
      "       0.93934583, 0.93934596, 0.93847176, 0.93673004, 0.94121132,\n",
      "       0.960392  , 0.94507551, 0.94582041, 0.93523382, 0.95453919,\n",
      "       0.96537555, 0.95827651, 0.95130147, 0.94121217, 0.9579029 ,\n",
      "       0.96500233, 0.95292155, 0.9562832 , 0.9581513 , 0.96288507,\n",
      "       0.92053719, 0.91767208, 0.92975469, 0.92963023, 0.91904319,\n",
      "       0.92651695, 0.9169264 , 0.92377628, 0.92290479, 0.92676533,\n",
      "       0.91842076, 0.92601884, 0.9242753 , 0.92464927, 0.91954212,\n",
      "       0.93050235, 0.93212137, 0.92390164, 0.92838497, 0.93274385,\n",
      "       0.92788597, 0.92265657, 0.93448747, 0.93623215, 0.93423811,\n",
      "       0.92601831, 0.9393447 , 0.93498555, 0.94046639, 0.93922013,\n",
      "       0.94345512, 0.94831255, 0.95167538, 0.9526712 , 0.95180017,\n",
      "       0.94108843, 0.94881086, 0.95665736, 0.95130167, 0.95379282,\n",
      "       0.95304577, 0.9498072 , 0.95329468, 0.95341904, 0.95466406,\n",
      "       0.94607099, 0.95180022, 0.953917  , 0.95080356, 0.95354395,\n",
      "       0.94457668, 0.95590994, 0.95715568, 0.96114095, 0.9580275 ,\n",
      "       0.96101634, 0.96624788, 0.96711958, 0.97135406, 0.97122951,\n",
      "       0.96500187, 0.97172761, 0.97085553, 0.97073151, 0.97334679,\n",
      "       0.9658742 , 0.9698597 , 0.97297355, 0.97322219, 0.97646061,\n",
      "       0.96587364, 0.96898781, 0.97010861, 0.97272412, 0.97160299,\n",
      "       0.95541225, 0.9630098 , 0.96662138, 0.96774213, 0.97110485,\n",
      "       0.97147873, 0.97795494, 0.97957429, 0.98032155, 0.98106891,\n",
      "       0.97085604, 0.97783037, 0.98044614, 0.98243843, 0.98293677,\n",
      "       0.97297319, 0.97783006, 0.97857786, 0.98206519, 0.98293706,\n",
      "       0.97235053, 0.97633602, 0.98057047, 0.97845317, 0.97845324,\n",
      "       0.96413066, 0.97185206, 0.9739697 , 0.97533922, 0.97446802,\n",
      "       0.9718522 , 0.97857787, 0.98343536, 0.98318618, 0.9848052 ,\n",
      "       0.97372032, 0.98106866, 0.98281261, 0.98356004, 0.98567717,\n",
      "       0.97284862, 0.98243864, 0.98443173, 0.98355978, 0.98654881,\n",
      "       0.97446776, 0.97857758, 0.98343517, 0.98306159, 0.98318582]),\n",
      " 'mean_train_score': array([0.84879885, 0.86162744, 0.8568622 , 0.85430894, 0.89385362,\n",
      "       0.91711331, 0.89861781, 0.88974385, 0.90331959, 0.89453919,\n",
      "       0.90331913, 0.89242127, 0.89976987, 0.90057989, 0.89600207,\n",
      "       0.89724817, 0.89484974, 0.89839931, 0.90145084, 0.88803129,\n",
      "       0.90758505, 0.89360525, 0.89204714, 0.89774709, 0.9061527 ,\n",
      "       0.90524866, 0.91926133, 0.93165384, 0.91661521, 0.91275374,\n",
      "       0.91315864, 0.9222503 , 0.92925666, 0.91879461, 0.92763739,\n",
      "       0.92418118, 0.9305956 , 0.92583119, 0.92906961, 0.92499062,\n",
      "       0.92383883, 0.92857171, 0.92928771, 0.92981704, 0.92959921,\n",
      "       0.92362183, 0.93249472, 0.92810464, 0.92813549, 0.92900744,\n",
      "       0.93591968, 0.94638187, 0.9442335 , 0.94644404, 0.94597692,\n",
      "       0.95142611, 0.95597211, 0.95675053, 0.95992652, 0.95522485,\n",
      "       0.95759139, 0.95821402, 0.96276005, 0.96294693, 0.96126545,\n",
      "       0.95388585, 0.96282226, 0.96322724, 0.96223074, 0.96503315,\n",
      "       0.95914825, 0.96216859, 0.96490848, 0.96450373, 0.96437947,\n",
      "       0.95510034, 0.96263575, 0.9647839 , 0.96873828, 0.96437917,\n",
      "       0.97278612, 0.97826636, 0.98088182, 0.98137987, 0.98253214,\n",
      "       0.97667829, 0.98022792, 0.9839333 , 0.98489856, 0.9846494 ,\n",
      "       0.9790136 , 0.98583257, 0.98461828, 0.98545901, 0.98654874,\n",
      "       0.98041476, 0.98299929, 0.98440025, 0.98620626, 0.98707813,\n",
      "       0.97496575, 0.9818782 , 0.98299898, 0.98567688, 0.98605058,\n",
      "       0.98692242, 0.9920289 , 0.99371031, 0.99433304, 0.99423964,\n",
      "       0.98763866, 0.99327438, 0.99523605, 0.99579651, 0.99501807,\n",
      "       0.99062776, 0.99386601, 0.99610789, 0.99554739, 0.99610788,\n",
      "       0.99040981, 0.99458213, 0.99595219, 0.99570307, 0.9962947 ,\n",
      "       0.98611279, 0.99081463, 0.99411504, 0.99458215, 0.99554738,\n",
      "       0.99258934, 0.99613899, 0.99810066, 0.99819403, 0.99788269,\n",
      "       0.99411507, 0.99666833, 0.99872336, 0.99878563, 0.99928384,\n",
      "       0.99423965, 0.99626354, 0.99891021, 0.99856769, 0.99884796,\n",
      "       0.99467553, 0.9975402 , 0.99834973, 0.9986611 , 0.99934614,\n",
      "       0.89936633, 0.9128471 , 0.92268684, 0.89223418, 0.90282155,\n",
      "       0.94336189, 0.94006096, 0.94084005, 0.93819286, 0.9456976 ,\n",
      "       0.96067445, 0.94821835, 0.94806378, 0.93884692, 0.95656404,\n",
      "       0.96459713, 0.95818335, 0.95379303, 0.94470129, 0.95917924,\n",
      "       0.96593617, 0.953543  , 0.95718701, 0.95921115, 0.96428582,\n",
      "       0.91586774, 0.92237528, 0.92875858, 0.93193436, 0.91941734,\n",
      "       0.92520829, 0.92203292, 0.92748172, 0.92869619, 0.92813585,\n",
      "       0.92078737, 0.92807317, 0.92872706, 0.92717009, 0.92221926,\n",
      "       0.93557739, 0.93370898, 0.92664079, 0.93280647, 0.93548413,\n",
      "       0.93202765, 0.92791761, 0.93732129, 0.938224  , 0.93672972,\n",
      "       0.93100012, 0.94074607, 0.94055918, 0.94579022, 0.946538  ,\n",
      "       0.9474097 , 0.95519382, 0.95712431, 0.95615936, 0.95640805,\n",
      "       0.94896613, 0.95756022, 0.96079855, 0.95802722, 0.95852547,\n",
      "       0.95793388, 0.95749803, 0.95818305, 0.95896145, 0.96054938,\n",
      "       0.95176881, 0.95833863, 0.96095415, 0.95986424, 0.95961507,\n",
      "       0.95413488, 0.9634763 , 0.96690142, 0.97104242, 0.9672436 ,\n",
      "       0.97116723, 0.97845303, 0.97807962, 0.9809752 , 0.98200269,\n",
      "       0.97319108, 0.98231406, 0.98184703, 0.98337289, 0.98440034,\n",
      "       0.97726992, 0.98085077, 0.98561472, 0.98368417, 0.98573923,\n",
      "       0.97702101, 0.98050827, 0.98240759, 0.98408898, 0.98443137,\n",
      "       0.9701393 , 0.97907592, 0.98256328, 0.98234521, 0.98517883,\n",
      "       0.98505408, 0.99137504, 0.99277617, 0.99237136, 0.9945199 ,\n",
      "       0.98679789, 0.99209118, 0.99389718, 0.99423966, 0.99536055,\n",
      "       0.98717138, 0.99243375, 0.99349236, 0.99470666, 0.99526712,\n",
      "       0.98720269, 0.99318099, 0.99483128, 0.99339899, 0.99405288,\n",
      "       0.98424457, 0.99075241, 0.99330549, 0.9934301 , 0.9930564 ,\n",
      "       0.98960028, 0.99514262, 0.99813176, 0.99691737, 0.99788266,\n",
      "       0.99209118, 0.99673063, 0.99800723, 0.99791379, 0.99894133,\n",
      "       0.99125053, 0.99707311, 0.99863   , 0.9981318 , 0.9988168 ,\n",
      "       0.9909702 , 0.99604565, 0.99760245, 0.99816291, 0.9976024 ]),\n",
      " 'param_criterion': masked_array(data=['gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
      "                   'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
      "                   'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
      "                   'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
      "                   'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
      "                   'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
      "                   'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
      "                   'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
      "                   'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
      "                   'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
      "                   'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
      "                   'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
      "                   'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
      "                   'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
      "                   'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
      "                   'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
      "                   'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
      "                   'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
      "                   'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
      "                   'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
      "                   'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
      "                   'gini', 'gini', 'gini', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy'],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object),\n",
      " 'param_max_depth': masked_array(data=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                   1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "                   3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5,\n",
      "                   5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "                   5, 5, 5, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "                   7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "                   9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 11,\n",
      "                   11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
      "                   11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 1, 1, 1, 1, 1,\n",
      "                   1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                   1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "                   3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "                   5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 7, 7,\n",
      "                   7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "                   7, 7, 7, 7, 7, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "                   9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 11, 11, 11, 11, 11,\n",
      "                   11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
      "                   11, 11, 11, 11, 11, 11],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object),\n",
      " 'param_max_features': masked_array(data=[1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 7, 7, 7,\n",
      "                   7, 7, 9, 9, 9, 9, 9, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 5,\n",
      "                   5, 5, 5, 5, 7, 7, 7, 7, 7, 9, 9, 9, 9, 9, 1, 1, 1, 1,\n",
      "                   1, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 7, 7, 7, 7, 7, 9, 9,\n",
      "                   9, 9, 9, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5,\n",
      "                   7, 7, 7, 7, 7, 9, 9, 9, 9, 9, 1, 1, 1, 1, 1, 3, 3, 3,\n",
      "                   3, 3, 5, 5, 5, 5, 5, 7, 7, 7, 7, 7, 9, 9, 9, 9, 9, 1,\n",
      "                   1, 1, 1, 1, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 7, 7, 7, 7,\n",
      "                   7, 9, 9, 9, 9, 9, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 5, 5,\n",
      "                   5, 5, 5, 7, 7, 7, 7, 7, 9, 9, 9, 9, 9, 1, 1, 1, 1, 1,\n",
      "                   3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 7, 7, 7, 7, 7, 9, 9, 9,\n",
      "                   9, 9, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 7,\n",
      "                   7, 7, 7, 7, 9, 9, 9, 9, 9, 1, 1, 1, 1, 1, 3, 3, 3, 3,\n",
      "                   3, 5, 5, 5, 5, 5, 7, 7, 7, 7, 7, 9, 9, 9, 9, 9, 1, 1,\n",
      "                   1, 1, 1, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 7, 7, 7, 7, 7,\n",
      "                   9, 9, 9, 9, 9, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 5, 5, 5,\n",
      "                   5, 5, 7, 7, 7, 7, 7, 9, 9, 9, 9, 9],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object),\n",
      " 'param_n_estimators': masked_array(data=[3, 6, 9, 10, 11, 3, 6, 9, 10, 11, 3, 6, 9, 10, 11, 3,\n",
      "                   6, 9, 10, 11, 3, 6, 9, 10, 11, 3, 6, 9, 10, 11, 3, 6,\n",
      "                   9, 10, 11, 3, 6, 9, 10, 11, 3, 6, 9, 10, 11, 3, 6, 9,\n",
      "                   10, 11, 3, 6, 9, 10, 11, 3, 6, 9, 10, 11, 3, 6, 9, 10,\n",
      "                   11, 3, 6, 9, 10, 11, 3, 6, 9, 10, 11, 3, 6, 9, 10, 11,\n",
      "                   3, 6, 9, 10, 11, 3, 6, 9, 10, 11, 3, 6, 9, 10, 11, 3,\n",
      "                   6, 9, 10, 11, 3, 6, 9, 10, 11, 3, 6, 9, 10, 11, 3, 6,\n",
      "                   9, 10, 11, 3, 6, 9, 10, 11, 3, 6, 9, 10, 11, 3, 6, 9,\n",
      "                   10, 11, 3, 6, 9, 10, 11, 3, 6, 9, 10, 11, 3, 6, 9, 10,\n",
      "                   11, 3, 6, 9, 10, 11, 3, 6, 9, 10, 11, 3, 6, 9, 10, 11,\n",
      "                   3, 6, 9, 10, 11, 3, 6, 9, 10, 11, 3, 6, 9, 10, 11, 3,\n",
      "                   6, 9, 10, 11, 3, 6, 9, 10, 11, 3, 6, 9, 10, 11, 3, 6,\n",
      "                   9, 10, 11, 3, 6, 9, 10, 11, 3, 6, 9, 10, 11, 3, 6, 9,\n",
      "                   10, 11, 3, 6, 9, 10, 11, 3, 6, 9, 10, 11, 3, 6, 9, 10,\n",
      "                   11, 3, 6, 9, 10, 11, 3, 6, 9, 10, 11, 3, 6, 9, 10, 11,\n",
      "                   3, 6, 9, 10, 11, 3, 6, 9, 10, 11, 3, 6, 9, 10, 11, 3,\n",
      "                   6, 9, 10, 11, 3, 6, 9, 10, 11, 3, 6, 9, 10, 11, 3, 6,\n",
      "                   9, 10, 11, 3, 6, 9, 10, 11, 3, 6, 9, 10, 11, 3, 6, 9,\n",
      "                   10, 11, 3, 6, 9, 10, 11, 3, 6, 9, 10, 11],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object),\n",
      " 'params': [{'criterion': 'gini',\n",
      "             'max_depth': 1,\n",
      "             'max_features': 1,\n",
      "             'n_estimators': 3},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 1,\n",
      "             'max_features': 1,\n",
      "             'n_estimators': 6},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 1,\n",
      "             'max_features': 1,\n",
      "             'n_estimators': 9},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 1,\n",
      "             'max_features': 1,\n",
      "             'n_estimators': 10},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 1,\n",
      "             'max_features': 1,\n",
      "             'n_estimators': 11},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 1,\n",
      "             'max_features': 3,\n",
      "             'n_estimators': 3},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 1,\n",
      "             'max_features': 3,\n",
      "             'n_estimators': 6},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 1,\n",
      "             'max_features': 3,\n",
      "             'n_estimators': 9},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 1,\n",
      "             'max_features': 3,\n",
      "             'n_estimators': 10},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 1,\n",
      "             'max_features': 3,\n",
      "             'n_estimators': 11},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 1,\n",
      "             'max_features': 5,\n",
      "             'n_estimators': 3},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 1,\n",
      "             'max_features': 5,\n",
      "             'n_estimators': 6},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 1,\n",
      "             'max_features': 5,\n",
      "             'n_estimators': 9},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 1,\n",
      "             'max_features': 5,\n",
      "             'n_estimators': 10},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 1,\n",
      "             'max_features': 5,\n",
      "             'n_estimators': 11},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 1,\n",
      "             'max_features': 7,\n",
      "             'n_estimators': 3},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 1,\n",
      "             'max_features': 7,\n",
      "             'n_estimators': 6},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 1,\n",
      "             'max_features': 7,\n",
      "             'n_estimators': 9},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 1,\n",
      "             'max_features': 7,\n",
      "             'n_estimators': 10},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 1,\n",
      "             'max_features': 7,\n",
      "             'n_estimators': 11},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 1,\n",
      "             'max_features': 9,\n",
      "             'n_estimators': 3},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 1,\n",
      "             'max_features': 9,\n",
      "             'n_estimators': 6},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 1,\n",
      "             'max_features': 9,\n",
      "             'n_estimators': 9},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 1,\n",
      "             'max_features': 9,\n",
      "             'n_estimators': 10},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 1,\n",
      "             'max_features': 9,\n",
      "             'n_estimators': 11},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 3,\n",
      "             'max_features': 1,\n",
      "             'n_estimators': 3},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 3,\n",
      "             'max_features': 1,\n",
      "             'n_estimators': 6},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 3,\n",
      "             'max_features': 1,\n",
      "             'n_estimators': 9},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 3,\n",
      "             'max_features': 1,\n",
      "             'n_estimators': 10},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 3,\n",
      "             'max_features': 1,\n",
      "             'n_estimators': 11},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 3,\n",
      "             'max_features': 3,\n",
      "             'n_estimators': 3},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 3,\n",
      "             'max_features': 3,\n",
      "             'n_estimators': 6},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 3,\n",
      "             'max_features': 3,\n",
      "             'n_estimators': 9},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 3,\n",
      "             'max_features': 3,\n",
      "             'n_estimators': 10},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 3,\n",
      "             'max_features': 3,\n",
      "             'n_estimators': 11},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 3,\n",
      "             'max_features': 5,\n",
      "             'n_estimators': 3},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 3,\n",
      "             'max_features': 5,\n",
      "             'n_estimators': 6},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 3,\n",
      "             'max_features': 5,\n",
      "             'n_estimators': 9},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 3,\n",
      "             'max_features': 5,\n",
      "             'n_estimators': 10},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 3,\n",
      "             'max_features': 5,\n",
      "             'n_estimators': 11},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 3,\n",
      "             'max_features': 7,\n",
      "             'n_estimators': 3},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 3,\n",
      "             'max_features': 7,\n",
      "             'n_estimators': 6},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 3,\n",
      "             'max_features': 7,\n",
      "             'n_estimators': 9},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 3,\n",
      "             'max_features': 7,\n",
      "             'n_estimators': 10},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 3,\n",
      "             'max_features': 7,\n",
      "             'n_estimators': 11},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 3,\n",
      "             'max_features': 9,\n",
      "             'n_estimators': 3},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 3,\n",
      "             'max_features': 9,\n",
      "             'n_estimators': 6},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 3,\n",
      "             'max_features': 9,\n",
      "             'n_estimators': 9},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 3,\n",
      "             'max_features': 9,\n",
      "             'n_estimators': 10},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 3,\n",
      "             'max_features': 9,\n",
      "             'n_estimators': 11},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 5,\n",
      "             'max_features': 1,\n",
      "             'n_estimators': 3},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 5,\n",
      "             'max_features': 1,\n",
      "             'n_estimators': 6},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 5,\n",
      "             'max_features': 1,\n",
      "             'n_estimators': 9},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 5,\n",
      "             'max_features': 1,\n",
      "             'n_estimators': 10},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 5,\n",
      "             'max_features': 1,\n",
      "             'n_estimators': 11},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 5,\n",
      "             'max_features': 3,\n",
      "             'n_estimators': 3},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 5,\n",
      "             'max_features': 3,\n",
      "             'n_estimators': 6},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 5,\n",
      "             'max_features': 3,\n",
      "             'n_estimators': 9},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 5,\n",
      "             'max_features': 3,\n",
      "             'n_estimators': 10},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 5,\n",
      "             'max_features': 3,\n",
      "             'n_estimators': 11},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 5,\n",
      "             'max_features': 5,\n",
      "             'n_estimators': 3},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 5,\n",
      "             'max_features': 5,\n",
      "             'n_estimators': 6},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 5,\n",
      "             'max_features': 5,\n",
      "             'n_estimators': 9},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 5,\n",
      "             'max_features': 5,\n",
      "             'n_estimators': 10},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 5,\n",
      "             'max_features': 5,\n",
      "             'n_estimators': 11},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 5,\n",
      "             'max_features': 7,\n",
      "             'n_estimators': 3},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 5,\n",
      "             'max_features': 7,\n",
      "             'n_estimators': 6},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 5,\n",
      "             'max_features': 7,\n",
      "             'n_estimators': 9},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 5,\n",
      "             'max_features': 7,\n",
      "             'n_estimators': 10},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 5,\n",
      "             'max_features': 7,\n",
      "             'n_estimators': 11},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 5,\n",
      "             'max_features': 9,\n",
      "             'n_estimators': 3},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 5,\n",
      "             'max_features': 9,\n",
      "             'n_estimators': 6},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 5,\n",
      "             'max_features': 9,\n",
      "             'n_estimators': 9},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 5,\n",
      "             'max_features': 9,\n",
      "             'n_estimators': 10},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 5,\n",
      "             'max_features': 9,\n",
      "             'n_estimators': 11},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 7,\n",
      "             'max_features': 1,\n",
      "             'n_estimators': 3},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 7,\n",
      "             'max_features': 1,\n",
      "             'n_estimators': 6},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 7,\n",
      "             'max_features': 1,\n",
      "             'n_estimators': 9},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 7,\n",
      "             'max_features': 1,\n",
      "             'n_estimators': 10},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 7,\n",
      "             'max_features': 1,\n",
      "             'n_estimators': 11},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 7,\n",
      "             'max_features': 3,\n",
      "             'n_estimators': 3},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 7,\n",
      "             'max_features': 3,\n",
      "             'n_estimators': 6},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 7,\n",
      "             'max_features': 3,\n",
      "             'n_estimators': 9},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 7,\n",
      "             'max_features': 3,\n",
      "             'n_estimators': 10},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 7,\n",
      "             'max_features': 3,\n",
      "             'n_estimators': 11},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 7,\n",
      "             'max_features': 5,\n",
      "             'n_estimators': 3},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 7,\n",
      "             'max_features': 5,\n",
      "             'n_estimators': 6},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 7,\n",
      "             'max_features': 5,\n",
      "             'n_estimators': 9},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 7,\n",
      "             'max_features': 5,\n",
      "             'n_estimators': 10},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 7,\n",
      "             'max_features': 5,\n",
      "             'n_estimators': 11},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 7,\n",
      "             'max_features': 7,\n",
      "             'n_estimators': 3},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 7,\n",
      "             'max_features': 7,\n",
      "             'n_estimators': 6},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 7,\n",
      "             'max_features': 7,\n",
      "             'n_estimators': 9},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 7,\n",
      "             'max_features': 7,\n",
      "             'n_estimators': 10},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 7,\n",
      "             'max_features': 7,\n",
      "             'n_estimators': 11},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 7,\n",
      "             'max_features': 9,\n",
      "             'n_estimators': 3},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 7,\n",
      "             'max_features': 9,\n",
      "             'n_estimators': 6},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 7,\n",
      "             'max_features': 9,\n",
      "             'n_estimators': 9},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 7,\n",
      "             'max_features': 9,\n",
      "             'n_estimators': 10},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 7,\n",
      "             'max_features': 9,\n",
      "             'n_estimators': 11},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 9,\n",
      "             'max_features': 1,\n",
      "             'n_estimators': 3},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 9,\n",
      "             'max_features': 1,\n",
      "             'n_estimators': 6},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 9,\n",
      "             'max_features': 1,\n",
      "             'n_estimators': 9},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 9,\n",
      "             'max_features': 1,\n",
      "             'n_estimators': 10},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 9,\n",
      "             'max_features': 1,\n",
      "             'n_estimators': 11},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 9,\n",
      "             'max_features': 3,\n",
      "             'n_estimators': 3},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 9,\n",
      "             'max_features': 3,\n",
      "             'n_estimators': 6},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 9,\n",
      "             'max_features': 3,\n",
      "             'n_estimators': 9},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 9,\n",
      "             'max_features': 3,\n",
      "             'n_estimators': 10},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 9,\n",
      "             'max_features': 3,\n",
      "             'n_estimators': 11},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 9,\n",
      "             'max_features': 5,\n",
      "             'n_estimators': 3},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 9,\n",
      "             'max_features': 5,\n",
      "             'n_estimators': 6},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 9,\n",
      "             'max_features': 5,\n",
      "             'n_estimators': 9},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 9,\n",
      "             'max_features': 5,\n",
      "             'n_estimators': 10},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 9,\n",
      "             'max_features': 5,\n",
      "             'n_estimators': 11},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 9,\n",
      "             'max_features': 7,\n",
      "             'n_estimators': 3},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 9,\n",
      "             'max_features': 7,\n",
      "             'n_estimators': 6},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 9,\n",
      "             'max_features': 7,\n",
      "             'n_estimators': 9},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 9,\n",
      "             'max_features': 7,\n",
      "             'n_estimators': 10},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 9,\n",
      "             'max_features': 7,\n",
      "             'n_estimators': 11},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 9,\n",
      "             'max_features': 9,\n",
      "             'n_estimators': 3},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 9,\n",
      "             'max_features': 9,\n",
      "             'n_estimators': 6},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 9,\n",
      "             'max_features': 9,\n",
      "             'n_estimators': 9},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 9,\n",
      "             'max_features': 9,\n",
      "             'n_estimators': 10},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 9,\n",
      "             'max_features': 9,\n",
      "             'n_estimators': 11},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 11,\n",
      "             'max_features': 1,\n",
      "             'n_estimators': 3},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 11,\n",
      "             'max_features': 1,\n",
      "             'n_estimators': 6},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 11,\n",
      "             'max_features': 1,\n",
      "             'n_estimators': 9},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 11,\n",
      "             'max_features': 1,\n",
      "             'n_estimators': 10},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 11,\n",
      "             'max_features': 1,\n",
      "             'n_estimators': 11},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 11,\n",
      "             'max_features': 3,\n",
      "             'n_estimators': 3},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 11,\n",
      "             'max_features': 3,\n",
      "             'n_estimators': 6},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 11,\n",
      "             'max_features': 3,\n",
      "             'n_estimators': 9},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 11,\n",
      "             'max_features': 3,\n",
      "             'n_estimators': 10},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 11,\n",
      "             'max_features': 3,\n",
      "             'n_estimators': 11},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 11,\n",
      "             'max_features': 5,\n",
      "             'n_estimators': 3},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 11,\n",
      "             'max_features': 5,\n",
      "             'n_estimators': 6},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 11,\n",
      "             'max_features': 5,\n",
      "             'n_estimators': 9},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 11,\n",
      "             'max_features': 5,\n",
      "             'n_estimators': 10},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 11,\n",
      "             'max_features': 5,\n",
      "             'n_estimators': 11},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 11,\n",
      "             'max_features': 7,\n",
      "             'n_estimators': 3},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 11,\n",
      "             'max_features': 7,\n",
      "             'n_estimators': 6},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 11,\n",
      "             'max_features': 7,\n",
      "             'n_estimators': 9},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 11,\n",
      "             'max_features': 7,\n",
      "             'n_estimators': 10},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 11,\n",
      "             'max_features': 7,\n",
      "             'n_estimators': 11},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 11,\n",
      "             'max_features': 9,\n",
      "             'n_estimators': 3},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 11,\n",
      "             'max_features': 9,\n",
      "             'n_estimators': 6},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 11,\n",
      "             'max_features': 9,\n",
      "             'n_estimators': 9},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 11,\n",
      "             'max_features': 9,\n",
      "             'n_estimators': 10},\n",
      "            {'criterion': 'gini',\n",
      "             'max_depth': 11,\n",
      "             'max_features': 9,\n",
      "             'n_estimators': 11},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 1,\n",
      "             'max_features': 1,\n",
      "             'n_estimators': 3},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 1,\n",
      "             'max_features': 1,\n",
      "             'n_estimators': 6},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 1,\n",
      "             'max_features': 1,\n",
      "             'n_estimators': 9},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 1,\n",
      "             'max_features': 1,\n",
      "             'n_estimators': 10},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 1,\n",
      "             'max_features': 1,\n",
      "             'n_estimators': 11},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 1,\n",
      "             'max_features': 3,\n",
      "             'n_estimators': 3},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 1,\n",
      "             'max_features': 3,\n",
      "             'n_estimators': 6},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 1,\n",
      "             'max_features': 3,\n",
      "             'n_estimators': 9},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 1,\n",
      "             'max_features': 3,\n",
      "             'n_estimators': 10},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 1,\n",
      "             'max_features': 3,\n",
      "             'n_estimators': 11},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 1,\n",
      "             'max_features': 5,\n",
      "             'n_estimators': 3},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 1,\n",
      "             'max_features': 5,\n",
      "             'n_estimators': 6},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 1,\n",
      "             'max_features': 5,\n",
      "             'n_estimators': 9},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 1,\n",
      "             'max_features': 5,\n",
      "             'n_estimators': 10},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 1,\n",
      "             'max_features': 5,\n",
      "             'n_estimators': 11},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 1,\n",
      "             'max_features': 7,\n",
      "             'n_estimators': 3},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 1,\n",
      "             'max_features': 7,\n",
      "             'n_estimators': 6},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 1,\n",
      "             'max_features': 7,\n",
      "             'n_estimators': 9},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 1,\n",
      "             'max_features': 7,\n",
      "             'n_estimators': 10},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 1,\n",
      "             'max_features': 7,\n",
      "             'n_estimators': 11},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 1,\n",
      "             'max_features': 9,\n",
      "             'n_estimators': 3},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 1,\n",
      "             'max_features': 9,\n",
      "             'n_estimators': 6},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 1,\n",
      "             'max_features': 9,\n",
      "             'n_estimators': 9},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 1,\n",
      "             'max_features': 9,\n",
      "             'n_estimators': 10},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 1,\n",
      "             'max_features': 9,\n",
      "             'n_estimators': 11},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 3,\n",
      "             'max_features': 1,\n",
      "             'n_estimators': 3},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 3,\n",
      "             'max_features': 1,\n",
      "             'n_estimators': 6},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 3,\n",
      "             'max_features': 1,\n",
      "             'n_estimators': 9},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 3,\n",
      "             'max_features': 1,\n",
      "             'n_estimators': 10},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 3,\n",
      "             'max_features': 1,\n",
      "             'n_estimators': 11},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 3,\n",
      "             'max_features': 3,\n",
      "             'n_estimators': 3},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 3,\n",
      "             'max_features': 3,\n",
      "             'n_estimators': 6},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 3,\n",
      "             'max_features': 3,\n",
      "             'n_estimators': 9},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 3,\n",
      "             'max_features': 3,\n",
      "             'n_estimators': 10},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 3,\n",
      "             'max_features': 3,\n",
      "             'n_estimators': 11},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 3,\n",
      "             'max_features': 5,\n",
      "             'n_estimators': 3},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 3,\n",
      "             'max_features': 5,\n",
      "             'n_estimators': 6},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 3,\n",
      "             'max_features': 5,\n",
      "             'n_estimators': 9},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 3,\n",
      "             'max_features': 5,\n",
      "             'n_estimators': 10},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 3,\n",
      "             'max_features': 5,\n",
      "             'n_estimators': 11},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 3,\n",
      "             'max_features': 7,\n",
      "             'n_estimators': 3},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 3,\n",
      "             'max_features': 7,\n",
      "             'n_estimators': 6},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 3,\n",
      "             'max_features': 7,\n",
      "             'n_estimators': 9},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 3,\n",
      "             'max_features': 7,\n",
      "             'n_estimators': 10},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 3,\n",
      "             'max_features': 7,\n",
      "             'n_estimators': 11},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 3,\n",
      "             'max_features': 9,\n",
      "             'n_estimators': 3},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 3,\n",
      "             'max_features': 9,\n",
      "             'n_estimators': 6},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 3,\n",
      "             'max_features': 9,\n",
      "             'n_estimators': 9},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 3,\n",
      "             'max_features': 9,\n",
      "             'n_estimators': 10},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 3,\n",
      "             'max_features': 9,\n",
      "             'n_estimators': 11},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 5,\n",
      "             'max_features': 1,\n",
      "             'n_estimators': 3},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 5,\n",
      "             'max_features': 1,\n",
      "             'n_estimators': 6},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 5,\n",
      "             'max_features': 1,\n",
      "             'n_estimators': 9},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 5,\n",
      "             'max_features': 1,\n",
      "             'n_estimators': 10},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 5,\n",
      "             'max_features': 1,\n",
      "             'n_estimators': 11},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 5,\n",
      "             'max_features': 3,\n",
      "             'n_estimators': 3},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 5,\n",
      "             'max_features': 3,\n",
      "             'n_estimators': 6},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 5,\n",
      "             'max_features': 3,\n",
      "             'n_estimators': 9},\n",
      "            {'criterion': 'entropy',\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             'max_depth': 5,\n",
      "             'max_features': 3,\n",
      "             'n_estimators': 10},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 5,\n",
      "             'max_features': 3,\n",
      "             'n_estimators': 11},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 5,\n",
      "             'max_features': 5,\n",
      "             'n_estimators': 3},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 5,\n",
      "             'max_features': 5,\n",
      "             'n_estimators': 6},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 5,\n",
      "             'max_features': 5,\n",
      "             'n_estimators': 9},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 5,\n",
      "             'max_features': 5,\n",
      "             'n_estimators': 10},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 5,\n",
      "             'max_features': 5,\n",
      "             'n_estimators': 11},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 5,\n",
      "             'max_features': 7,\n",
      "             'n_estimators': 3},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 5,\n",
      "             'max_features': 7,\n",
      "             'n_estimators': 6},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 5,\n",
      "             'max_features': 7,\n",
      "             'n_estimators': 9},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 5,\n",
      "             'max_features': 7,\n",
      "             'n_estimators': 10},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 5,\n",
      "             'max_features': 7,\n",
      "             'n_estimators': 11},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 5,\n",
      "             'max_features': 9,\n",
      "             'n_estimators': 3},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 5,\n",
      "             'max_features': 9,\n",
      "             'n_estimators': 6},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 5,\n",
      "             'max_features': 9,\n",
      "             'n_estimators': 9},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 5,\n",
      "             'max_features': 9,\n",
      "             'n_estimators': 10},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 5,\n",
      "             'max_features': 9,\n",
      "             'n_estimators': 11},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 7,\n",
      "             'max_features': 1,\n",
      "             'n_estimators': 3},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 7,\n",
      "             'max_features': 1,\n",
      "             'n_estimators': 6},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 7,\n",
      "             'max_features': 1,\n",
      "             'n_estimators': 9},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 7,\n",
      "             'max_features': 1,\n",
      "             'n_estimators': 10},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 7,\n",
      "             'max_features': 1,\n",
      "             'n_estimators': 11},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 7,\n",
      "             'max_features': 3,\n",
      "             'n_estimators': 3},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 7,\n",
      "             'max_features': 3,\n",
      "             'n_estimators': 6},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 7,\n",
      "             'max_features': 3,\n",
      "             'n_estimators': 9},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 7,\n",
      "             'max_features': 3,\n",
      "             'n_estimators': 10},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 7,\n",
      "             'max_features': 3,\n",
      "             'n_estimators': 11},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 7,\n",
      "             'max_features': 5,\n",
      "             'n_estimators': 3},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 7,\n",
      "             'max_features': 5,\n",
      "             'n_estimators': 6},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 7,\n",
      "             'max_features': 5,\n",
      "             'n_estimators': 9},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 7,\n",
      "             'max_features': 5,\n",
      "             'n_estimators': 10},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 7,\n",
      "             'max_features': 5,\n",
      "             'n_estimators': 11},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 7,\n",
      "             'max_features': 7,\n",
      "             'n_estimators': 3},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 7,\n",
      "             'max_features': 7,\n",
      "             'n_estimators': 6},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 7,\n",
      "             'max_features': 7,\n",
      "             'n_estimators': 9},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 7,\n",
      "             'max_features': 7,\n",
      "             'n_estimators': 10},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 7,\n",
      "             'max_features': 7,\n",
      "             'n_estimators': 11},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 7,\n",
      "             'max_features': 9,\n",
      "             'n_estimators': 3},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 7,\n",
      "             'max_features': 9,\n",
      "             'n_estimators': 6},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 7,\n",
      "             'max_features': 9,\n",
      "             'n_estimators': 9},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 7,\n",
      "             'max_features': 9,\n",
      "             'n_estimators': 10},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 7,\n",
      "             'max_features': 9,\n",
      "             'n_estimators': 11},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 9,\n",
      "             'max_features': 1,\n",
      "             'n_estimators': 3},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 9,\n",
      "             'max_features': 1,\n",
      "             'n_estimators': 6},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 9,\n",
      "             'max_features': 1,\n",
      "             'n_estimators': 9},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 9,\n",
      "             'max_features': 1,\n",
      "             'n_estimators': 10},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 9,\n",
      "             'max_features': 1,\n",
      "             'n_estimators': 11},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 9,\n",
      "             'max_features': 3,\n",
      "             'n_estimators': 3},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 9,\n",
      "             'max_features': 3,\n",
      "             'n_estimators': 6},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 9,\n",
      "             'max_features': 3,\n",
      "             'n_estimators': 9},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 9,\n",
      "             'max_features': 3,\n",
      "             'n_estimators': 10},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 9,\n",
      "             'max_features': 3,\n",
      "             'n_estimators': 11},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 9,\n",
      "             'max_features': 5,\n",
      "             'n_estimators': 3},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 9,\n",
      "             'max_features': 5,\n",
      "             'n_estimators': 6},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 9,\n",
      "             'max_features': 5,\n",
      "             'n_estimators': 9},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 9,\n",
      "             'max_features': 5,\n",
      "             'n_estimators': 10},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 9,\n",
      "             'max_features': 5,\n",
      "             'n_estimators': 11},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 9,\n",
      "             'max_features': 7,\n",
      "             'n_estimators': 3},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 9,\n",
      "             'max_features': 7,\n",
      "             'n_estimators': 6},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 9,\n",
      "             'max_features': 7,\n",
      "             'n_estimators': 9},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 9,\n",
      "             'max_features': 7,\n",
      "             'n_estimators': 10},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 9,\n",
      "             'max_features': 7,\n",
      "             'n_estimators': 11},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 9,\n",
      "             'max_features': 9,\n",
      "             'n_estimators': 3},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 9,\n",
      "             'max_features': 9,\n",
      "             'n_estimators': 6},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 9,\n",
      "             'max_features': 9,\n",
      "             'n_estimators': 9},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 9,\n",
      "             'max_features': 9,\n",
      "             'n_estimators': 10},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 9,\n",
      "             'max_features': 9,\n",
      "             'n_estimators': 11},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 11,\n",
      "             'max_features': 1,\n",
      "             'n_estimators': 3},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 11,\n",
      "             'max_features': 1,\n",
      "             'n_estimators': 6},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 11,\n",
      "             'max_features': 1,\n",
      "             'n_estimators': 9},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 11,\n",
      "             'max_features': 1,\n",
      "             'n_estimators': 10},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 11,\n",
      "             'max_features': 1,\n",
      "             'n_estimators': 11},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 11,\n",
      "             'max_features': 3,\n",
      "             'n_estimators': 3},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 11,\n",
      "             'max_features': 3,\n",
      "             'n_estimators': 6},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 11,\n",
      "             'max_features': 3,\n",
      "             'n_estimators': 9},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 11,\n",
      "             'max_features': 3,\n",
      "             'n_estimators': 10},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 11,\n",
      "             'max_features': 3,\n",
      "             'n_estimators': 11},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 11,\n",
      "             'max_features': 5,\n",
      "             'n_estimators': 3},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 11,\n",
      "             'max_features': 5,\n",
      "             'n_estimators': 6},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 11,\n",
      "             'max_features': 5,\n",
      "             'n_estimators': 9},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 11,\n",
      "             'max_features': 5,\n",
      "             'n_estimators': 10},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 11,\n",
      "             'max_features': 5,\n",
      "             'n_estimators': 11},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 11,\n",
      "             'max_features': 7,\n",
      "             'n_estimators': 3},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 11,\n",
      "             'max_features': 7,\n",
      "             'n_estimators': 6},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 11,\n",
      "             'max_features': 7,\n",
      "             'n_estimators': 9},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 11,\n",
      "             'max_features': 7,\n",
      "             'n_estimators': 10},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 11,\n",
      "             'max_features': 7,\n",
      "             'n_estimators': 11},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 11,\n",
      "             'max_features': 9,\n",
      "             'n_estimators': 3},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 11,\n",
      "             'max_features': 9,\n",
      "             'n_estimators': 6},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 11,\n",
      "             'max_features': 9,\n",
      "             'n_estimators': 9},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 11,\n",
      "             'max_features': 9,\n",
      "             'n_estimators': 10},\n",
      "            {'criterion': 'entropy',\n",
      "             'max_depth': 11,\n",
      "             'max_features': 9,\n",
      "             'n_estimators': 11}],\n",
      " 'rank_test_score': array([300, 297, 298, 299, 290, 269, 282, 295, 279, 294, 277, 293, 283,\n",
      "       281, 286, 287, 288, 284, 280, 296, 273, 292, 289, 285, 275, 274,\n",
      "       264, 222, 268, 270, 271, 255, 233, 267, 242, 254, 235, 248, 231,\n",
      "       257, 256, 246, 251, 243, 241, 262, 225, 244, 238, 236, 229, 208,\n",
      "       215, 203, 213, 195, 194, 200, 168, 191, 189, 183, 163, 150, 156,\n",
      "       199, 152, 161, 165, 144, 187, 164, 178, 153, 151, 193, 179, 159,\n",
      "       158, 173, 143, 133, 115, 117,  94, 125, 114,  93,  71,  83, 121,\n",
      "       106,  98,  87,  76, 123, 108, 107,  78,  59, 145, 137, 122, 119,\n",
      "       111, 116,  68,  48,  36,  43,  95,  67,  45,  15,  29,  74,  55,\n",
      "        37,  30,  34,  90,  46,  38,  11,  31, 127, 120,  82,  75,  62,\n",
      "        91,  61,  18,  22,  24,  70,  65,   5,  10,   6,  80,  60,  16,\n",
      "        33,   3,  63,  49,   9,  35,   4, 276, 272, 259, 291, 278, 210,\n",
      "       209, 214, 216, 205, 142, 198, 197, 218, 167, 132, 146, 185, 204,\n",
      "       149, 134, 176, 157, 147, 139, 258, 265, 227, 228, 261, 237, 266,\n",
      "       250, 252, 234, 263, 239, 247, 245, 260, 226, 224, 249, 230, 223,\n",
      "       232, 253, 220, 217, 221, 240, 211, 219, 207, 212, 202, 192, 182,\n",
      "       177, 181, 206, 190, 155, 184, 170, 175, 188, 174, 172, 166, 196,\n",
      "       180, 169, 186, 171, 201, 160, 154, 140, 148, 141, 129, 126, 102,\n",
      "       103, 135,  99, 109, 110,  81, 130, 113,  85,  84,  64, 131, 118,\n",
      "       112,  89, 100, 162, 138, 128, 124, 104, 101,  56,  47,  44,  39,\n",
      "       105,  57,  42,  28,  25,  86,  58,  51,  32,  23,  92,  66,  41,\n",
      "        54,  53, 136,  97,  77,  69,  72,  96,  50,  14,  19,   7,  79,\n",
      "        40,  26,  12,   2,  88,  27,   8,  13,   1,  73,  52,  17,  21,\n",
      "        20]),\n",
      " 'split0_test_score': array([0.92216687, 0.9128269 , 0.82627646, 0.71668742, 0.88293898,\n",
      "       0.89227895, 0.86674969, 0.88854296, 0.89227895, 0.87671233,\n",
      "       0.89912827, 0.86550436, 0.87858032, 0.8879203 , 0.87920299,\n",
      "       0.89227895, 0.84806974, 0.8611457 , 0.85740971, 0.85740971,\n",
      "       0.89912827, 0.86612702, 0.89912827, 0.89912827, 0.89912827,\n",
      "       0.9003736 , 0.89290162, 0.92216687, 0.89290162, 0.89414695,\n",
      "       0.9128269 , 0.89290162, 0.91967621, 0.90161893, 0.90099626,\n",
      "       0.90597758, 0.91656289, 0.90722291, 0.91594022, 0.9128269 ,\n",
      "       0.90784558, 0.91594022, 0.90535492, 0.90722291, 0.90784558,\n",
      "       0.91344956, 0.92154421, 0.90909091, 0.91594022, 0.91531756,\n",
      "       0.91780822, 0.94084682, 0.94022416, 0.93960149, 0.93835616,\n",
      "       0.93897883, 0.93711083, 0.92590286, 0.94582814, 0.92590286,\n",
      "       0.9358655 , 0.93088418, 0.94396015, 0.94645081, 0.94894147,\n",
      "       0.93711083, 0.95205479, 0.94956413, 0.9358655 , 0.95392279,\n",
      "       0.94084682, 0.94582814, 0.93026152, 0.94022416, 0.94769614,\n",
      "       0.94769614, 0.94645081, 0.94707347, 0.94520548, 0.93960149,\n",
      "       0.95267746, 0.9626401 , 0.96201743, 0.96326276, 0.96575342,\n",
      "       0.96513076, 0.9626401 , 0.96699875, 0.97447073, 0.96824408,\n",
      "       0.95516812, 0.96450809, 0.96388543, 0.97073474, 0.96886675,\n",
      "       0.95765878, 0.96762142, 0.96139477, 0.97198007, 0.97384807,\n",
      "       0.95703611, 0.95516812, 0.95516812, 0.96388543, 0.96326276,\n",
      "       0.95641345, 0.96699875, 0.97882939, 0.9732254 , 0.97820672,\n",
      "       0.9732254 , 0.96886675, 0.97758406, 0.98069738, 0.97882939,\n",
      "       0.97260274, 0.97571606, 0.97758406, 0.9732254 , 0.97882939,\n",
      "       0.96450809, 0.97447073, 0.97882939, 0.98132005, 0.97758406,\n",
      "       0.95392279, 0.96014944, 0.96948941, 0.97135741, 0.97571606,\n",
      "       0.96948941, 0.96886675, 0.97945205, 0.98381071, 0.97758406,\n",
      "       0.97198007, 0.97011208, 0.97882939, 0.98318804, 0.98132005,\n",
      "       0.96948941, 0.97135741, 0.97945205, 0.97384807, 0.98318804,\n",
      "       0.96513076, 0.97073474, 0.98132005, 0.97198007, 0.98318804,\n",
      "       0.90660025, 0.91033624, 0.92590286, 0.82067248, 0.92839352,\n",
      "       0.9501868 , 0.95330012, 0.90224159, 0.91967621, 0.93960149,\n",
      "       0.94582814, 0.94458281, 0.92590286, 0.90161893, 0.93399751,\n",
      "       0.94084682, 0.95143213, 0.95143213, 0.93711083, 0.95205479,\n",
      "       0.96014944, 0.94022416, 0.95205479, 0.9501868 , 0.94645081,\n",
      "       0.85118306, 0.92465753, 0.92839352, 0.90846824, 0.91905355,\n",
      "       0.91656289, 0.90224159, 0.91033624, 0.90784558, 0.93275218,\n",
      "       0.92963885, 0.90286426, 0.91344956, 0.92839352, 0.91158157,\n",
      "       0.92029888, 0.91469489, 0.91967621, 0.92092154, 0.92216687,\n",
      "       0.90535492, 0.90410959, 0.90971357, 0.92839352, 0.91967621,\n",
      "       0.89912827, 0.93835616, 0.92465753, 0.92777086, 0.94271482,\n",
      "       0.93960149, 0.94271482, 0.9483188 , 0.94022416, 0.94645081,\n",
      "       0.91656289, 0.93648817, 0.9483188 , 0.94022416, 0.94396015,\n",
      "       0.93150685, 0.93835616, 0.94520548, 0.94645081, 0.93897883,\n",
      "       0.94520548, 0.9377335 , 0.94022416, 0.93835616, 0.93835616,\n",
      "       0.93026152, 0.95143213, 0.94956413, 0.94271482, 0.94894147,\n",
      "       0.9626401 , 0.9607721 , 0.96450809, 0.96139477, 0.96326276,\n",
      "       0.95454545, 0.97011208, 0.96014944, 0.96948941, 0.96014944,\n",
      "       0.96637609, 0.95890411, 0.97011208, 0.97260274, 0.96762142,\n",
      "       0.95828144, 0.96575342, 0.96513076, 0.96575342, 0.96388543,\n",
      "       0.94520548, 0.96326276, 0.95392279, 0.95890411, 0.96699875,\n",
      "       0.96201743, 0.96886675, 0.97758406, 0.97447073, 0.97882939,\n",
      "       0.9607721 , 0.97073474, 0.98007472, 0.97820672, 0.97882939,\n",
      "       0.96513076, 0.96637609, 0.9750934 , 0.97571606, 0.98069738,\n",
      "       0.96824408, 0.97011208, 0.9732254 , 0.97260274, 0.96948941,\n",
      "       0.95516812, 0.96388543, 0.96762142, 0.96948941, 0.96513076,\n",
      "       0.96824408, 0.97571606, 0.98132005, 0.98132005, 0.97882939,\n",
      "       0.97384807, 0.97758406, 0.97820672, 0.97820672, 0.98132005,\n",
      "       0.97198007, 0.97758406, 0.98443337, 0.98194271, 0.98194271,\n",
      "       0.97447073, 0.9750934 , 0.97945205, 0.98007472, 0.97945205]),\n",
      " 'split0_train_score': array([0.92028647, 0.92884945, 0.85193835, 0.75136229, 0.89755566,\n",
      "       0.91032228, 0.87965125, 0.90066947, 0.90923245, 0.89226218,\n",
      "       0.90969952, 0.88260937, 0.89989102, 0.90814261, 0.89397478,\n",
      "       0.91032228, 0.86704032, 0.88042971, 0.87544761, 0.87544761,\n",
      "       0.90969952, 0.87840573, 0.90969952, 0.90969952, 0.90969952,\n",
      "       0.89989102, 0.90845399, 0.92931652, 0.90907676, 0.90051378,\n",
      "       0.91841818, 0.90471742, 0.93974778, 0.92199907, 0.92106492,\n",
      "       0.92604702, 0.93274171, 0.9311848 , 0.93352016, 0.92900514,\n",
      "       0.92340028, 0.93367585, 0.93461   , 0.93258602, 0.92729254,\n",
      "       0.9328974 , 0.9427059 , 0.93196326, 0.93071773, 0.93429861,\n",
      "       0.94255021, 0.95438269, 0.95360423, 0.95531683, 0.94846645,\n",
      "       0.95609528, 0.95687374, 0.95126888, 0.96341274, 0.94301728,\n",
      "       0.9509575 , 0.95687374, 0.96014324, 0.96294566, 0.960766  ,\n",
      "       0.95593959, 0.96699362, 0.96574809, 0.954227  , 0.9655924 ,\n",
      "       0.95640666, 0.95983185, 0.95874202, 0.96092169, 0.96372412,\n",
      "       0.96061031, 0.96512533, 0.96434688, 0.96512533, 0.96092169,\n",
      "       0.97228709, 0.98022731, 0.98287405, 0.98131714, 0.98458664,\n",
      "       0.97835902, 0.97913747, 0.98380819, 0.98676631, 0.98645493,\n",
      "       0.98022731, 0.98645493, 0.98334112, 0.98894598, 0.98458664,\n",
      "       0.98069438, 0.98676631, 0.98458664, 0.986922  , 0.98941305,\n",
      "       0.97820333, 0.98505371, 0.980383  , 0.98598786, 0.98801183,\n",
      "       0.98614355, 0.99237117, 0.99299393, 0.99330531, 0.99314962,\n",
      "       0.98972443, 0.993461  , 0.99626343, 0.99750895, 0.99579636,\n",
      "       0.99517359, 0.99455083, 0.99626343, 0.99564067, 0.99719757,\n",
      "       0.99205979, 0.993461  , 0.99610774, 0.9967305 , 0.99657481,\n",
      "       0.98567648, 0.9901915 , 0.99408376, 0.993461  , 0.99626343,\n",
      "       0.9917484 , 0.99626343, 0.99782033, 0.99797602, 0.9982874 ,\n",
      "       0.99470652, 0.99704188, 0.99891017, 0.99891017, 0.99891017,\n",
      "       0.99626343, 0.99486221, 0.99875448, 0.9984431 , 0.99937724,\n",
      "       0.99330531, 0.99750895, 0.9984431 , 0.9982874 , 0.99953293,\n",
      "       0.91421454, 0.92900514, 0.9313405 , 0.82251284, 0.93990347,\n",
      "       0.96216721, 0.96855052, 0.91187918, 0.93367585, 0.95313716,\n",
      "       0.94862214, 0.96341274, 0.94410712, 0.9213763 , 0.95064612,\n",
      "       0.95609528, 0.96528102, 0.96699362, 0.95702943, 0.96777207,\n",
      "       0.97337693, 0.95765219, 0.96792776, 0.96605947, 0.96419119,\n",
      "       0.85894442, 0.94161607, 0.93974778, 0.91639421, 0.92775961,\n",
      "       0.9344543 , 0.91530437, 0.92822669, 0.92884945, 0.94675385,\n",
      "       0.94581971, 0.92495719, 0.93398723, 0.94566402, 0.92666978,\n",
      "       0.94083761, 0.93492138, 0.93772381, 0.94021485, 0.94457419,\n",
      "       0.9296279 , 0.9296279 , 0.93243033, 0.94690954, 0.93865795,\n",
      "       0.91623852, 0.95282578, 0.93912502, 0.94457419, 0.95858633,\n",
      "       0.95360423, 0.95702943, 0.96278997, 0.95609528, 0.95811926,\n",
      "       0.94301728, 0.95858633, 0.96029893, 0.95780788, 0.95765219,\n",
      "       0.95204733, 0.9590534 , 0.95796357, 0.96092169, 0.95765219,\n",
      "       0.95936478, 0.95640666, 0.95858633, 0.96061031, 0.95453838,\n",
      "       0.95516114, 0.96247859, 0.96652655, 0.96185583, 0.96839483,\n",
      "       0.97617936, 0.97851471, 0.98022731, 0.97804764, 0.97944886,\n",
      "       0.96995174, 0.98536509, 0.9836525 , 0.98629924, 0.98443095,\n",
      "       0.97898178, 0.97929317, 0.98723338, 0.98738907, 0.98256267,\n",
      "       0.97508952, 0.98489802, 0.98287405, 0.98723338, 0.98302974,\n",
      "       0.97446676, 0.98567648, 0.98022731, 0.98318543, 0.98567648,\n",
      "       0.97882609, 0.99361669, 0.99486221, 0.99034719, 0.9967305 ,\n",
      "       0.98614355, 0.99330531, 0.99564067, 0.99532929, 0.99470652,\n",
      "       0.98816752, 0.99143702, 0.9950179 , 0.99532929, 0.99579636,\n",
      "       0.993461  , 0.99408376, 0.99377238, 0.99439514, 0.99579636,\n",
      "       0.98271836, 0.99096995, 0.99299393, 0.99314962, 0.9901915 ,\n",
      "       0.99252686, 0.99486221, 0.99891017, 0.99750895, 0.9967305 ,\n",
      "       0.99377238, 0.99719757, 0.9984431 , 0.99750895, 0.9984431 ,\n",
      "       0.99283824, 0.99626343, 0.99922155, 0.99875448, 0.99922155,\n",
      "       0.99143702, 0.99564067, 0.99859879, 0.9982874 , 0.99735326]),\n",
      " 'split1_test_score': array([0.86986301, 0.85554172, 0.89912827, 0.84371108, 0.89788294,\n",
      "       0.9501868 , 0.92714819, 0.89041096, 0.91718555, 0.90660025,\n",
      "       0.91158157, 0.91158157, 0.90660025, 0.91718555, 0.88293898,\n",
      "       0.91594022, 0.91158157, 0.91158157, 0.91158157, 0.91158157,\n",
      "       0.91158157, 0.91158157, 0.88356164, 0.91158157, 0.91158157,\n",
      "       0.87982565, 0.90535492, 0.92590286, 0.92278954, 0.92839352,\n",
      "       0.92839352, 0.92154421, 0.93337484, 0.92963885, 0.93399751,\n",
      "       0.93835616, 0.93462017, 0.92901619, 0.93337484, 0.92652553,\n",
      "       0.94209215, 0.94520548, 0.93524284, 0.93897883, 0.93960149,\n",
      "       0.94956413, 0.93711083, 0.93088418, 0.92714819, 0.94582814,\n",
      "       0.93399751, 0.94582814, 0.94520548, 0.9483188 , 0.94333748,\n",
      "       0.95392279, 0.95392279, 0.96139477, 0.9607721 , 0.95765878,\n",
      "       0.9626401 , 0.96699875, 0.96513076, 0.96388543, 0.9607721 ,\n",
      "       0.95330012, 0.96513076, 0.96637609, 0.96450809, 0.96326276,\n",
      "       0.9626401 , 0.96326276, 0.96886675, 0.96139477, 0.96201743,\n",
      "       0.94894147, 0.96450809, 0.96014944, 0.96513076, 0.9607721 ,\n",
      "       0.95641345, 0.9750934 , 0.97073474, 0.9732254 , 0.97384807,\n",
      "       0.97198007, 0.97571606, 0.97882939, 0.97820672, 0.98256538,\n",
      "       0.97260274, 0.97447073, 0.97571606, 0.97758406, 0.97820672,\n",
      "       0.97571606, 0.97882939, 0.97758406, 0.98194271, 0.97882939,\n",
      "       0.95952677, 0.96699875, 0.96762142, 0.97198007, 0.97758406,\n",
      "       0.97820672, 0.97882939, 0.98194271, 0.98879203, 0.98381071,\n",
      "       0.96886675, 0.97882939, 0.98318804, 0.98381071, 0.9856787 ,\n",
      "       0.97447073, 0.98069738, 0.98505604, 0.9856787 , 0.98256538,\n",
      "       0.97945205, 0.98256538, 0.98381071, 0.98505604, 0.98505604,\n",
      "       0.96948941, 0.97260274, 0.97820672, 0.9750934 , 0.97135741,\n",
      "       0.9732254 , 0.98069738, 0.98443337, 0.98256538, 0.98692403,\n",
      "       0.97571606, 0.97945205, 0.98941469, 0.98318804, 0.98256538,\n",
      "       0.97696139, 0.97882939, 0.98069738, 0.98381071, 0.9856787 ,\n",
      "       0.97945205, 0.97758406, 0.98132005, 0.98381071, 0.98505604,\n",
      "       0.86924035, 0.91220423, 0.91158157, 0.89103362, 0.87235367,\n",
      "       0.96824408, 0.94333748, 0.96699875, 0.96014944, 0.93150685,\n",
      "       0.96575342, 0.95454545, 0.96139477, 0.95392279, 0.96014944,\n",
      "       0.97260274, 0.96388543, 0.95890411, 0.95143213, 0.95952677,\n",
      "       0.96388543, 0.95205479, 0.96388543, 0.96388543, 0.96388543,\n",
      "       0.94956413, 0.90473225, 0.93088418, 0.95267746, 0.92403487,\n",
      "       0.93026152, 0.92652553, 0.92901619, 0.92901619, 0.91843088,\n",
      "       0.92029888, 0.94146949, 0.92278954, 0.91656289, 0.92652553,\n",
      "       0.92963885, 0.92963885, 0.91407223, 0.94333748, 0.94022416,\n",
      "       0.93026152, 0.93960149, 0.95205479, 0.94894147, 0.94520548,\n",
      "       0.93026152, 0.94769614, 0.9358655 , 0.94645081, 0.9483188 ,\n",
      "       0.94956413, 0.95828144, 0.95828144, 0.95828144, 0.95205479,\n",
      "       0.95392279, 0.95392279, 0.96450809, 0.9607721 , 0.96513076,\n",
      "       0.96388543, 0.96014944, 0.96326276, 0.96201743, 0.96699875,\n",
      "       0.96388543, 0.97011208, 0.9607721 , 0.96014944, 0.95890411,\n",
      "       0.95392279, 0.95579078, 0.9732254 , 0.96886675, 0.96326276,\n",
      "       0.95765878, 0.96948941, 0.96948941, 0.97882939, 0.97758406,\n",
      "       0.9732254 , 0.97882939, 0.98007472, 0.97571606, 0.98256538,\n",
      "       0.97820672, 0.98256538, 0.98007472, 0.97758406, 0.98505604,\n",
      "       0.9750934 , 0.97696139, 0.98069738, 0.98256538, 0.97820672,\n",
      "       0.96513076, 0.97198007, 0.97447073, 0.97073474, 0.97820672,\n",
      "       0.97384807, 0.98505604, 0.97882939, 0.98069738, 0.98256538,\n",
      "       0.98194271, 0.98069738, 0.98381071, 0.9875467 , 0.9856787 ,\n",
      "       0.97758406, 0.98194271, 0.98007472, 0.9875467 , 0.98381071,\n",
      "       0.97882939, 0.98007472, 0.9856787 , 0.98381071, 0.98630137,\n",
      "       0.96824408, 0.98069738, 0.98194271, 0.97571606, 0.9732254 ,\n",
      "       0.98132005, 0.98007472, 0.98505604, 0.9875467 , 0.98879203,\n",
      "       0.97571606, 0.98505604, 0.98816936, 0.98630137, 0.99003736,\n",
      "       0.97882939, 0.98318804, 0.98692403, 0.98256538, 0.98941469,\n",
      "       0.97820672, 0.98069738, 0.98381071, 0.98630137, 0.98256538]),\n",
      " 'split1_train_score': array([0.87435778, 0.87264518, 0.89335202, 0.85209404, 0.89989102,\n",
      "       0.95718512, 0.92044216, 0.87933987, 0.9050288 , 0.89646583,\n",
      "       0.90658571, 0.90658571, 0.8984898 , 0.90892106, 0.87311225,\n",
      "       0.91078935, 0.90658571, 0.90658571, 0.90658571, 0.90643002,\n",
      "       0.90658571, 0.90658571, 0.8740464 , 0.90658571, 0.90658571,\n",
      "       0.87326794, 0.90331621, 0.91966371, 0.91577145, 0.91919664,\n",
      "       0.92340028, 0.91514868, 0.9230889 , 0.92028647, 0.92744823,\n",
      "       0.93803519, 0.93476569, 0.91530437, 0.92231045, 0.92293321,\n",
      "       0.93274171, 0.94161607, 0.93025066, 0.93227464, 0.93211895,\n",
      "       0.95360423, 0.92666978, 0.92916083, 0.92277752, 0.93694535,\n",
      "       0.92729254, 0.94239452, 0.94722092, 0.94255021, 0.94675385,\n",
      "       0.95064612, 0.95656235, 0.96045462, 0.95983185, 0.95952047,\n",
      "       0.96528102, 0.96496964, 0.96496964, 0.96496964, 0.96356843,\n",
      "       0.95531683, 0.96123307, 0.96683793, 0.9640355 , 0.96465826,\n",
      "       0.96325704, 0.96714931, 0.97228709, 0.96683793, 0.96917328,\n",
      "       0.94893352, 0.97104157, 0.96310135, 0.97275416, 0.96543671,\n",
      "       0.97073019, 0.97882609, 0.97991593, 0.97913747, 0.9819399 ,\n",
      "       0.97524521, 0.98271836, 0.98723338, 0.98598786, 0.98832321,\n",
      "       0.98100576, 0.98661062, 0.98552078, 0.98801183, 0.98894598,\n",
      "       0.980383  , 0.98505371, 0.98614355, 0.98801183, 0.98598786,\n",
      "       0.9705745 , 0.97929317, 0.97991593, 0.98458664, 0.98552078,\n",
      "       0.98879028, 0.99128133, 0.99626343, 0.99579636, 0.99470652,\n",
      "       0.98770045, 0.99361669, 0.99548498, 0.9967305 , 0.99470652,\n",
      "       0.98832321, 0.99439514, 0.9967305 , 0.9950179 , 0.99719757,\n",
      "       0.9917484 , 0.99517359, 0.99641912, 0.99517359, 0.9967305 ,\n",
      "       0.98707769, 0.99330531, 0.99423945, 0.99470652, 0.99486221,\n",
      "       0.99330531, 0.99470652, 0.99891017, 0.99859879, 0.99859879,\n",
      "       0.99486221, 0.99688619, 0.99797602, 0.99922155, 0.99922155,\n",
      "       0.99423945, 0.99766464, 0.99922155, 0.99891017, 0.99906586,\n",
      "       0.99517359, 0.99766464, 0.99813171, 0.9982874 , 0.99984431,\n",
      "       0.85847735, 0.91078935, 0.91110073, 0.88525611, 0.8673517 ,\n",
      "       0.96528102, 0.941149  , 0.96434688, 0.95516114, 0.92480149,\n",
      "       0.96450257, 0.95189164, 0.95874202, 0.94799938, 0.95827495,\n",
      "       0.96777207, 0.96278997, 0.95796357, 0.94908921, 0.95765219,\n",
      "       0.96278997, 0.94410712, 0.96278997, 0.96247859, 0.96278997,\n",
      "       0.93678966, 0.90705278, 0.92090923, 0.95049043, 0.91437023,\n",
      "       0.9263584 , 0.92542426, 0.92153199, 0.92246614, 0.91187918,\n",
      "       0.91997509, 0.93414292, 0.91701697, 0.90923245, 0.92262183,\n",
      "       0.92931652, 0.92090923, 0.91219057, 0.93803519, 0.92931652,\n",
      "       0.92511288, 0.93056204, 0.94753231, 0.94037054, 0.93414292,\n",
      "       0.9313405 , 0.94426281, 0.9313405 , 0.94488557, 0.94472988,\n",
      "       0.94192745, 0.95827495, 0.95811926, 0.9574965 , 0.95204733,\n",
      "       0.94628678, 0.95360423, 0.96247859, 0.95656235, 0.96154445,\n",
      "       0.96465826, 0.96263428, 0.96216721, 0.96263428, 0.9640355 ,\n",
      "       0.9574965 , 0.96481395, 0.95843064, 0.96123307, 0.95843064,\n",
      "       0.9509575 , 0.96029893, 0.97197571, 0.97259847, 0.96637085,\n",
      "       0.9688619 , 0.97617936, 0.97649074, 0.98489802, 0.98614355,\n",
      "       0.97524521, 0.97898178, 0.97913747, 0.98661062, 0.98474233,\n",
      "       0.98022731, 0.98225128, 0.9901915 , 0.98536509, 0.98910167,\n",
      "       0.97976024, 0.98396388, 0.98271836, 0.986922  , 0.9852094 ,\n",
      "       0.97306555, 0.98349681, 0.98676631, 0.98069438, 0.98723338,\n",
      "       0.98598786, 0.99237117, 0.99221548, 0.99283824, 0.99439514,\n",
      "       0.99096995, 0.99050288, 0.99626343, 0.99579636, 0.99641912,\n",
      "       0.98661062, 0.99252686, 0.993461  , 0.99641912, 0.99564067,\n",
      "       0.98801183, 0.993461  , 0.99641912, 0.99486221, 0.99688619,\n",
      "       0.98302974, 0.99268255, 0.99704188, 0.99486221, 0.99314962,\n",
      "       0.98956874, 0.99470652, 0.99719757, 0.99782033, 0.99813171,\n",
      "       0.99112564, 0.99564067, 0.99813171, 0.9984431 , 0.99906586,\n",
      "       0.99205979, 0.99766464, 0.99922155, 0.99859879, 0.99891017,\n",
      "       0.9901915 , 0.99704188, 0.99688619, 0.99906586, 0.99750895]),\n",
      " 'split2_test_score': array([0.84682441, 0.80448319, 0.89165629, 0.89601494, 0.90784558,\n",
      "       0.90722291, 0.91967621, 0.88916563, 0.89103362, 0.88729763,\n",
      "       0.8860523 , 0.90909091, 0.90660025, 0.89788294, 0.90909091,\n",
      "       0.88667497, 0.90909091, 0.89352428, 0.91344956, 0.87048568,\n",
      "       0.90909091, 0.90909091, 0.89227895, 0.90909091, 0.90161893,\n",
      "       0.89788294, 0.93462017, 0.92590286, 0.91780822, 0.91158157,\n",
      "       0.90348692, 0.92839352, 0.92465753, 0.91594022, 0.92777086,\n",
      "       0.91469489, 0.9252802 , 0.91780822, 0.92590286, 0.90971357,\n",
      "       0.89788294, 0.91407223, 0.9234122 , 0.92029888, 0.91780822,\n",
      "       0.90410959, 0.91905355, 0.92216687, 0.91469489, 0.91967621,\n",
      "       0.91967621, 0.93088418, 0.92963885, 0.92652553, 0.9252802 ,\n",
      "       0.94333748, 0.93897883, 0.94146949, 0.9483188 , 0.9501868 ,\n",
      "       0.9483188 , 0.94458281, 0.94894147, 0.95516812, 0.9483188 ,\n",
      "       0.93462017, 0.94582814, 0.94458281, 0.94769614, 0.95952677,\n",
      "       0.9483188 , 0.9483188 , 0.94956413, 0.95579078, 0.95454545,\n",
      "       0.94894147, 0.9483188 , 0.94956413, 0.94769614, 0.9483188 ,\n",
      "       0.95641345, 0.95703611, 0.96824408, 0.95890411, 0.97011208,\n",
      "       0.96139477, 0.9607721 , 0.96886675, 0.96886675, 0.96513076,\n",
      "       0.96762142, 0.96513076, 0.96575342, 0.96762142, 0.97260274,\n",
      "       0.96139477, 0.96762142, 0.96637609, 0.96948941, 0.97696139,\n",
      "       0.96388543, 0.96326276, 0.96762142, 0.96824408, 0.96575342,\n",
      "       0.96886675, 0.98318804, 0.9750934 , 0.98007472, 0.98069738,\n",
      "       0.97633873, 0.97633873, 0.97820672, 0.98256538, 0.98069738,\n",
      "       0.9732254 , 0.97820672, 0.97820672, 0.98630137, 0.98443337,\n",
      "       0.97135741, 0.97758406, 0.98256538, 0.98318804, 0.98194271,\n",
      "       0.97633873, 0.97571606, 0.96699875, 0.97633873, 0.97945205,\n",
      "       0.97011208, 0.98007472, 0.98879203, 0.98443337, 0.98194271,\n",
      "       0.98007472, 0.97696139, 0.98816936, 0.98256538, 0.9875467 ,\n",
      "       0.97135741, 0.98256538, 0.9856787 , 0.98505604, 0.98816936,\n",
      "       0.97820672, 0.98443337, 0.98692403, 0.98505604, 0.9856787 ,\n",
      "       0.91594022, 0.93337484, 0.90410959, 0.91594022, 0.9128269 ,\n",
      "       0.93524284, 0.93275218, 0.9377335 , 0.93711083, 0.95143213,\n",
      "       0.96014944, 0.91407223, 0.93835616, 0.91843088, 0.96388543,\n",
      "       0.97135741, 0.97447073, 0.95143213, 0.94582814, 0.94707347,\n",
      "       0.97447073, 0.94956413, 0.95828144, 0.97447073, 0.97447073,\n",
      "       0.9377335 , 0.89352428, 0.93960149, 0.9358655 , 0.91656289,\n",
      "       0.90660025, 0.93337484, 0.91220423, 0.9358655 , 0.92652553,\n",
      "       0.90099626, 0.9252802 , 0.93026152, 0.92403487, 0.89912827,\n",
      "       0.93026152, 0.92963885, 0.9109589 , 0.92465753, 0.9234122 ,\n",
      "       0.90971357, 0.91967621, 0.92777086, 0.93026152, 0.92714819,\n",
      "       0.93088418, 0.92278954, 0.93088418, 0.93711083, 0.94271482,\n",
      "       0.94458281, 0.94271482, 0.94396015, 0.95267746, 0.9483188 ,\n",
      "       0.93088418, 0.94458281, 0.95454545, 0.94022416, 0.94333748,\n",
      "       0.95454545, 0.94209215, 0.94582814, 0.94769614, 0.94396015,\n",
      "       0.93711083, 0.9483188 , 0.9483188 , 0.9358655 , 0.94645081,\n",
      "       0.93275218, 0.95703611, 0.94956413, 0.9607721 , 0.95952677,\n",
      "       0.96886675, 0.95890411, 0.96762142, 0.9607721 , 0.96699875,\n",
      "       0.96388543, 0.96326276, 0.9607721 , 0.96014944, 0.96886675,\n",
      "       0.95765878, 0.96886675, 0.97447073, 0.96575342, 0.97571606,\n",
      "       0.96201743, 0.96450809, 0.9626401 , 0.96699875, 0.96388543,\n",
      "       0.95392279, 0.95703611, 0.96450809, 0.96450809, 0.9626401 ,\n",
      "       0.96948941, 0.97696139, 0.98256538, 0.97945205, 0.98132005,\n",
      "       0.96699875, 0.97945205, 0.9732254 , 0.97758406, 0.98007472,\n",
      "       0.9732254 , 0.98007472, 0.97820672, 0.98069738, 0.98630137,\n",
      "       0.96762142, 0.97571606, 0.98443337, 0.97758406, 0.97758406,\n",
      "       0.96637609, 0.96824408, 0.96948941, 0.97447073, 0.97945205,\n",
      "       0.96824408, 0.98318804, 0.98381071, 0.97633873, 0.98381071,\n",
      "       0.96948941, 0.98816936, 0.98692403, 0.98381071, 0.98381071,\n",
      "       0.96824408, 0.98069738, 0.98816936, 0.9875467 , 0.9856787 ,\n",
      "       0.9750934 , 0.98318804, 0.98381071, 0.98256538, 0.98069738]),\n",
      " 'split2_train_score': array([0.84648918, 0.80274015, 0.89537599, 0.89117235, 0.91063366,\n",
      "       0.90362759, 0.91608283, 0.89739997, 0.90051378, 0.90066947,\n",
      "       0.88105247, 0.90705278, 0.9067414 , 0.89724428, 0.90720847,\n",
      "       0.88167523, 0.90705278, 0.89054959, 0.91078935, 0.8673517 ,\n",
      "       0.90720847, 0.90720847, 0.88899268, 0.90720847, 0.9002024 ,\n",
      "       0.89662152, 0.92853807, 0.92573564, 0.92386735, 0.91296902,\n",
      "       0.90814261, 0.92900514, 0.93274171, 0.9230889 , 0.93102911,\n",
      "       0.92184338, 0.93507707, 0.92449011, 0.93429861, 0.91826249,\n",
      "       0.90923245, 0.92589133, 0.93149619, 0.93211895, 0.92853807,\n",
      "       0.90892106, 0.93071773, 0.92869376, 0.92729254, 0.92589133,\n",
      "       0.93694535, 0.94550833, 0.94239452, 0.93974778, 0.93414292,\n",
      "       0.954227  , 0.95531683, 0.96061031, 0.95843064, 0.95983185,\n",
      "       0.96496964, 0.96154445, 0.96683793, 0.96543671, 0.95874202,\n",
      "       0.94753231, 0.9623229 , 0.96356843, 0.967305  , 0.97026312,\n",
      "       0.96263428, 0.96387981, 0.96574809, 0.96761638, 0.96761638,\n",
      "       0.96185583, 0.9640355 , 0.96450257, 0.96823914, 0.96434688,\n",
      "       0.97571228, 0.97726919, 0.98162852, 0.97695781, 0.98007162,\n",
      "       0.97882609, 0.98209559, 0.98131714, 0.98396388, 0.98131714,\n",
      "       0.98053869, 0.98116145, 0.98380819, 0.98349681, 0.98707769,\n",
      "       0.97976024, 0.97820333, 0.98209559, 0.98801183, 0.98567648,\n",
      "       0.97493383, 0.98209559, 0.98349681, 0.98552078, 0.98225128,\n",
      "       0.9884789 , 0.99252686, 0.993461  , 0.99268255, 0.99392807,\n",
      "       0.98723338, 0.99205979, 0.99455083, 0.99470652, 0.99486221,\n",
      "       0.9884789 , 0.99252686, 0.99532929, 0.99688619, 0.99626343,\n",
      "       0.98956874, 0.99455083, 0.99517359, 0.99657481, 0.99657481,\n",
      "       0.98583217, 0.99096995, 0.99283824, 0.99455083, 0.99564067,\n",
      "       0.99190409, 0.99626343, 0.99813171, 0.99750895, 0.9984431 ,\n",
      "       0.99361669, 0.99595205, 0.99937724, 0.99766464, 0.99968862,\n",
      "       0.993461  , 0.99704188, 0.99906586, 0.99782033, 0.99859879,\n",
      "       0.99486221, 0.99813171, 0.99813171, 0.99937724, 0.99922155,\n",
      "       0.91141211, 0.93601121, 0.9100109 , 0.92059785, 0.91390316,\n",
      "       0.93196326, 0.93056204, 0.94099331, 0.94099331, 0.95516114,\n",
      "       0.96310135, 0.91608283, 0.93663397, 0.92573564, 0.95998754,\n",
      "       0.96341274, 0.96932897, 0.95049043, 0.94535264, 0.94566402,\n",
      "       0.96948466, 0.94379573, 0.95173595, 0.96932897, 0.96932897,\n",
      "       0.93274171, 0.8936634 , 0.93585552, 0.93398723, 0.9165499 ,\n",
      "       0.90378328, 0.94005916, 0.92589133, 0.94690954, 0.93071773,\n",
      "       0.90814261, 0.92433442, 0.94130469, 0.92993928, 0.90565156,\n",
      "       0.94192745, 0.93585552, 0.92449011, 0.94177176, 0.94005916,\n",
      "       0.91904095, 0.93398723, 0.93803519, 0.941149  , 0.94083761,\n",
      "       0.93725673, 0.91966371, 0.94146038, 0.95002335, 0.95687374,\n",
      "       0.95500545, 0.95453838, 0.95344854, 0.96341274, 0.96061031,\n",
      "       0.94706523, 0.96138876, 0.9640355 , 0.9574965 , 0.95983185,\n",
      "       0.96528102, 0.95547252, 0.96185583, 0.95920909, 0.96185583,\n",
      "       0.95531683, 0.96668224, 0.96294566, 0.95936478, 0.96699362,\n",
      "       0.95313716, 0.97088588, 0.96450257, 0.97462245, 0.97119726,\n",
      "       0.97586797, 0.97773626, 0.98147283, 0.97773626, 0.980383  ,\n",
      "       0.97617936, 0.97913747, 0.97913747, 0.98100576, 0.98754476,\n",
      "       0.97368831, 0.98225128, 0.98910167, 0.97976024, 0.986922  ,\n",
      "       0.97960455, 0.98022731, 0.98334112, 0.9836525 , 0.98225128,\n",
      "       0.96808345, 0.97571228, 0.98271836, 0.98022731, 0.98287405,\n",
      "       0.98583217, 0.98894598, 0.99330531, 0.99283824, 0.99221548,\n",
      "       0.98411957, 0.99159271, 0.99050288, 0.99377238, 0.99470652,\n",
      "       0.986922  , 0.99252686, 0.99330531, 0.993461  , 0.99532929,\n",
      "       0.98614355, 0.99439514, 0.9967305 , 0.99470652, 0.99283824,\n",
      "       0.98567648, 0.99268255, 0.99003581, 0.99221548, 0.99455083,\n",
      "       0.98863459, 0.99579636, 0.99735326, 0.9950179 , 0.9982874 ,\n",
      "       0.99065857, 0.99719757, 0.9984431 , 0.99750895, 0.99875448,\n",
      "       0.9901915 , 0.9982874 , 0.99859879, 0.99875448, 0.99922155,\n",
      "       0.99128133, 0.99797602, 0.99704188, 0.99797602, 0.9967305 ]),\n",
      " 'split3_test_score': array([0.77148194, 0.86674969, 0.77148194, 0.89975093, 0.87110834,\n",
      "       0.90473225, 0.88107098, 0.89227895, 0.90224159, 0.90597758,\n",
      "       0.90722291, 0.87235367, 0.89788294, 0.90722291, 0.90784558,\n",
      "       0.90473225, 0.89041096, 0.90473225, 0.90473225, 0.90971357,\n",
      "       0.90473225, 0.90473225, 0.875467  , 0.90722291, 0.90473225,\n",
      "       0.9128269 , 0.93026152, 0.93897883, 0.93088418, 0.90971357,\n",
      "       0.8985056 , 0.92777086, 0.93150685, 0.91344956, 0.92777086,\n",
      "       0.91656289, 0.92403487, 0.91905355, 0.92278954, 0.91780822,\n",
      "       0.93212951, 0.91718555, 0.91843088, 0.92092154, 0.93212951,\n",
      "       0.9358655 , 0.93275218, 0.93088418, 0.92963885, 0.91531756,\n",
      "       0.92714819, 0.9358655 , 0.92465753, 0.93897883, 0.9377335 ,\n",
      "       0.93960149, 0.94956413, 0.9377335 , 0.95267746, 0.95392279,\n",
      "       0.94769614, 0.9483188 , 0.94769614, 0.95392279, 0.95641345,\n",
      "       0.94396015, 0.95205479, 0.95392279, 0.95516812, 0.95579078,\n",
      "       0.9483188 , 0.95828144, 0.94769614, 0.95703611, 0.96326276,\n",
      "       0.94209215, 0.94894147, 0.95454545, 0.95330012, 0.95641345,\n",
      "       0.95828144, 0.96326276, 0.96886675, 0.97135741, 0.97260274,\n",
      "       0.96575342, 0.96575342, 0.97011208, 0.97447073, 0.97198007,\n",
      "       0.96637609, 0.97135741, 0.97571606, 0.96762142, 0.96824408,\n",
      "       0.97260274, 0.96886675, 0.97135741, 0.96886675, 0.97758406,\n",
      "       0.94645081, 0.96948941, 0.97198007, 0.96450809, 0.96824408,\n",
      "       0.97198007, 0.97571606, 0.9750934 , 0.97758406, 0.97882939,\n",
      "       0.97073474, 0.97696139, 0.97758406, 0.98132005, 0.97945205,\n",
      "       0.9732254 , 0.97882939, 0.98069738, 0.98381071, 0.97882939,\n",
      "       0.97011208, 0.98318804, 0.97882939, 0.98505604, 0.98381071,\n",
      "       0.95703611, 0.96762142, 0.9732254 , 0.9750934 , 0.97758406,\n",
      "       0.97198007, 0.97633873, 0.98194271, 0.98007472, 0.98318804,\n",
      "       0.97384807, 0.9750934 , 0.98256538, 0.98381071, 0.98630137,\n",
      "       0.97384807, 0.97384807, 0.98318804, 0.98381071, 0.98381071,\n",
      "       0.97696139, 0.97758406, 0.98443337, 0.98194271, 0.9856787 ,\n",
      "       0.95890411, 0.87422167, 0.94396015, 0.92963885, 0.91469489,\n",
      "       0.9128269 , 0.92092154, 0.95828144, 0.9252802 , 0.96450809,\n",
      "       0.98443337, 0.9358655 , 0.96326276, 0.96014944, 0.96450809,\n",
      "       0.96948941, 0.94894147, 0.94894147, 0.94894147, 0.96014944,\n",
      "       0.95765878, 0.9483188 , 0.95765878, 0.95765878, 0.95765878,\n",
      "       0.94084682, 0.94707347, 0.9252802 , 0.92465753, 0.92154421,\n",
      "       0.92963885, 0.90909091, 0.93212951, 0.91718555, 0.92963885,\n",
      "       0.91967621, 0.92092154, 0.91656289, 0.91220423, 0.92216687,\n",
      "       0.92963885, 0.93711083, 0.92465753, 0.92216687, 0.93711083,\n",
      "       0.94894147, 0.90909091, 0.94022416, 0.92403487, 0.94209215,\n",
      "       0.93150685, 0.94520548, 0.94209215, 0.93711083, 0.93524284,\n",
      "       0.94022416, 0.94645081, 0.95080946, 0.96014944, 0.94956413,\n",
      "       0.95143213, 0.94956413, 0.95454545, 0.95454545, 0.95330012,\n",
      "       0.94956413, 0.9501868 , 0.95080946, 0.95267746, 0.96388543,\n",
      "       0.93648817, 0.94520548, 0.95952677, 0.95516812, 0.95330012,\n",
      "       0.94146949, 0.95579078, 0.95205479, 0.96450809, 0.95454545,\n",
      "       0.95828144, 0.96326276, 0.96201743, 0.97384807, 0.97073474,\n",
      "       0.96637609, 0.97135741, 0.97696139, 0.96824408, 0.97447073,\n",
      "       0.95765878, 0.96388543, 0.96326276, 0.9732254 , 0.9732254 ,\n",
      "       0.96762142, 0.96388543, 0.96762142, 0.97073474, 0.97447073,\n",
      "       0.9483188 , 0.95267746, 0.96450809, 0.96886675, 0.97198007,\n",
      "       0.97073474, 0.97882939, 0.97633873, 0.98007472, 0.97758406,\n",
      "       0.96699875, 0.97882939, 0.97882939, 0.98692403, 0.9856787 ,\n",
      "       0.97135741, 0.98381071, 0.97696139, 0.98069738, 0.97945205,\n",
      "       0.96948941, 0.97447073, 0.97945205, 0.97820672, 0.97758406,\n",
      "       0.95703611, 0.97260274, 0.97073474, 0.98007472, 0.97073474,\n",
      "       0.96886675, 0.97447073, 0.97945205, 0.98194271, 0.98443337,\n",
      "       0.97384807, 0.97758406, 0.97696139, 0.97882939, 0.98318804,\n",
      "       0.97011208, 0.98505604, 0.97820672, 0.98069738, 0.98692403,\n",
      "       0.97011208, 0.97820672, 0.98318804, 0.98069738, 0.9875467 ]),\n",
      " 'split3_train_score': array([0.77782967, 0.86921999, 0.77331465, 0.90845399, 0.87015413,\n",
      "       0.9082983 , 0.88790285, 0.89506461, 0.91047797, 0.90923245,\n",
      "       0.91312471, 0.87482485, 0.90316052, 0.91187918, 0.91219057,\n",
      "       0.9082983 , 0.89288494, 0.9082983 , 0.9082983 , 0.91592714,\n",
      "       0.9082983 , 0.9082983 , 0.88136385, 0.91296902, 0.90814261,\n",
      "       0.91717266, 0.93352016, 0.94675385, 0.93180757, 0.91748404,\n",
      "       0.90565156, 0.93087342, 0.92822669, 0.91748404, 0.93352016,\n",
      "       0.91343609, 0.92822669, 0.92682547, 0.92573564, 0.92838238,\n",
      "       0.93772381, 0.92199907, 0.92480149, 0.9263584 , 0.93881364,\n",
      "       0.93772381, 0.93025066, 0.9313405 , 0.9311848 , 0.92168768,\n",
      "       0.93320878, 0.94348435, 0.93678966, 0.94488557, 0.94940059,\n",
      "       0.94566402, 0.95547252, 0.95453838, 0.95811926, 0.95967616,\n",
      "       0.954227  , 0.95080181, 0.960766  , 0.96216721, 0.96325704,\n",
      "       0.95484976, 0.96029893, 0.96294566, 0.96574809, 0.96419119,\n",
      "       0.95952047, 0.96419119, 0.96372412, 0.96434688, 0.96808345,\n",
      "       0.95142457, 0.95998754, 0.96699362, 0.96746069, 0.96823914,\n",
      "       0.97135295, 0.97929317, 0.97976024, 0.98411957, 0.98567648,\n",
      "       0.97431107, 0.97680212, 0.98536509, 0.98505371, 0.98256267,\n",
      "       0.97493383, 0.98801183, 0.98661062, 0.98302974, 0.98489802,\n",
      "       0.98131714, 0.98707769, 0.98334112, 0.98209559, 0.9884789 ,\n",
      "       0.97633505, 0.98147283, 0.98349681, 0.9852094 , 0.98894598,\n",
      "       0.98474233, 0.99205979, 0.99252686, 0.99548498, 0.99532929,\n",
      "       0.98925736, 0.99408376, 0.99564067, 0.9950179 , 0.9950179 ,\n",
      "       0.99159271, 0.99470652, 0.99704188, 0.99470652, 0.99439514,\n",
      "       0.98956874, 0.99470652, 0.9967305 , 0.99408376, 0.99579636,\n",
      "       0.98427526, 0.99143702, 0.99408376, 0.99579636, 0.99517359,\n",
      "       0.99268255, 0.99719757, 0.9982874 , 0.99813171, 0.99657481,\n",
      "       0.99283824, 0.99688619, 0.99813171, 0.99859879, 0.99922155,\n",
      "       0.993461  , 0.99532929, 0.99875448, 0.99891017, 0.99937724,\n",
      "       0.99470652, 0.99782033, 0.99859879, 0.99859879, 0.99937724,\n",
      "       0.9623229 , 0.87295656, 0.94877783, 0.93336447, 0.90705278,\n",
      "       0.92449011, 0.91763973, 0.96528102, 0.92495719, 0.97368831,\n",
      "       0.98707769, 0.94192745, 0.96901759, 0.96792776, 0.96792776,\n",
      "       0.97259847, 0.95189164, 0.95671804, 0.95407131, 0.96621516,\n",
      "       0.96419119, 0.95562821, 0.96434688, 0.96434688, 0.96434688,\n",
      "       0.94317297, 0.95173595, 0.92916083, 0.93071773, 0.9263584 ,\n",
      "       0.92557995, 0.9165499 , 0.93710104, 0.92340028, 0.93414292,\n",
      "       0.91623852, 0.92682547, 0.92075354, 0.91857388, 0.92775961,\n",
      "       0.93383154, 0.94068192, 0.92775961, 0.92371166, 0.9361669 ,\n",
      "       0.95438269, 0.91935233, 0.94177176, 0.9263584 , 0.94504126,\n",
      "       0.93896933, 0.94706523, 0.94893352, 0.94223883, 0.94052623,\n",
      "       0.94908921, 0.95562821, 0.95843064, 0.96154445, 0.95593959,\n",
      "       0.95562821, 0.960766  , 0.96263428, 0.9623229 , 0.95920909,\n",
      "       0.954227  , 0.95967616, 0.9574965 , 0.9590534 , 0.96263428,\n",
      "       0.94208314, 0.94893352, 0.967305  , 0.95811926, 0.95593959,\n",
      "       0.95313716, 0.96512533, 0.9705745 , 0.97353262, 0.95858633,\n",
      "       0.97041881, 0.97898178, 0.97913747, 0.98271836, 0.98053869,\n",
      "       0.97617936, 0.98427526, 0.98474233, 0.98334112, 0.98225128,\n",
      "       0.97695781, 0.98302974, 0.97835902, 0.98287405, 0.98552078,\n",
      "       0.98022731, 0.97664643, 0.98287405, 0.98022731, 0.98505371,\n",
      "       0.95858633, 0.97353262, 0.98271836, 0.98411957, 0.98816752,\n",
      "       0.98629924, 0.99128133, 0.99034719, 0.99221548, 0.99579636,\n",
      "       0.98707769, 0.99314962, 0.99517359, 0.99283824, 0.99517359,\n",
      "       0.98302974, 0.99564067, 0.99283824, 0.99268255, 0.99314962,\n",
      "       0.98271836, 0.99159271, 0.99392807, 0.99143702, 0.99299393,\n",
      "       0.98411957, 0.99112564, 0.99221548, 0.99470652, 0.99361669,\n",
      "       0.99034719, 0.99548498, 0.99875448, 0.99564067, 0.99797602,\n",
      "       0.99299393, 0.99766464, 0.99719757, 0.99735326, 0.99937724,\n",
      "       0.99205979, 0.99610774, 0.99859879, 0.99719757, 0.9982874 ,\n",
      "       0.98988012, 0.99548498, 0.9982874 , 0.99735326, 0.99750895]),\n",
      " 'split4_test_score': array([0.82616822, 0.83862928, 0.88535826, 0.88286604, 0.90342679,\n",
      "       0.91277259, 0.89283489, 0.88286604, 0.90031153, 0.88037383,\n",
      "       0.91339564, 0.89844237, 0.89781931, 0.87850467, 0.90093458,\n",
      "       0.87788162, 0.90903427, 0.91339564, 0.91277259, 0.87850467,\n",
      "       0.91277259, 0.86791277, 0.91277259, 0.85358255, 0.91277259,\n",
      "       0.94454829, 0.92772586, 0.95202492, 0.90903427, 0.91900312,\n",
      "       0.91775701, 0.93831776, 0.92772586, 0.9165109 , 0.93707165,\n",
      "       0.93707165, 0.93271028, 0.94641745, 0.94205607, 0.9364486 ,\n",
      "       0.92398754, 0.93084112, 0.93395639, 0.9376947 , 0.93208723,\n",
      "       0.88971963, 0.94392523, 0.93146417, 0.94330218, 0.9364486 ,\n",
      "       0.94392523, 0.9482866 , 0.94517134, 0.95514019, 0.95077882,\n",
      "       0.95451713, 0.95576324, 0.95825545, 0.96448598, 0.95638629,\n",
      "       0.95264798, 0.96697819, 0.9694704 , 0.9694704 , 0.9682243 ,\n",
      "       0.95638629, 0.97258567, 0.96323988, 0.97009346, 0.96323988,\n",
      "       0.95389408, 0.9588785 , 0.96635514, 0.97196262, 0.96074766,\n",
      "       0.95264798, 0.95327103, 0.9682243 , 0.9682243 , 0.96199377,\n",
      "       0.97383178, 0.9682243 , 0.97694704, 0.9788162 , 0.97819315,\n",
      "       0.97383178, 0.98317757, 0.97570093, 0.97694704, 0.97819315,\n",
      "       0.97943925, 0.9788162 , 0.97819315, 0.98068536, 0.98255452,\n",
      "       0.97196262, 0.97133956, 0.97757009, 0.97694704, 0.98130841,\n",
      "       0.9682243 , 0.96573209, 0.97819315, 0.97570093, 0.97819315,\n",
      "       0.97133956, 0.97320872, 0.98380062, 0.98753894, 0.98068536,\n",
      "       0.97133956, 0.97819315, 0.98442368, 0.98878505, 0.98691589,\n",
      "       0.97819315, 0.97819315, 0.98566978, 0.98255452, 0.98380062,\n",
      "       0.97757009, 0.98317757, 0.98255452, 0.98380062, 0.98255452,\n",
      "       0.97819315, 0.96697819, 0.9788162 , 0.97383178, 0.98006231,\n",
      "       0.97819315, 0.9788162 , 0.98255452, 0.98442368, 0.98504673,\n",
      "       0.97258567, 0.98068536, 0.98691589, 0.98629283, 0.98816199,\n",
      "       0.97632399, 0.98130841, 0.98816199, 0.98380062, 0.98753894,\n",
      "       0.98380062, 0.98317757, 0.98566978, 0.98504673, 0.98753894,\n",
      "       0.86728972, 0.92024922, 0.91339564, 0.90404984, 0.88161994,\n",
      "       0.93021807, 0.94641745, 0.9271028 , 0.94143302, 0.91900312,\n",
      "       0.94579439, 0.97632399, 0.94018692, 0.94205607, 0.95015576,\n",
      "       0.97258567, 0.95264798, 0.94579439, 0.92274143, 0.97071651,\n",
      "       0.96884735, 0.97445483, 0.94953271, 0.94454829, 0.97196262,\n",
      "       0.92336449, 0.91838006, 0.92461059, 0.92647975, 0.91401869,\n",
      "       0.94953271, 0.91339564, 0.93520249, 0.92461059, 0.92647975,\n",
      "       0.92149533, 0.93956386, 0.93831776, 0.94205607, 0.93831776,\n",
      "       0.94267913, 0.94953271, 0.95015576, 0.93084112, 0.94080997,\n",
      "       0.94517134, 0.94080997, 0.94267913, 0.94953271, 0.93707165,\n",
      "       0.93831776, 0.94267913, 0.94143302, 0.95389408, 0.9271028 ,\n",
      "       0.94330218, 0.95140187, 0.95700935, 0.95202492, 0.96261682,\n",
      "       0.95264798, 0.95950156, 0.96137072, 0.96074766, 0.96323988,\n",
      "       0.96573209, 0.95825545, 0.96137072, 0.95825545, 0.95950156,\n",
      "       0.94766355, 0.9576324 , 0.96074766, 0.96448598, 0.97071651,\n",
      "       0.96448598, 0.95950156, 0.96137072, 0.96884735, 0.96386293,\n",
      "       0.9576324 , 0.9788162 , 0.97196262, 0.98193146, 0.97757009,\n",
      "       0.96697819, 0.97507788, 0.97632399, 0.98006231, 0.98068536,\n",
      "       0.9694704 , 0.97507788, 0.97694704, 0.97694704, 0.98068536,\n",
      "       0.96635514, 0.97383178, 0.97445483, 0.97757009, 0.97757009,\n",
      "       0.96448598, 0.97009346, 0.97570093, 0.97570093, 0.97570093,\n",
      "       0.98130841, 0.98006231, 0.98255452, 0.98691589, 0.98504673,\n",
      "       0.97757009, 0.97943925, 0.98629283, 0.98193146, 0.98442368,\n",
      "       0.97757009, 0.97694704, 0.98255452, 0.98566978, 0.98442368,\n",
      "       0.97757009, 0.98130841, 0.98006231, 0.98006231, 0.98130841,\n",
      "       0.97383178, 0.97383178, 0.98006231, 0.97694704, 0.98380062,\n",
      "       0.97258567, 0.97943925, 0.98753894, 0.98878505, 0.98816199,\n",
      "       0.97570093, 0.97694704, 0.98380062, 0.99065421, 0.99003115,\n",
      "       0.97507788, 0.98566978, 0.98442368, 0.98504673, 0.98878505,\n",
      "       0.97445483, 0.97570093, 0.98691589, 0.98566978, 0.98566978]),\n",
      " 'split4_train_score': array([0.82503113, 0.83468244, 0.87033001, 0.86846202, 0.89103362,\n",
      "       0.90613325, 0.88900996, 0.87624533, 0.89134496, 0.874066  ,\n",
      "       0.90613325, 0.89103362, 0.89056663, 0.87671233, 0.89352428,\n",
      "       0.87515567, 0.90068493, 0.90613325, 0.90613325, 0.875     ,\n",
      "       0.90613325, 0.86752802, 0.90613325, 0.85227273, 0.90613325,\n",
      "       0.93929016, 0.92247821, 0.9367995 , 0.90255293, 0.91360523,\n",
      "       0.91018057, 0.93150685, 0.92247821, 0.91111457, 0.92512453,\n",
      "       0.92154421, 0.92216687, 0.93135118, 0.92948319, 0.92636986,\n",
      "       0.91609589, 0.91967621, 0.9252802 , 0.9257472 , 0.92123288,\n",
      "       0.88496264, 0.93212951, 0.91936488, 0.92870486, 0.9262142 ,\n",
      "       0.93960149, 0.94613948, 0.94115816, 0.9497198 , 0.9511208 ,\n",
      "       0.95049813, 0.95563512, 0.95688045, 0.95983811, 0.95407846,\n",
      "       0.95252179, 0.95688045, 0.96108344, 0.95921544, 0.95999377,\n",
      "       0.95579078, 0.96326276, 0.95703611, 0.95983811, 0.96046077,\n",
      "       0.95392279, 0.95579078, 0.9640411 , 0.96279577, 0.95330012,\n",
      "       0.95267746, 0.95298879, 0.96497509, 0.97011208, 0.96295143,\n",
      "       0.97384807, 0.97571606, 0.98023039, 0.98536737, 0.98038605,\n",
      "       0.97665006, 0.98038605, 0.98194271, 0.98272105, 0.98458904,\n",
      "       0.97836239, 0.98692403, 0.98381071, 0.98381071, 0.98723537,\n",
      "       0.97991905, 0.97789539, 0.98583437, 0.98599004, 0.98583437,\n",
      "       0.97478207, 0.98147572, 0.98770237, 0.9870797 , 0.98552304,\n",
      "       0.98645704, 0.99190535, 0.99330635, 0.99439601, 0.99408468,\n",
      "       0.98427771, 0.99315068, 0.99424035, 0.99501868, 0.99470735,\n",
      "       0.98957036, 0.99315068, 0.99517435, 0.99548568, 0.99548568,\n",
      "       0.98910336, 0.99501868, 0.99533001, 0.99595268, 0.99579701,\n",
      "       0.98770237, 0.98816936, 0.99533001, 0.99439601, 0.99579701,\n",
      "       0.99330635, 0.99626401, 0.99735367, 0.99875467, 0.99750934,\n",
      "       0.99455168, 0.99657534, 0.99922167, 0.999533  , 0.99937733,\n",
      "       0.99377335, 0.99641968, 0.99875467, 0.99875467, 0.99782067,\n",
      "       0.99533001, 0.99657534, 0.99844334, 0.99875467, 0.99875467,\n",
      "       0.85040473, 0.91547323, 0.91220423, 0.8994396 , 0.88589664,\n",
      "       0.93290785, 0.94240349, 0.92169988, 0.93617684, 0.92169988,\n",
      "       0.94006849, 0.96777709, 0.93181818, 0.93119552, 0.94598381,\n",
      "       0.9631071 , 0.94162516, 0.9367995 , 0.91796389, 0.95859278,\n",
      "       0.95983811, 0.96653176, 0.9391345 , 0.93384184, 0.9607721 ,\n",
      "       0.90768991, 0.91780822, 0.91811955, 0.92808219, 0.91204857,\n",
      "       0.9358655 , 0.9128269 , 0.92465753, 0.92185554, 0.91718555,\n",
      "       0.9137609 , 0.93010585, 0.93057285, 0.93244085, 0.92839352,\n",
      "       0.93197385, 0.93617684, 0.93103985, 0.92029888, 0.92730386,\n",
      "       0.93197385, 0.92605853, 0.92683686, 0.9363325 , 0.92496887,\n",
      "       0.93119552, 0.93991283, 0.94193649, 0.94722914, 0.93197385,\n",
      "       0.93742217, 0.95049813, 0.95283313, 0.94224782, 0.95532379,\n",
      "       0.95283313, 0.95345579, 0.95454545, 0.95594645, 0.95438979,\n",
      "       0.95345579, 0.9506538 , 0.95143213, 0.95298879, 0.95656912,\n",
      "       0.94458281, 0.95485679, 0.95750311, 0.95999377, 0.9621731 ,\n",
      "       0.95828144, 0.95859278, 0.96092777, 0.97260274, 0.97166874,\n",
      "       0.96450809, 0.98085305, 0.97306974, 0.98147572, 0.98349938,\n",
      "       0.96839975, 0.98381071, 0.98256538, 0.97960772, 0.98303238,\n",
      "       0.9764944 , 0.97742839, 0.98318804, 0.98303238, 0.98458904,\n",
      "       0.97042341, 0.97680573, 0.98023039, 0.98240971, 0.9866127 ,\n",
      "       0.9764944 , 0.97696139, 0.98038605, 0.98349938, 0.98194271,\n",
      "       0.98832503, 0.99066002, 0.99315068, 0.99361768, 0.99346202,\n",
      "       0.9856787 , 0.99190535, 0.99190535, 0.99346202, 0.99579701,\n",
      "       0.99112702, 0.99003736, 0.99283935, 0.99564134, 0.99641968,\n",
      "       0.9856787 , 0.99237235, 0.99330635, 0.99159402, 0.99174969,\n",
      "       0.9856787 , 0.98630137, 0.99424035, 0.99221669, 0.99377335,\n",
      "       0.98692403, 0.99486301, 0.99844334, 0.998599  , 0.99828767,\n",
      "       0.99190535, 0.99595268, 0.99782067, 0.99875467, 0.999066  ,\n",
      "       0.98910336, 0.99704234, 0.99750934, 0.99735367, 0.99844334,\n",
      "       0.99206102, 0.99408468, 0.99719801, 0.998132  , 0.99891034]),\n",
      " 'std_fit_time': array([0.00154536, 0.00348829, 0.00680905, 0.00665794, 0.0057321 ,\n",
      "       0.00247528, 0.00343098, 0.00555034, 0.00792889, 0.01238292,\n",
      "       0.00873175, 0.01179003, 0.00582905, 0.0059377 , 0.00674646,\n",
      "       0.00079837, 0.0091978 , 0.00173833, 0.00232706, 0.00784258,\n",
      "       0.00101683, 0.00347617, 0.00522514, 0.0137125 , 0.0081132 ,\n",
      "       0.0013231 , 0.00538945, 0.01168831, 0.00815164, 0.00325205,\n",
      "       0.0038055 , 0.00590416, 0.00232655, 0.00480407, 0.00583587,\n",
      "       0.00193394, 0.00327781, 0.00424932, 0.01116648, 0.0088167 ,\n",
      "       0.0032777 , 0.00333705, 0.00429655, 0.00879931, 0.00171602,\n",
      "       0.00569133, 0.00486189, 0.00929178, 0.01481931, 0.02745563,\n",
      "       0.00146612, 0.00252314, 0.0044425 , 0.00410683, 0.0035461 ,\n",
      "       0.00921448, 0.00598395, 0.01444443, 0.01221436, 0.00917529,\n",
      "       0.00330116, 0.00835333, 0.00648191, 0.01712412, 0.00506911,\n",
      "       0.00517825, 0.0146224 , 0.0131648 , 0.00453108, 0.00830562,\n",
      "       0.0047204 , 0.01253029, 0.00247514, 0.02819778, 0.02031031,\n",
      "       0.00074654, 0.00411682, 0.00257004, 0.00861649, 0.00877146,\n",
      "       0.00477855, 0.00306422, 0.00544919, 0.01180976, 0.00804392,\n",
      "       0.00553468, 0.00904459, 0.01641198, 0.02199029, 0.01237054,\n",
      "       0.00660419, 0.00775112, 0.00972276, 0.01336641, 0.00383698,\n",
      "       0.00687967, 0.00690992, 0.01656193, 0.01570191, 0.03969161,\n",
      "       0.00337311, 0.00116296, 0.00277902, 0.00641365, 0.00541074,\n",
      "       0.00583645, 0.02064622, 0.0147069 , 0.0117595 , 0.02149834,\n",
      "       0.00324088, 0.02579222, 0.01319232, 0.01496666, 0.02848494,\n",
      "       0.01290542, 0.01044836, 0.01908367, 0.0171154 , 0.02786857,\n",
      "       0.00771444, 0.01583265, 0.02861276, 0.02829613, 0.03322747,\n",
      "       0.00184971, 0.00514848, 0.01596511, 0.01156221, 0.00461667,\n",
      "       0.00832993, 0.00813702, 0.01458431, 0.01973992, 0.02690251,\n",
      "       0.00946129, 0.00415558, 0.01392887, 0.01113026, 0.0115716 ,\n",
      "       0.01028032, 0.01422302, 0.00664076, 0.02500893, 0.01857705,\n",
      "       0.01384254, 0.02588705, 0.0355664 , 0.0242221 , 0.04936542,\n",
      "       0.00132261, 0.00252312, 0.00330182, 0.00362324, 0.00325329,\n",
      "       0.00339758, 0.00456564, 0.00794344, 0.0082152 , 0.01246656,\n",
      "       0.006658  , 0.00533015, 0.01199073, 0.00840138, 0.01816597,\n",
      "       0.00360122, 0.0043423 , 0.00881683, 0.01662361, 0.01111279,\n",
      "       0.00566312, 0.00529984, 0.00778655, 0.00781208, 0.01284398,\n",
      "       0.00232567, 0.00906773, 0.00236007, 0.00347792, 0.01061778,\n",
      "       0.00308988, 0.00707985, 0.00411736, 0.01304386, 0.01143435,\n",
      "       0.00886126, 0.01343139, 0.0168383 , 0.01358838, 0.01173947,\n",
      "       0.0072573 , 0.01550945, 0.01979638, 0.00877192, 0.02120916,\n",
      "       0.00603036, 0.01035747, 0.01494527, 0.02329065, 0.03680839,\n",
      "       0.00227469, 0.00518571, 0.00852388, 0.01004125, 0.01865813,\n",
      "       0.01083322, 0.0049833 , 0.01454023, 0.0166997 , 0.0151513 ,\n",
      "       0.00628894, 0.01546857, 0.01788534, 0.01793618, 0.01507293,\n",
      "       0.01089662, 0.01828748, 0.02757348, 0.01923198, 0.02485298,\n",
      "       0.01760933, 0.01267224, 0.02173144, 0.02006964, 0.03372283,\n",
      "       0.00195469, 0.00614729, 0.01020221, 0.01040219, 0.01461651,\n",
      "       0.00586956, 0.01123038, 0.02090658, 0.02011089, 0.00957469,\n",
      "       0.00832993, 0.01360656, 0.01987081, 0.02269355, 0.02588744,\n",
      "       0.01868437, 0.01160295, 0.03268773, 0.02302627, 0.02107561,\n",
      "       0.00870795, 0.0275708 , 0.03087189, 0.03418591, 0.02329546,\n",
      "       0.00337286, 0.00488578, 0.00443316, 0.01021763, 0.00864795,\n",
      "       0.0076481 , 0.00689834, 0.02386946, 0.01890858, 0.01409633,\n",
      "       0.00385755, 0.01980215, 0.04102642, 0.08261755, 0.08191176,\n",
      "       0.03362426, 0.02700871, 0.02351316, 0.02992399, 0.03420869,\n",
      "       0.01029989, 0.02354535, 0.03358492, 0.02986231, 0.05706952,\n",
      "       0.00223891, 0.00159593, 0.00321621, 0.00499005, 0.00968122,\n",
      "       0.00506953, 0.01783361, 0.02736097, 0.0153163 , 0.02832803,\n",
      "       0.00967777, 0.02146781, 0.02601229, 0.03041175, 0.02632185,\n",
      "       0.00980422, 0.01944759, 0.0238246 , 0.04057506, 0.05056968,\n",
      "       0.01055511, 0.02989766, 0.03995585, 0.03758852, 0.12930608]),\n",
      " 'std_score_time': array([7.97307553e-04, 4.89786253e-04, 1.97987933e-06, 4.03865970e-03,\n",
      "       4.89824304e-04, 4.00425462e-04, 7.47845526e-04, 1.11420836e-06,\n",
      "       6.30150518e-04, 7.98058715e-04, 1.35328191e-03, 6.30526204e-04,\n",
      "       4.88909140e-04, 7.45448825e-04, 1.01841281e-03, 1.30935003e-06,\n",
      "       7.98797992e-04, 4.00597779e-04, 3.98686528e-04, 7.98392657e-04,\n",
      "       5.00704840e-03, 7.47616060e-04, 7.94056247e-04, 3.98614343e-04,\n",
      "       1.78798039e-06, 3.99161567e-04, 3.98567658e-04, 1.49211359e-03,\n",
      "       4.88305408e-04, 6.30524957e-04, 1.19113884e-06, 1.93353136e-03,\n",
      "       3.99828313e-04, 3.99161111e-04, 1.82871465e-03, 4.87896495e-04,\n",
      "       3.99566106e-04, 1.21195460e-06, 3.98566460e-04, 4.88889993e-04,\n",
      "       4.88578494e-04, 3.99567130e-04, 7.46098082e-04, 7.16843432e-07,\n",
      "       4.89103684e-04, 7.97629975e-04, 4.90018299e-04, 6.31203638e-04,\n",
      "       6.31581165e-04, 4.88714100e-04, 3.98731716e-04, 4.88305687e-04,\n",
      "       4.89220345e-04, 3.98667074e-04, 4.88616946e-04, 3.99567187e-04,\n",
      "       7.98189702e-04, 3.99375570e-04, 6.29696470e-04, 3.98827340e-04,\n",
      "       7.98332792e-04, 4.88734073e-04, 4.88461018e-04, 4.88383415e-04,\n",
      "       4.88694759e-04, 3.99113841e-04, 4.89435364e-04, 3.99256564e-04,\n",
      "       3.99232861e-04, 4.88928407e-04, 3.98612119e-04, 6.31355187e-04,\n",
      "       3.98207068e-04, 4.88501123e-04, 6.33477922e-04, 3.99136782e-04,\n",
      "       3.98781039e-04, 4.87818643e-04, 1.99561228e-03, 6.30073152e-04,\n",
      "       3.99256137e-04, 7.23159356e-07, 4.88384113e-04, 3.98948897e-04,\n",
      "       4.88948073e-04, 3.98636121e-04, 3.99041443e-04, 4.88947701e-04,\n",
      "       7.98285133e-04, 4.88870222e-04, 7.46888546e-04, 1.27592928e-06,\n",
      "       4.88383927e-04, 3.98206868e-04, 7.32531142e-07, 4.89220391e-04,\n",
      "       8.60951905e-07, 1.16356278e-03, 1.21195460e-06, 7.47258590e-04,\n",
      "       7.97463205e-04, 6.30223703e-04, 3.98685758e-04, 3.56832255e-07,\n",
      "       1.59602178e-03, 3.99544032e-04, 7.46519001e-04, 6.29922035e-04,\n",
      "       3.98780213e-04, 3.98707857e-04, 4.88754584e-04, 1.03814798e-06,\n",
      "       7.46365794e-04, 4.89181442e-04, 1.09258010e-03, 4.88753189e-04,\n",
      "       4.88733491e-04, 6.31128293e-04, 4.89181396e-04, 2.12998664e-03,\n",
      "       3.98827853e-04, 4.88500006e-04, 4.88655624e-04, 4.89200787e-04,\n",
      "       6.31054114e-04, 7.97796670e-04, 4.88246944e-04, 9.77213820e-04,\n",
      "       7.98070991e-04, 7.98823034e-04, 8.74056949e-07, 4.88188514e-04,\n",
      "       8.44957597e-07, 9.76999930e-04, 1.60007485e-03, 6.46813391e-07,\n",
      "       7.46799121e-04, 1.01732677e-03, 3.98756054e-04, 1.93428438e-03,\n",
      "       4.86280395e-07, 3.99184266e-04, 3.98328271e-04, 3.99327488e-04,\n",
      "       3.99065488e-04, 7.98630762e-04, 3.99661416e-04, 3.98850623e-04,\n",
      "       4.88812118e-04, 6.30525833e-04, 7.98321283e-04, 7.62939453e-07,\n",
      "       4.89202298e-04, 3.99709612e-04, 7.46862858e-04, 8.74056949e-07,\n",
      "       8.86968386e-07, 7.46290157e-04, 6.30600612e-04, 3.71082477e-03,\n",
      "       3.99613903e-04, 1.20064529e-06, 4.00043596e-04, 1.49211957e-03,\n",
      "       3.98637804e-04, 3.98755626e-04, 1.59668957e-03, 4.88655554e-04,\n",
      "       1.26104975e-03, 1.82848608e-03, 1.85018309e-03, 3.98827255e-04,\n",
      "       3.99184864e-04, 7.46391600e-04, 1.85089751e-03, 3.98805847e-04,\n",
      "       4.87819156e-04, 1.68182298e-06, 7.46136794e-04, 1.46597303e-03,\n",
      "       4.88519308e-04, 3.99162336e-04, 4.88636341e-04, 7.46480873e-04,\n",
      "       4.88461716e-04, 1.21570099e-06, 4.87877760e-04, 9.77486463e-04,\n",
      "       3.98543834e-04, 4.88577842e-04, 9.70220087e-07, 4.89473508e-04,\n",
      "       3.98278704e-04, 3.99041956e-04, 1.16315373e-03, 4.62310777e-07,\n",
      "       7.46799106e-04, 3.98707657e-04, 4.88402740e-04, 3.99423567e-04,\n",
      "       3.99757239e-04, 4.88811350e-04, 1.19748117e-03, 3.99042753e-04,\n",
      "       7.96676130e-04, 7.97820752e-04, 7.46162641e-04, 3.98397460e-04,\n",
      "       3.98994101e-04, 1.19676598e-03, 1.49292260e-03, 3.98350561e-04,\n",
      "       1.23057250e-06, 4.88500378e-04, 1.44945947e-06, 1.16330100e-03,\n",
      "       4.88870361e-04, 7.46824857e-04, 4.88188444e-04, 7.45766812e-04,\n",
      "       3.99446786e-04, 7.97820182e-04, 5.64201334e-07, 4.88753002e-04,\n",
      "       4.88928337e-04, 3.98945819e-04, 7.97880413e-04, 3.98446299e-04,\n",
      "       3.98851563e-04, 1.49273805e-03, 7.98714774e-04, 2.86290004e-03,\n",
      "       4.88325230e-04, 3.98923078e-04, 4.89259728e-04, 4.88986674e-04,\n",
      "       3.98779272e-04, 6.31278866e-04, 7.46468299e-04, 1.93400856e-03,\n",
      "       3.99423140e-04, 4.87896402e-04, 3.99447299e-04, 4.89123733e-04,\n",
      "       7.59953377e-07, 7.98606972e-04, 3.99232434e-04, 3.98540554e-04,\n",
      "       7.46238626e-04, 9.77253211e-04, 1.09249348e-03, 4.89044952e-04,\n",
      "       6.31052835e-04, 1.95401379e-03, 4.89259728e-04, 2.22124060e-03,\n",
      "       2.39381801e-03, 7.46404026e-04, 1.16248368e-03, 2.52315503e-03,\n",
      "       4.89259728e-04, 1.31800413e-06, 2.12929895e-03, 9.77856221e-04,\n",
      "       7.45970899e-04, 1.46598609e-03, 1.46622616e-03, 7.46111071e-04,\n",
      "       7.07263802e-07, 7.46901179e-04, 7.47889859e-07, 3.99232918e-04,\n",
      "       9.77525246e-04, 7.89305942e-07, 1.01727061e-03, 6.64157308e-07,\n",
      "       4.88714449e-04, 6.31052673e-04, 1.85086687e-03, 1.30063836e-06,\n",
      "       3.99065032e-04, 4.87586441e-04, 6.64157308e-07, 7.97403142e-04,\n",
      "       4.88324881e-04, 7.46799349e-04, 1.04469787e-06, 4.88013249e-04,\n",
      "       3.99065146e-04, 3.99184352e-04, 1.09271062e-03, 1.99475289e-03,\n",
      "       7.97999565e-04, 4.88617086e-04, 3.98731745e-04, 4.88636062e-04,\n",
      "       7.97891695e-04, 3.99398832e-04, 7.97736931e-04, 1.01776648e-03]),\n",
      " 'std_test_score': array([0.04961747, 0.03549895, 0.0490149 , 0.06850185, 0.01365828,\n",
      "       0.01956164, 0.02284009, 0.00316196, 0.00936669, 0.01263085,\n",
      "       0.01000647, 0.01899132, 0.01023626, 0.01367068, 0.01256054,\n",
      "       0.01342879, 0.02402074, 0.01917747, 0.02151619, 0.02158765,\n",
      "       0.00499463, 0.02043123, 0.01284317, 0.02167265, 0.00538584,\n",
      "       0.02148916, 0.01619229, 0.01109683, 0.01299492, 0.01131709,\n",
      "       0.01055441, 0.01540891, 0.00489615, 0.00892236, 0.01278207,\n",
      "       0.0129067 , 0.00649194, 0.01320465, 0.00898225, 0.0097257 ,\n",
      "       0.01603139, 0.01186923, 0.01097233, 0.01192762, 0.01145708,\n",
      "       0.02157453, 0.0093756 , 0.00862705, 0.01041883, 0.01240355,\n",
      "       0.00961368, 0.00637107, 0.00837853, 0.00965918, 0.00833958,\n",
      "       0.0068206 , 0.00766042, 0.01322481, 0.00715181, 0.01173489,\n",
      "       0.00863881, 0.0138764 , 0.01023632, 0.00804948, 0.00747998,\n",
      "       0.00859725, 0.0098111 , 0.00818674, 0.01214212, 0.00380504,\n",
      "       0.00722731, 0.00667577, 0.0140436 , 0.01025341, 0.0058121 ,\n",
      "       0.00341588, 0.00650153, 0.00761315, 0.00922515, 0.00840845,\n",
      "       0.00738139, 0.00606611, 0.00479258, 0.00713496, 0.00411486,\n",
      "       0.00460492, 0.00851922, 0.00444267, 0.00320702, 0.00638464,\n",
      "       0.00798966, 0.00547262, 0.0058435 , 0.00534677, 0.00551805,\n",
      "       0.00702749, 0.00421194, 0.00632754, 0.00494843, 0.00243682,\n",
      "       0.00735568, 0.00491113, 0.00754461, 0.00448522, 0.00615314,\n",
      "       0.0071685 , 0.00543577, 0.00352818, 0.00592393, 0.00195303,\n",
      "       0.00253508, 0.00359478, 0.00298072, 0.00288026, 0.0033299 ,\n",
      "       0.00201803, 0.00159513, 0.00337165, 0.00473574, 0.00241302,\n",
      "       0.0053828 , 0.00355132, 0.00208258, 0.00138663, 0.00253982,\n",
      "       0.00988913, 0.00532311, 0.00466318, 0.00169032, 0.00313316,\n",
      "       0.00309568, 0.00431307, 0.00311662, 0.00164102, 0.00316411,\n",
      "       0.00291267, 0.00372081, 0.00392572, 0.00130273, 0.00273815,\n",
      "       0.00285338, 0.00431201, 0.00318731, 0.00413719, 0.00196758,\n",
      "       0.00623188, 0.00487572, 0.00227465, 0.00492702, 0.00139548,\n",
      "       0.0338181 , 0.01967935, 0.01396915, 0.03801008, 0.02130416,\n",
      "       0.01874077, 0.01134532, 0.02301676, 0.01409086, 0.01572891,\n",
      "       0.01436591, 0.02055353, 0.01435977, 0.02204124, 0.01148426,\n",
      "       0.01231796, 0.00958552, 0.00432965, 0.01042486, 0.00804135,\n",
      "       0.00605255, 0.01147158, 0.0050423 , 0.01046816, 0.01014945,\n",
      "       0.03569184, 0.01824459, 0.00541432, 0.01450981, 0.00353737,\n",
      "       0.0144855 , 0.01142165, 0.01041477, 0.00967249, 0.00476897,\n",
      "       0.00942415, 0.01403495, 0.00907593, 0.0103688 , 0.01332843,\n",
      "       0.00712601, 0.01134901, 0.01394023, 0.00822079, 0.00823471,\n",
      "       0.017808  , 0.01518908, 0.01461228, 0.01080983, 0.00951167,\n",
      "       0.01375653, 0.00883522, 0.00657778, 0.00894183, 0.0073461 ,\n",
      "       0.00357523, 0.00591739, 0.00535983, 0.00696802, 0.00570556,\n",
      "       0.0149066 , 0.00788296, 0.00569759, 0.00932606, 0.00920925,\n",
      "       0.01230256, 0.00859239, 0.0076414 , 0.00598657, 0.01114662,\n",
      "       0.00992775, 0.01116139, 0.00829425, 0.01159042, 0.01099651,\n",
      "       0.01294649, 0.0026171 , 0.00913596, 0.00969647, 0.00562836,\n",
      "       0.00434685, 0.00722851, 0.00352609, 0.00877665, 0.00569562,\n",
      "       0.00606675, 0.00521678, 0.00858492, 0.00680603, 0.00817756,\n",
      "       0.00774961, 0.00831136, 0.00584703, 0.00422073, 0.00601493,\n",
      "       0.0056714 , 0.00535926, 0.00659963, 0.00641372, 0.00642762,\n",
      "       0.00816691, 0.00739108, 0.00792769, 0.0056943 , 0.00567188,\n",
      "       0.00626491, 0.00527695, 0.00256168, 0.00396652, 0.00265631,\n",
      "       0.00773891, 0.00359977, 0.0044809 , 0.00419409, 0.00290903,\n",
      "       0.0046168 , 0.00615973, 0.00256683, 0.0041737 , 0.00250814,\n",
      "       0.00482979, 0.00403243, 0.00439291, 0.00364264, 0.00550679,\n",
      "       0.00702442, 0.00564318, 0.00585716, 0.00346781, 0.00655118,\n",
      "       0.00500286, 0.00313998, 0.00282638, 0.00452276, 0.00357782,\n",
      "       0.00227336, 0.00463888, 0.00451733, 0.00466751, 0.00365052,\n",
      "       0.00374673, 0.00298166, 0.00343312, 0.0024456 , 0.00265822,\n",
      "       0.00258239, 0.00304318, 0.00238   , 0.00253203, 0.0030234 ]),\n",
      " 'std_train_score': array([0.04768774, 0.04217009, 0.04472279, 0.05494329, 0.01342641,\n",
      "       0.02015936, 0.0164206 , 0.00996743, 0.00693383, 0.01167493,\n",
      "       0.01141274, 0.01282611, 0.00541205, 0.01292463, 0.01357825,\n",
      "       0.01553703, 0.01481837, 0.01103944, 0.01310386, 0.019352  ,\n",
      "       0.0012828 , 0.0172074 , 0.01383845, 0.02284786, 0.00322904,\n",
      "       0.02203136, 0.01158245, 0.00936739, 0.01037969, 0.00654942,\n",
      "       0.00667742, 0.01061607, 0.0064383 , 0.00428102, 0.00437398,\n",
      "       0.00804103, 0.00487307, 0.00587763, 0.00456339, 0.00397551,\n",
      "       0.01044553, 0.0080712 , 0.00375014, 0.00308323, 0.00579132,\n",
      "       0.02406031, 0.00541341, 0.00454325, 0.00302354, 0.00569382,\n",
      "       0.00529808, 0.00422207, 0.00574431, 0.00551158, 0.00608319,\n",
      "       0.00358657, 0.00062512, 0.00356587, 0.00188012, 0.00647812,\n",
      "       0.00623855, 0.00480096, 0.00265128, 0.00222811, 0.00187107,\n",
      "       0.00319972, 0.00231242, 0.00340345, 0.00471512, 0.00314393,\n",
      "       0.00357757, 0.00394876, 0.00436644, 0.00248636, 0.00583711,\n",
      "       0.00516542, 0.0059815 , 0.00126653, 0.00256708, 0.00244996,\n",
      "       0.00180111, 0.00159445, 0.00119514, 0.00309816, 0.00224136,\n",
      "       0.00173759, 0.00212794, 0.00218004, 0.00143627, 0.00253846,\n",
      "       0.00222873, 0.00239802, 0.00124286, 0.00249577, 0.00161695,\n",
      "       0.00055993, 0.00410111, 0.00152077, 0.00218983, 0.00155655,\n",
      "       0.00251693, 0.00185148, 0.00279163, 0.0008362 , 0.00233339,\n",
      "       0.00151576, 0.00043363, 0.00131587, 0.00120437, 0.00073682,\n",
      "       0.00192014, 0.00067789, 0.00074056, 0.00111392, 0.00040586,\n",
      "       0.00255509, 0.00086723, 0.00074322, 0.0007473 , 0.00106955,\n",
      "       0.00123579, 0.00060229, 0.00060684, 0.00097779, 0.00041058,\n",
      "       0.00119192, 0.00167336, 0.00079042, 0.00074597, 0.00048845,\n",
      "       0.00066515, 0.00080239, 0.00051531, 0.00044696, 0.00075374,\n",
      "       0.00077167, 0.00038889, 0.00056909, 0.00064119, 0.00025297,\n",
      "       0.00105126, 0.0010421 , 0.0001969 , 0.00041076, 0.00058739,\n",
      "       0.00071969, 0.00052455, 0.00018685, 0.0004012 , 0.00036039,\n",
      "       0.04098353, 0.0219091 , 0.01523436, 0.03862091, 0.02474   ,\n",
      "       0.01690827, 0.0168004 , 0.02170269, 0.00995405, 0.0197024 ,\n",
      "       0.01604705, 0.01843461, 0.01388022, 0.01711803, 0.00762555,\n",
      "       0.00547734, 0.01009537, 0.00999892, 0.01395956, 0.0078572 ,\n",
      "       0.00485976, 0.00864854, 0.01052262, 0.01288351, 0.00282948,\n",
      "       0.03090364, 0.02150473, 0.00831976, 0.01101233, 0.00641508,\n",
      "       0.01148527, 0.00996528, 0.0052724 , 0.00943827, 0.0124384 ,\n",
      "       0.01309375, 0.00363918, 0.00883149, 0.01243486, 0.00852379,\n",
      "       0.00496438, 0.00670259, 0.00844833, 0.00896388, 0.00646502,\n",
      "       0.01201647, 0.00497149, 0.00718924, 0.00682601, 0.00685104,\n",
      "       0.0080089 , 0.01134354, 0.00565473, 0.00264195, 0.01003509,\n",
      "       0.00676305, 0.00266573, 0.00365299, 0.00744223, 0.0028635 ,\n",
      "       0.00459387, 0.00342005, 0.00334727, 0.00224758, 0.00241473,\n",
      "       0.0057904 , 0.00411074, 0.00388477, 0.00325806, 0.00291352,\n",
      "       0.00705045, 0.0065705 , 0.0036928 , 0.00107159, 0.00450935,\n",
      "       0.00246308, 0.00430258, 0.00402043, 0.00465332, 0.00473811,\n",
      "       0.00441446, 0.00153146, 0.00299563, 0.00274733, 0.00247691,\n",
      "       0.00333254, 0.00270517, 0.00231698, 0.00278665, 0.00181632,\n",
      "       0.00224515, 0.00213586, 0.00434188, 0.00257022, 0.00219846,\n",
      "       0.00378728, 0.00346158, 0.00110846, 0.00267717, 0.00157913,\n",
      "       0.00640897, 0.00468242, 0.00236265, 0.00157462, 0.00241601,\n",
      "       0.00324082, 0.00157719, 0.00148231, 0.00110552, 0.00161004,\n",
      "       0.00229515, 0.00103924, 0.00226966, 0.00113116, 0.00066355,\n",
      "       0.00261416, 0.00184596, 0.00080219, 0.00140303, 0.00111675,\n",
      "       0.00356019, 0.001053  , 0.00144158, 0.00154598, 0.00194776,\n",
      "       0.0012592 , 0.00234304, 0.00231562, 0.00115815, 0.00150184,\n",
      "       0.00185572, 0.00042227, 0.00071687, 0.00135876, 0.00058755,\n",
      "       0.00115417, 0.00078761, 0.00046601, 0.00057083, 0.00031756,\n",
      "       0.00138293, 0.00082618, 0.00062573, 0.00070311, 0.00038887,\n",
      "       0.00081231, 0.00134481, 0.00070038, 0.00055177, 0.0007142 ])}\n"
     ]
    }
   ],
   "source": [
    "# Lets try to optimzie with GridSearch\n",
    "rfc3_params = {'n_estimators':[3,6,9,10,11], 'criterion':['gini', 'entropy'],\n",
    "              'max_depth':[1,3,5,7,9,11], 'max_features':[1,3,5,7,9]}\n",
    "\n",
    "rfc3_gridsearch = GridSearchCV(rfc, rfc3_params, cv=5, verbose=1, scoring='recall', n_jobs=-1)\n",
    "\n",
    "rfc3_gridsearch.fit(smote_X1, smote_y1)\n",
    "\n",
    "print(rfc3_gridsearch.best_params_)\n",
    "print(rfc3_gridsearch.best_score_)\n",
    "pp.pprint(rfc3_gridsearch.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores: [0.96100864 0.96848004 0.9603082  0.96707146 0.96566223]\n",
      "Mean Cross-Validation Score: 0.9645061123473656\n",
      "[[3225  134]\n",
      " [  16   55]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.98      3359\n",
      "           1       0.29      0.77      0.42        71\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      3430\n",
      "   macro avg       0.64      0.87      0.70      3430\n",
      "weighted avg       0.98      0.96      0.97      3430\n",
      "\n",
      "AUC Score 0.9663841938202601\n"
     ]
    }
   ],
   "source": [
    "rfc4 = RandomForestClassifier(n_estimators=11, criterion='entropy', max_features=7, n_jobs=-1, random_state=0, max_depth=11)\n",
    "\n",
    "# Fitting the model\n",
    "rfct4 = rfc4.fit(smote_X1, smote_y1)\n",
    "\n",
    "\n",
    "# Checking cross-validation values\n",
    "print('Cross-Validation Scores:', cross_val_score(rfct4, smote_X1, smote_y1, cv=5))\n",
    "print('Mean Cross-Validation Score:', np.mean(cross_val_score(rfct4, smote_X1, smote_y1, cv=5)))\n",
    "\n",
    "# Constructing the confusion matrix\n",
    "predictions_rfct4 = rfct4.predict(X1_test)\n",
    "predictions_proba_rfct4 = rfct4.predict_proba(X1_test)\n",
    "print(confusion_matrix(y1_test, predictions_rfct4))\n",
    "print(classification_report(y1_test, predictions_rfct4))\n",
    "print('AUC Score', roc_auc_score(y1_test, predictions_proba_rfct4[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    6.0s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   36.7s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 720 out of 720 | elapsed:  3.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 13, 'n_estimators': 17}\n",
      "0.9885416006496551\n",
      "{'mean_fit_time': array([0.60138202, 0.70600429, 0.82060242, 0.63868613, 0.91475863,\n",
      "       0.841748  , 0.6754971 , 1.00594077, 0.89739842, 0.95375876,\n",
      "       1.23589401, 0.80096312, 0.87772279, 0.74081736, 0.62822428,\n",
      "       0.81239672, 0.74141588, 0.72695684, 1.16811666, 0.82634506,\n",
      "       1.19078999, 1.31264315, 1.33188739, 1.3484478 , 0.91096234,\n",
      "       0.764747  , 0.78569841, 0.84339166, 0.92307873, 1.14871778,\n",
      "       1.22745004, 1.02247767, 1.3762382 , 1.26767969, 1.22452431,\n",
      "       1.19340739, 0.78190827, 0.71042671, 0.74420948, 0.90657473,\n",
      "       0.94950237, 0.97068748, 1.27919664, 1.07193165, 1.39245706,\n",
      "       1.7529099 , 1.45750136, 1.33841996, 1.06096191, 0.899194  ,\n",
      "       0.98524771, 1.04300966, 1.00072427, 1.06155963, 1.15630693,\n",
      "       1.43176894, 1.31029506, 1.56800556, 1.41361957, 1.40125151,\n",
      "       0.90178795, 0.86588383, 0.92233338, 0.89121537, 1.08270321,\n",
      "       0.98835602, 1.10464468, 1.20098729, 1.38848572, 1.45909653,\n",
      "       1.66753941, 1.21754279, 1.04939256, 0.93449993, 0.90497894,\n",
      "       1.02366199, 0.96920671, 1.3086988 , 1.06355443, 1.14314227,\n",
      "       1.58097062, 1.47206192, 1.50657034, 1.61129017, 0.95584283,\n",
      "       0.85395217, 0.74487944, 1.16142154, 1.26960239, 1.11514187,\n",
      "       1.6332746 , 1.38653417, 1.79759154, 1.90131316, 1.6573667 ,\n",
      "       1.75949388, 1.20278311, 0.96980519, 1.32226167, 1.07851415,\n",
      "       1.15331464, 1.36577621, 1.33502808, 1.33442988, 1.71700621,\n",
      "       1.59952068, 1.46188951, 1.72737927, 1.00032363, 0.85591102,\n",
      "       1.06155987, 0.90757113, 1.13596072, 1.20517421, 1.23788743,\n",
      "       1.19699817, 1.69526467, 1.59413586, 1.68289857, 1.7269804 ,\n",
      "       1.07632012, 0.89061646, 0.87924771, 1.12259712, 1.19899154,\n",
      "       1.08749065, 1.51774025, 1.22591982, 1.5506516 , 1.57917519,\n",
      "       1.74991994, 1.58894963, 1.25344706, 0.8850318 , 1.04719729,\n",
      "       0.91375556, 1.13496394, 1.15870256, 1.13975096, 1.29473624,\n",
      "       1.58137012, 1.74453306, 1.70523887, 1.64851403]),\n",
      " 'mean_score_time': array([0.30133438, 0.2867682 , 0.16253638, 0.45517526, 0.22979417,\n",
      "       0.31158004, 0.42237844, 0.14740353, 0.45937138, 0.36245041,\n",
      "       0.30840173, 0.38766642, 0.2114749 , 0.2337842 , 0.24342937,\n",
      "       0.23141794, 0.47353768, 0.47076559, 0.15067868, 0.67016392,\n",
      "       0.27646866, 0.3157001 , 0.42956858, 0.31840639, 0.23620753,\n",
      "       0.22103567, 0.35425277, 0.29840722, 0.49003282, 0.32195725,\n",
      "       0.383816  , 0.51607161, 0.5263823 , 0.48969021, 0.54115276,\n",
      "       0.40085697, 0.31267657, 0.38437228, 0.40292168, 0.32552962,\n",
      "       0.46061463, 0.43072376, 0.30184088, 0.86443357, 0.5456821 ,\n",
      "       0.38576922, 0.78490109, 0.3917522 , 0.29102149, 0.31110353,\n",
      "       0.23453984, 0.40790887, 0.40012846, 0.56847987, 0.52320075,\n",
      "       0.54035659, 0.54434505, 0.55691085, 0.53058391, 0.66262774,\n",
      "       0.32034283, 0.3107688 , 0.36881289, 0.41907926, 0.43403935,\n",
      "       0.52200775, 0.53895869, 0.63829422, 0.64846601, 0.5519237 ,\n",
      "       0.61695004, 0.61635146, 0.31735196, 0.30658031, 0.38157973,\n",
      "       0.43024964, 0.53875937, 0.44481063, 0.59341373, 0.61754832,\n",
      "       0.52918439, 0.59221621, 0.63530078, 0.51163177, 0.43483682,\n",
      "       0.2794302 , 0.45126185, 0.26588383, 0.62940044, 0.61491261,\n",
      "       0.45473833, 0.68908086, 0.48131247, 0.38158007, 0.83416872,\n",
      "       0.45717702, 0.18450713, 0.28563676, 0.30558372, 0.48071432,\n",
      "       0.70115976, 0.23797364, 0.66182647, 0.56688247, 0.52000985,\n",
      "       0.71309309, 0.65125847, 0.59700303, 0.40431962, 0.31814876,\n",
      "       0.34806943, 0.52858615, 0.58344007, 0.48849421, 0.7085063 ,\n",
      "       0.73583183, 0.72546029, 0.58842649, 0.66023417, 0.55810723,\n",
      "       0.33091626, 0.44560843, 0.45199175, 0.40032988, 0.54135246,\n",
      "       0.66362739, 0.52998261, 0.78609772, 0.62133827, 0.47074142,\n",
      "       0.72506032, 0.43164549, 0.33610077, 0.41010361, 0.39753814,\n",
      "       0.46296158, 0.61934381, 0.42705612, 0.63889203, 0.67359896,\n",
      "       0.67200289, 0.52200427, 0.71847854, 0.40140328]),\n",
      " 'mean_test_score': array([0.97297308, 0.97334672, 0.97309754, 0.97347119, 0.97359568,\n",
      "       0.97484117, 0.97384477, 0.9733466 , 0.97421843, 0.97521482,\n",
      "       0.97446753, 0.97434303, 0.98106855, 0.98206517, 0.98169141,\n",
      "       0.98144237, 0.98306143, 0.98206512, 0.98355969, 0.98368434,\n",
      "       0.98368428, 0.98418245, 0.98331057, 0.98393346, 0.98580144,\n",
      "       0.98468057, 0.98468051, 0.98505431, 0.98580149, 0.98592611,\n",
      "       0.98592601, 0.98567705, 0.9861752 , 0.98555254, 0.98754533,\n",
      "       0.98642442, 0.98517889, 0.98468061, 0.98617522, 0.9853034 ,\n",
      "       0.98642427, 0.98629993, 0.98717161, 0.98679802, 0.98679802,\n",
      "       0.98667346, 0.98742077, 0.98779432, 0.98580157, 0.98480516,\n",
      "       0.98592619, 0.98592613, 0.98754526, 0.98704712, 0.98841707,\n",
      "       0.98754531, 0.9885416 , 0.98729621, 0.98779441, 0.98804353,\n",
      "       0.9859262 , 0.984307  , 0.98704717, 0.98542799, 0.98766988,\n",
      "       0.98592627, 0.98754528, 0.98605067, 0.98729623, 0.98654893,\n",
      "       0.98717166, 0.98679802, 0.98692266, 0.98455616, 0.98667341,\n",
      "       0.98542804, 0.98804352, 0.98580172, 0.98841713, 0.98642435,\n",
      "       0.98779442, 0.98729631, 0.98829257, 0.9872963 , 0.98567716,\n",
      "       0.98268801, 0.98530353, 0.98306153, 0.98567687, 0.98368423,\n",
      "       0.98629971, 0.98505426, 0.9866734 , 0.98617523, 0.98704709,\n",
      "       0.9861753 , 0.98580146, 0.98268772, 0.98605057, 0.98331057,\n",
      "       0.98642432, 0.98468068, 0.98791888, 0.98567705, 0.98779434,\n",
      "       0.98605065, 0.98742074, 0.98692259, 0.98430701, 0.98119328,\n",
      "       0.98542798, 0.98355983, 0.98592603, 0.98368431, 0.9874208 ,\n",
      "       0.98492982, 0.98717176, 0.98492988, 0.98679816, 0.98642439,\n",
      "       0.98567713, 0.98194074, 0.98654889, 0.98318621, 0.98704712,\n",
      "       0.9841825 , 0.98791895, 0.98530343, 0.98791892, 0.98530344,\n",
      "       0.98754533, 0.98629987, 0.98517865, 0.98144221, 0.98580144,\n",
      "       0.98318594, 0.98742071, 0.98517885, 0.98854159, 0.98567705,\n",
      "       0.9877943 , 0.98492963, 0.98679787, 0.98530334]),\n",
      " 'mean_train_score': array([0.98371527, 0.98446254, 0.98455594, 0.98536552, 0.9853344 ,\n",
      "       0.98545895, 0.98508531, 0.98496076, 0.98471167, 0.98539669,\n",
      "       0.98561463, 0.98570805, 0.99464438, 0.99514258, 0.99501802,\n",
      "       0.99542281, 0.99542282, 0.99601441, 0.995921  , 0.99601441,\n",
      "       0.99623237, 0.99626351, 0.99623237, 0.99613896, 0.99816291,\n",
      "       0.99847428, 0.99859884, 0.99859882, 0.99875451, 0.9989102 ,\n",
      "       0.99900361, 0.99915929, 0.99900361, 0.99912816, 0.9991593 ,\n",
      "       0.9991593 , 0.99891022, 0.99897248, 0.99915929, 0.99919042,\n",
      "       0.99915928, 0.99931498, 0.99956407, 0.99953294, 0.99956407,\n",
      "       0.99953294, 0.99956409, 0.99968863, 0.99934612, 0.99922157,\n",
      "       0.99959521, 0.99943952, 0.99965749, 0.99965749, 0.99968863,\n",
      "       0.99971977, 0.99981318, 0.99971976, 0.99993772, 0.99993772,\n",
      "       0.99928385, 0.99934612, 0.99947067, 0.99956408, 0.99965749,\n",
      "       0.99968863, 0.9997509 , 0.99978204, 0.99984431, 0.99981318,\n",
      "       0.99987545, 0.99984431, 0.99934612, 0.99919044, 0.9996575 ,\n",
      "       0.99956408, 0.9997509 , 0.9997509 , 0.99987545, 0.99987545,\n",
      "       0.99987545, 0.99981318, 0.99987545, 0.99981318, 0.99940841,\n",
      "       0.99912817, 0.99956409, 0.99931498, 0.99978204, 0.99962636,\n",
      "       0.99987546, 0.99971977, 0.99987546, 0.99971977, 0.99993773,\n",
      "       0.99987546, 0.99934612, 0.99912816, 0.99962635, 0.99937725,\n",
      "       0.99968863, 0.99965749, 0.99981318, 0.99978204, 0.99978204,\n",
      "       0.99968864, 0.99990659, 0.99981319, 0.99953295, 0.99925271,\n",
      "       0.99968863, 0.99937726, 0.99971977, 0.99962636, 0.99984432,\n",
      "       0.9997509 , 0.99981318, 0.99971977, 0.99990659, 0.99984432,\n",
      "       0.99953294, 0.99906589, 0.99971976, 0.99922156, 0.99968863,\n",
      "       0.99956408, 0.99984432, 0.99978204, 0.99984432, 0.99978205,\n",
      "       0.99990659, 0.99987546, 0.99947068, 0.99900362, 0.99965749,\n",
      "       0.99928384, 0.99971977, 0.99953294, 0.99981318, 0.99971977,\n",
      "       0.99987545, 0.99975091, 0.99993773, 0.99987546]),\n",
      " 'param_max_depth': masked_array(data=[7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 9, 9, 9, 9, 9, 9,\n",
      "                   9, 9, 9, 9, 9, 9, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
      "                   11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "                   12, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 14,\n",
      "                   14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15,\n",
      "                   15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16,\n",
      "                   16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17,\n",
      "                   17, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "                   18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
      "                   19, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object),\n",
      " 'param_n_estimators': masked_array(data=[9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 9, 10,\n",
      "                   11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 9, 10, 11, 12,\n",
      "                   13, 14, 15, 16, 17, 18, 19, 20, 9, 10, 11, 12, 13, 14,\n",
      "                   15, 16, 17, 18, 19, 20, 9, 10, 11, 12, 13, 14, 15, 16,\n",
      "                   17, 18, 19, 20, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
      "                   19, 20, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n",
      "                   9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 9, 10,\n",
      "                   11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 9, 10, 11, 12,\n",
      "                   13, 14, 15, 16, 17, 18, 19, 20, 9, 10, 11, 12, 13, 14,\n",
      "                   15, 16, 17, 18, 19, 20, 9, 10, 11, 12, 13, 14, 15, 16,\n",
      "                   17, 18, 19, 20],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object),\n",
      " 'params': [{'max_depth': 7, 'n_estimators': 9},\n",
      "            {'max_depth': 7, 'n_estimators': 10},\n",
      "            {'max_depth': 7, 'n_estimators': 11},\n",
      "            {'max_depth': 7, 'n_estimators': 12},\n",
      "            {'max_depth': 7, 'n_estimators': 13},\n",
      "            {'max_depth': 7, 'n_estimators': 14},\n",
      "            {'max_depth': 7, 'n_estimators': 15},\n",
      "            {'max_depth': 7, 'n_estimators': 16},\n",
      "            {'max_depth': 7, 'n_estimators': 17},\n",
      "            {'max_depth': 7, 'n_estimators': 18},\n",
      "            {'max_depth': 7, 'n_estimators': 19},\n",
      "            {'max_depth': 7, 'n_estimators': 20},\n",
      "            {'max_depth': 9, 'n_estimators': 9},\n",
      "            {'max_depth': 9, 'n_estimators': 10},\n",
      "            {'max_depth': 9, 'n_estimators': 11},\n",
      "            {'max_depth': 9, 'n_estimators': 12},\n",
      "            {'max_depth': 9, 'n_estimators': 13},\n",
      "            {'max_depth': 9, 'n_estimators': 14},\n",
      "            {'max_depth': 9, 'n_estimators': 15},\n",
      "            {'max_depth': 9, 'n_estimators': 16},\n",
      "            {'max_depth': 9, 'n_estimators': 17},\n",
      "            {'max_depth': 9, 'n_estimators': 18},\n",
      "            {'max_depth': 9, 'n_estimators': 19},\n",
      "            {'max_depth': 9, 'n_estimators': 20},\n",
      "            {'max_depth': 11, 'n_estimators': 9},\n",
      "            {'max_depth': 11, 'n_estimators': 10},\n",
      "            {'max_depth': 11, 'n_estimators': 11},\n",
      "            {'max_depth': 11, 'n_estimators': 12},\n",
      "            {'max_depth': 11, 'n_estimators': 13},\n",
      "            {'max_depth': 11, 'n_estimators': 14},\n",
      "            {'max_depth': 11, 'n_estimators': 15},\n",
      "            {'max_depth': 11, 'n_estimators': 16},\n",
      "            {'max_depth': 11, 'n_estimators': 17},\n",
      "            {'max_depth': 11, 'n_estimators': 18},\n",
      "            {'max_depth': 11, 'n_estimators': 19},\n",
      "            {'max_depth': 11, 'n_estimators': 20},\n",
      "            {'max_depth': 12, 'n_estimators': 9},\n",
      "            {'max_depth': 12, 'n_estimators': 10},\n",
      "            {'max_depth': 12, 'n_estimators': 11},\n",
      "            {'max_depth': 12, 'n_estimators': 12},\n",
      "            {'max_depth': 12, 'n_estimators': 13},\n",
      "            {'max_depth': 12, 'n_estimators': 14},\n",
      "            {'max_depth': 12, 'n_estimators': 15},\n",
      "            {'max_depth': 12, 'n_estimators': 16},\n",
      "            {'max_depth': 12, 'n_estimators': 17},\n",
      "            {'max_depth': 12, 'n_estimators': 18},\n",
      "            {'max_depth': 12, 'n_estimators': 19},\n",
      "            {'max_depth': 12, 'n_estimators': 20},\n",
      "            {'max_depth': 13, 'n_estimators': 9},\n",
      "            {'max_depth': 13, 'n_estimators': 10},\n",
      "            {'max_depth': 13, 'n_estimators': 11},\n",
      "            {'max_depth': 13, 'n_estimators': 12},\n",
      "            {'max_depth': 13, 'n_estimators': 13},\n",
      "            {'max_depth': 13, 'n_estimators': 14},\n",
      "            {'max_depth': 13, 'n_estimators': 15},\n",
      "            {'max_depth': 13, 'n_estimators': 16},\n",
      "            {'max_depth': 13, 'n_estimators': 17},\n",
      "            {'max_depth': 13, 'n_estimators': 18},\n",
      "            {'max_depth': 13, 'n_estimators': 19},\n",
      "            {'max_depth': 13, 'n_estimators': 20},\n",
      "            {'max_depth': 14, 'n_estimators': 9},\n",
      "            {'max_depth': 14, 'n_estimators': 10},\n",
      "            {'max_depth': 14, 'n_estimators': 11},\n",
      "            {'max_depth': 14, 'n_estimators': 12},\n",
      "            {'max_depth': 14, 'n_estimators': 13},\n",
      "            {'max_depth': 14, 'n_estimators': 14},\n",
      "            {'max_depth': 14, 'n_estimators': 15},\n",
      "            {'max_depth': 14, 'n_estimators': 16},\n",
      "            {'max_depth': 14, 'n_estimators': 17},\n",
      "            {'max_depth': 14, 'n_estimators': 18},\n",
      "            {'max_depth': 14, 'n_estimators': 19},\n",
      "            {'max_depth': 14, 'n_estimators': 20},\n",
      "            {'max_depth': 15, 'n_estimators': 9},\n",
      "            {'max_depth': 15, 'n_estimators': 10},\n",
      "            {'max_depth': 15, 'n_estimators': 11},\n",
      "            {'max_depth': 15, 'n_estimators': 12},\n",
      "            {'max_depth': 15, 'n_estimators': 13},\n",
      "            {'max_depth': 15, 'n_estimators': 14},\n",
      "            {'max_depth': 15, 'n_estimators': 15},\n",
      "            {'max_depth': 15, 'n_estimators': 16},\n",
      "            {'max_depth': 15, 'n_estimators': 17},\n",
      "            {'max_depth': 15, 'n_estimators': 18},\n",
      "            {'max_depth': 15, 'n_estimators': 19},\n",
      "            {'max_depth': 15, 'n_estimators': 20},\n",
      "            {'max_depth': 16, 'n_estimators': 9},\n",
      "            {'max_depth': 16, 'n_estimators': 10},\n",
      "            {'max_depth': 16, 'n_estimators': 11},\n",
      "            {'max_depth': 16, 'n_estimators': 12},\n",
      "            {'max_depth': 16, 'n_estimators': 13},\n",
      "            {'max_depth': 16, 'n_estimators': 14},\n",
      "            {'max_depth': 16, 'n_estimators': 15},\n",
      "            {'max_depth': 16, 'n_estimators': 16},\n",
      "            {'max_depth': 16, 'n_estimators': 17},\n",
      "            {'max_depth': 16, 'n_estimators': 18},\n",
      "            {'max_depth': 16, 'n_estimators': 19},\n",
      "            {'max_depth': 16, 'n_estimators': 20},\n",
      "            {'max_depth': 17, 'n_estimators': 9},\n",
      "            {'max_depth': 17, 'n_estimators': 10},\n",
      "            {'max_depth': 17, 'n_estimators': 11},\n",
      "            {'max_depth': 17, 'n_estimators': 12},\n",
      "            {'max_depth': 17, 'n_estimators': 13},\n",
      "            {'max_depth': 17, 'n_estimators': 14},\n",
      "            {'max_depth': 17, 'n_estimators': 15},\n",
      "            {'max_depth': 17, 'n_estimators': 16},\n",
      "            {'max_depth': 17, 'n_estimators': 17},\n",
      "            {'max_depth': 17, 'n_estimators': 18},\n",
      "            {'max_depth': 17, 'n_estimators': 19},\n",
      "            {'max_depth': 17, 'n_estimators': 20},\n",
      "            {'max_depth': 18, 'n_estimators': 9},\n",
      "            {'max_depth': 18, 'n_estimators': 10},\n",
      "            {'max_depth': 18, 'n_estimators': 11},\n",
      "            {'max_depth': 18, 'n_estimators': 12},\n",
      "            {'max_depth': 18, 'n_estimators': 13},\n",
      "            {'max_depth': 18, 'n_estimators': 14},\n",
      "            {'max_depth': 18, 'n_estimators': 15},\n",
      "            {'max_depth': 18, 'n_estimators': 16},\n",
      "            {'max_depth': 18, 'n_estimators': 17},\n",
      "            {'max_depth': 18, 'n_estimators': 18},\n",
      "            {'max_depth': 18, 'n_estimators': 19},\n",
      "            {'max_depth': 18, 'n_estimators': 20},\n",
      "            {'max_depth': 19, 'n_estimators': 9},\n",
      "            {'max_depth': 19, 'n_estimators': 10},\n",
      "            {'max_depth': 19, 'n_estimators': 11},\n",
      "            {'max_depth': 19, 'n_estimators': 12},\n",
      "            {'max_depth': 19, 'n_estimators': 13},\n",
      "            {'max_depth': 19, 'n_estimators': 14},\n",
      "            {'max_depth': 19, 'n_estimators': 15},\n",
      "            {'max_depth': 19, 'n_estimators': 16},\n",
      "            {'max_depth': 19, 'n_estimators': 17},\n",
      "            {'max_depth': 19, 'n_estimators': 18},\n",
      "            {'max_depth': 19, 'n_estimators': 19},\n",
      "            {'max_depth': 19, 'n_estimators': 20},\n",
      "            {'max_depth': 20, 'n_estimators': 9},\n",
      "            {'max_depth': 20, 'n_estimators': 10},\n",
      "            {'max_depth': 20, 'n_estimators': 11},\n",
      "            {'max_depth': 20, 'n_estimators': 12},\n",
      "            {'max_depth': 20, 'n_estimators': 13},\n",
      "            {'max_depth': 20, 'n_estimators': 14},\n",
      "            {'max_depth': 20, 'n_estimators': 15},\n",
      "            {'max_depth': 20, 'n_estimators': 16},\n",
      "            {'max_depth': 20, 'n_estimators': 17},\n",
      "            {'max_depth': 20, 'n_estimators': 18},\n",
      "            {'max_depth': 20, 'n_estimators': 19},\n",
      "            {'max_depth': 20, 'n_estimators': 20}],\n",
      " 'rank_test_score': array([144, 141, 143, 140, 139, 134, 138, 142, 137, 133, 135, 136, 132,\n",
      "       125, 128, 129, 122, 126, 116, 111, 113, 109, 117, 110,  75, 103,\n",
      "       104,  95,  73,  68,  70,  79,  60,  83,  18,  49,  92, 102,  59,\n",
      "        90,  53,  54,  32,  42,  40,  44,  23,  14,  72, 100,  66,  67,\n",
      "        21,  35,   4,  19,   1,  29,  12,   6,  65, 107,  33,  85,  16,\n",
      "        64,  20,  61,  28,  47,  31,  40,  37, 105,  45,  84,   7,  71,\n",
      "         3,  51,  11,  26,   5,  27,  77, 123,  87, 121,  82, 114,  56,\n",
      "        96,  46,  58,  36,  57,  74, 124,  63, 118,  52, 101,  10,  81,\n",
      "        13,  62,  24,  38, 106, 131,  86, 115,  69, 112,  22,  98,  30,\n",
      "        97,  39,  50,  78, 127,  48, 119,  34, 108,   8,  89,   9,  88,\n",
      "        17,  55,  94, 130,  76, 120,  25,  93,   2,  79,  15,  99,  43,\n",
      "        91]),\n",
      " 'split0_test_score': array([0.96886675, 0.96762142, 0.96699875, 0.96575342, 0.96637609,\n",
      "       0.96699875, 0.96762142, 0.96575342, 0.96699875, 0.96637609,\n",
      "       0.96575342, 0.96575342, 0.9750934 , 0.97696139, 0.97758406,\n",
      "       0.97633873, 0.97945205, 0.97945205, 0.98069738, 0.98132005,\n",
      "       0.97945205, 0.98194271, 0.97945205, 0.98194271, 0.98443337,\n",
      "       0.98381071, 0.98443337, 0.98381071, 0.9856787 , 0.98630137,\n",
      "       0.98630137, 0.98505604, 0.98630137, 0.9856787 , 0.9875467 ,\n",
      "       0.98630137, 0.98443337, 0.98194271, 0.98630137, 0.98194271,\n",
      "       0.9856787 , 0.98443337, 0.98692403, 0.98816936, 0.9875467 ,\n",
      "       0.9875467 , 0.98816936, 0.9875467 , 0.98318804, 0.98007472,\n",
      "       0.98505604, 0.98256538, 0.98630137, 0.9856787 , 0.98816936,\n",
      "       0.98692403, 0.98879203, 0.9875467 , 0.98816936, 0.98816936,\n",
      "       0.98630137, 0.98069738, 0.98941469, 0.98194271, 0.98816936,\n",
      "       0.98505604, 0.9875467 , 0.9856787 , 0.98692403, 0.98630137,\n",
      "       0.98879203, 0.98816936, 0.98630137, 0.98007472, 0.98816936,\n",
      "       0.98505604, 0.98941469, 0.98630137, 0.99128269, 0.99003736,\n",
      "       0.99003736, 0.9875467 , 0.98941469, 0.98816936, 0.9856787 ,\n",
      "       0.98007472, 0.98630137, 0.98194271, 0.98505604, 0.98256538,\n",
      "       0.98505604, 0.98505604, 0.98630137, 0.9856787 , 0.98692403,\n",
      "       0.9856787 , 0.9856787 , 0.98194271, 0.98879203, 0.98381071,\n",
      "       0.98816936, 0.98630137, 0.99066002, 0.99003736, 0.99066002,\n",
      "       0.98816936, 0.99003736, 0.98941469, 0.98630137, 0.97758406,\n",
      "       0.9875467 , 0.98256538, 0.98505604, 0.98318804, 0.99003736,\n",
      "       0.9856787 , 0.98879203, 0.98505604, 0.9875467 , 0.9875467 ,\n",
      "       0.98879203, 0.97945205, 0.98816936, 0.98443337, 0.98692403,\n",
      "       0.98381071, 0.99066002, 0.98692403, 0.99003736, 0.9856787 ,\n",
      "       0.98941469, 0.9875467 , 0.98630137, 0.98007472, 0.98692403,\n",
      "       0.98381071, 0.98630137, 0.98443337, 0.99066002, 0.98692403,\n",
      "       0.99003736, 0.9856787 , 0.9875467 , 0.98692403]),\n",
      " 'split0_train_score': array([0.98863459, 0.9884789 , 0.98785614, 0.98785614, 0.9884789 ,\n",
      "       0.98879028, 0.98816752, 0.98770045, 0.98661062, 0.98676631,\n",
      "       0.98723338, 0.98785614, 0.99392807, 0.99423945, 0.99392807,\n",
      "       0.99470652, 0.99439514, 0.99532929, 0.99548498, 0.9950179 ,\n",
      "       0.99548498, 0.99564067, 0.99548498, 0.99532929, 0.9982874 ,\n",
      "       0.99813171, 0.99875448, 0.99813171, 0.99859879, 0.99875448,\n",
      "       0.99891017, 0.99906586, 0.99875448, 0.99906586, 0.99891017,\n",
      "       0.99891017, 0.99906586, 0.99891017, 0.99922155, 0.99906586,\n",
      "       0.99906586, 0.99937724, 0.99937724, 0.99953293, 0.99968862,\n",
      "       0.99953293, 0.99968862, 0.99968862, 0.99968862, 0.99953293,\n",
      "       0.99984431, 0.99984431, 1.        , 1.        , 0.99984431,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       0.99953293, 0.99937724, 1.        , 0.99968862, 1.        ,\n",
      "       1.        , 1.        , 0.99984431, 1.        , 0.99984431,\n",
      "       1.        , 1.        , 0.99953293, 0.99922155, 0.99984431,\n",
      "       0.99953293, 0.99984431, 1.        , 0.99984431, 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 0.99953293,\n",
      "       0.99875448, 0.99968862, 0.99891017, 1.        , 0.99984431,\n",
      "       1.        , 0.99968862, 1.        , 0.99984431, 1.        ,\n",
      "       1.        , 0.99953293, 0.99891017, 0.99984431, 0.99968862,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 0.99984431, 0.99906586,\n",
      "       0.99984431, 0.99953293, 1.        , 0.99984431, 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       0.99968862, 0.99891017, 0.99984431, 0.99937724, 1.        ,\n",
      "       0.99984431, 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 0.99984431, 0.99922155, 0.99984431,\n",
      "       0.99953293, 1.        , 0.99984431, 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        ]),\n",
      " 'split1_test_score': array([0.97820672, 0.97820672, 0.97945205, 0.98194271, 0.98381071,\n",
      "       0.98318804, 0.98256538, 0.98256538, 0.98256538, 0.98381071,\n",
      "       0.98318804, 0.98381071, 0.98505604, 0.98630137, 0.98692403,\n",
      "       0.98816936, 0.98816936, 0.98692403, 0.98692403, 0.98692403,\n",
      "       0.98692403, 0.9875467 , 0.9875467 , 0.9875467 , 0.9856787 ,\n",
      "       0.9856787 , 0.98692403, 0.9875467 , 0.98879203, 0.98816936,\n",
      "       0.9875467 , 0.98879203, 0.9875467 , 0.98816936, 0.99003736,\n",
      "       0.98816936, 0.98443337, 0.9856787 , 0.98630137, 0.98630137,\n",
      "       0.98816936, 0.9875467 , 0.98816936, 0.98692403, 0.98816936,\n",
      "       0.98630137, 0.9875467 , 0.98941469, 0.98381071, 0.98443337,\n",
      "       0.9856787 , 0.98816936, 0.9875467 , 0.9875467 , 0.98630137,\n",
      "       0.98816936, 0.98816936, 0.98630137, 0.98692403, 0.98941469,\n",
      "       0.98692403, 0.98692403, 0.98879203, 0.98879203, 0.98879203,\n",
      "       0.9875467 , 0.98879203, 0.98692403, 0.98816936, 0.98692403,\n",
      "       0.9875467 , 0.9875467 , 0.98443337, 0.98443337, 0.9856787 ,\n",
      "       0.98443337, 0.98816936, 0.9856787 , 0.98879203, 0.98443337,\n",
      "       0.9875467 , 0.98630137, 0.98879203, 0.98816936, 0.98692403,\n",
      "       0.98318804, 0.9856787 , 0.98318804, 0.98630137, 0.98443337,\n",
      "       0.98692403, 0.9856787 , 0.98879203, 0.98692403, 0.98879203,\n",
      "       0.98816936, 0.98443337, 0.98194271, 0.98381071, 0.98256538,\n",
      "       0.98630137, 0.98505604, 0.9875467 , 0.9856787 , 0.9875467 ,\n",
      "       0.98443337, 0.9875467 , 0.98692403, 0.98132005, 0.97820672,\n",
      "       0.98381071, 0.97945205, 0.9856787 , 0.98132005, 0.9856787 ,\n",
      "       0.98132005, 0.98505604, 0.98194271, 0.98692403, 0.9856787 ,\n",
      "       0.98318804, 0.98007472, 0.98381071, 0.98132005, 0.9856787 ,\n",
      "       0.98443337, 0.98630137, 0.98381071, 0.98630137, 0.98318804,\n",
      "       0.9875467 , 0.9856787 , 0.98194271, 0.97820672, 0.98256538,\n",
      "       0.98069738, 0.9856787 , 0.98381071, 0.98692403, 0.98381071,\n",
      "       0.98630137, 0.98318804, 0.98692403, 0.98505604]),\n",
      " 'split1_train_score': array([0.98209559, 0.98209559, 0.98349681, 0.98629924, 0.98614355,\n",
      "       0.9852094 , 0.98458664, 0.98489802, 0.98552078, 0.98661062,\n",
      "       0.98661062, 0.98661062, 0.993461  , 0.99392807, 0.99408376,\n",
      "       0.99439514, 0.99470652, 0.99486221, 0.99470652, 0.99532929,\n",
      "       0.99564067, 0.99610774, 0.99532929, 0.99517359, 0.99906586,\n",
      "       0.99922155, 0.99953293, 0.99953293, 0.99953293, 0.99953293,\n",
      "       0.99953293, 0.99968862, 0.99968862, 0.99968862, 0.99968862,\n",
      "       0.99953293, 0.99906586, 0.99875448, 0.99891017, 0.99906586,\n",
      "       0.99906586, 0.99953293, 0.99968862, 0.99968862, 0.99953293,\n",
      "       0.99953293, 0.99953293, 0.99968862, 0.99968862, 0.99922155,\n",
      "       0.99968862, 0.99922155, 0.99937724, 0.99984431, 0.99953293,\n",
      "       0.99984431, 0.99968862, 0.99968862, 0.99984431, 1.        ,\n",
      "       0.99937724, 0.99922155, 0.99937724, 0.99968862, 0.99953293,\n",
      "       0.99984431, 0.99984431, 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 0.99953293, 0.99937724, 1.        ,\n",
      "       0.99953293, 0.99984431, 0.99968862, 1.        , 0.99984431,\n",
      "       1.        , 1.        , 1.        , 1.        , 0.99984431,\n",
      "       0.99968862, 1.        , 0.99953293, 0.99968862, 0.99953293,\n",
      "       0.99984431, 0.99968862, 0.99984431, 0.99984431, 1.        ,\n",
      "       1.        , 0.99937724, 0.99906586, 0.99984431, 0.99937724,\n",
      "       0.99968862, 0.99953293, 0.99984431, 0.99968862, 0.99968862,\n",
      "       0.99968862, 0.99984431, 0.99984431, 0.99953293, 0.99937724,\n",
      "       0.99984431, 0.99937724, 0.99953293, 0.99937724, 0.99984431,\n",
      "       0.99953293, 0.99968862, 0.99968862, 0.99984431, 0.99984431,\n",
      "       0.99968862, 0.99922155, 0.99984431, 0.99906586, 0.99937724,\n",
      "       0.99922155, 0.99984431, 0.99968862, 0.99968862, 0.99968862,\n",
      "       0.99984431, 0.99984431, 0.99968862, 0.99906586, 0.99984431,\n",
      "       0.99906586, 0.99953293, 0.99922155, 0.99968862, 0.99953293,\n",
      "       0.99968862, 0.99968862, 1.        , 0.99984431]),\n",
      " 'split2_test_score': array([0.96637609, 0.96824408, 0.96575342, 0.96637609, 0.96637609,\n",
      "       0.97011208, 0.96637609, 0.96637609, 0.96699875, 0.96886675,\n",
      "       0.96762142, 0.96762142, 0.98007472, 0.98132005, 0.98007472,\n",
      "       0.98007472, 0.98194271, 0.98132005, 0.98256538, 0.98256538,\n",
      "       0.98318804, 0.98194271, 0.98256538, 0.98194271, 0.98692403,\n",
      "       0.9856787 , 0.98256538, 0.9856787 , 0.9856787 , 0.9856787 ,\n",
      "       0.98318804, 0.98381071, 0.98318804, 0.98318804, 0.98692403,\n",
      "       0.98630137, 0.98630137, 0.9875467 , 0.98692403, 0.9875467 ,\n",
      "       0.98692403, 0.9875467 , 0.98816936, 0.9875467 , 0.98692403,\n",
      "       0.98692403, 0.98692403, 0.98630137, 0.9875467 , 0.98816936,\n",
      "       0.98692403, 0.98879203, 0.99003736, 0.99066002, 0.99190535,\n",
      "       0.99003736, 0.99066002, 0.99003736, 0.99128269, 0.98941469,\n",
      "       0.9856787 , 0.98443337, 0.9856787 , 0.9856787 , 0.98816936,\n",
      "       0.98692403, 0.98816936, 0.98630137, 0.98816936, 0.98630137,\n",
      "       0.98692403, 0.98692403, 0.98816936, 0.98692403, 0.98692403,\n",
      "       0.98692403, 0.98692403, 0.98692403, 0.9875467 , 0.98692403,\n",
      "       0.98692403, 0.98692403, 0.9875467 , 0.98630137, 0.9875467 ,\n",
      "       0.98630137, 0.9856787 , 0.98443337, 0.9856787 , 0.9856787 ,\n",
      "       0.98692403, 0.98505604, 0.9875467 , 0.98816936, 0.9875467 ,\n",
      "       0.98630137, 0.98630137, 0.9856787 , 0.98630137, 0.98630137,\n",
      "       0.98630137, 0.98630137, 0.98692403, 0.98505604, 0.9875467 ,\n",
      "       0.98692403, 0.9875467 , 0.98630137, 0.9856787 , 0.9856787 ,\n",
      "       0.9856787 , 0.98505604, 0.9856787 , 0.9856787 , 0.9875467 ,\n",
      "       0.9875467 , 0.98630137, 0.9856787 , 0.98692403, 0.98692403,\n",
      "       0.9875467 , 0.98692403, 0.98879203, 0.98630137, 0.98816936,\n",
      "       0.98630137, 0.9875467 , 0.98692403, 0.9875467 , 0.98692403,\n",
      "       0.9875467 , 0.98692403, 0.98692403, 0.98692403, 0.9875467 ,\n",
      "       0.98692403, 0.98941469, 0.98816936, 0.98879203, 0.98692403,\n",
      "       0.9875467 , 0.9856787 , 0.98816936, 0.9856787 ]),\n",
      " 'split2_train_score': array([0.98147283, 0.98287405, 0.98349681, 0.98474233, 0.98411957,\n",
      "       0.98552078, 0.98443095, 0.98489802, 0.98443095, 0.98458664,\n",
      "       0.98505371, 0.9852094 , 0.99361669, 0.99470652, 0.99423945,\n",
      "       0.9950179 , 0.99564067, 0.99610774, 0.99610774, 0.99579636,\n",
      "       0.99626343, 0.99579636, 0.99641912, 0.99626343, 0.99766464,\n",
      "       0.9982874 , 0.99797602, 0.99813171, 0.9982874 , 0.99813171,\n",
      "       0.99813171, 0.9984431 , 0.99797602, 0.9984431 , 0.99859879,\n",
      "       0.99859879, 0.99891017, 0.99906586, 0.99891017, 0.99875448,\n",
      "       0.99875448, 0.99906586, 0.99937724, 0.99937724, 0.99953293,\n",
      "       0.99937724, 0.99953293, 0.99968862, 0.99937724, 0.99937724,\n",
      "       0.99922155, 0.99922155, 0.99953293, 0.99922155, 0.99968862,\n",
      "       0.99953293, 0.99968862, 0.99953293, 0.99984431, 0.99968862,\n",
      "       0.99937724, 0.99953293, 0.99922155, 0.99937724, 0.99953293,\n",
      "       0.99953293, 0.99968862, 0.99984431, 0.99968862, 0.99953293,\n",
      "       0.99953293, 0.99953293, 0.99922155, 0.99953293, 0.99937724,\n",
      "       0.99937724, 0.99953293, 0.99953293, 0.99984431, 0.99984431,\n",
      "       0.99968862, 0.99968862, 0.99968862, 0.99968862, 0.99922155,\n",
      "       0.99891017, 0.99922155, 0.99922155, 0.99968862, 0.99953293,\n",
      "       0.99984431, 0.99984431, 0.99984431, 0.99968862, 0.99984431,\n",
      "       0.99984431, 0.99906586, 0.99906586, 0.99906586, 0.99875448,\n",
      "       0.99906586, 0.99937724, 0.99937724, 0.99968862, 0.99953293,\n",
      "       0.99968862, 0.99984431, 0.99984431, 0.99937724, 0.99906586,\n",
      "       0.99922155, 0.99891017, 0.99953293, 0.99953293, 0.99968862,\n",
      "       0.99968862, 0.99968862, 0.99968862, 0.99984431, 0.99984431,\n",
      "       0.99906586, 0.99875448, 0.99906586, 0.99859879, 0.99937724,\n",
      "       0.99937724, 0.99968862, 0.99968862, 0.99984431, 0.99984431,\n",
      "       0.99984431, 0.99984431, 0.99922155, 0.99891017, 0.99906586,\n",
      "       0.99891017, 0.99953293, 0.99953293, 0.99968862, 0.99968862,\n",
      "       0.99984431, 0.99984431, 0.99984431, 0.99984431]),\n",
      " 'split3_test_score': array([0.97384807, 0.97447073, 0.97571606, 0.97571606, 0.97571606,\n",
      "       0.97696139, 0.97633873, 0.97571606, 0.97696139, 0.97820672,\n",
      "       0.97758406, 0.97633873, 0.98318804, 0.98069738, 0.98132005,\n",
      "       0.98007472, 0.98256538, 0.98007472, 0.98256538, 0.98132005,\n",
      "       0.98256538, 0.98318804, 0.98256538, 0.98132005, 0.98630137,\n",
      "       0.98381071, 0.98505604, 0.98256538, 0.98443337, 0.98381071,\n",
      "       0.98630137, 0.98381071, 0.9856787 , 0.98318804, 0.98443337,\n",
      "       0.98318804, 0.98318804, 0.98318804, 0.98443337, 0.98381071,\n",
      "       0.98505604, 0.98256538, 0.98505604, 0.98381071, 0.98381071,\n",
      "       0.98443337, 0.98505604, 0.98692403, 0.9856787 , 0.98443337,\n",
      "       0.98381071, 0.98381071, 0.9856787 , 0.98381071, 0.98692403,\n",
      "       0.98443337, 0.98692403, 0.98443337, 0.98443337, 0.98443337,\n",
      "       0.98318804, 0.98318804, 0.98318804, 0.98318804, 0.98443337,\n",
      "       0.98194271, 0.98505604, 0.98443337, 0.98443337, 0.98443337,\n",
      "       0.98443337, 0.98381071, 0.98443337, 0.98318804, 0.98505604,\n",
      "       0.98256538, 0.9856787 , 0.98194271, 0.9856787 , 0.98381071,\n",
      "       0.98505604, 0.98443337, 0.98630137, 0.98381071, 0.98132005,\n",
      "       0.97882939, 0.98132005, 0.98132005, 0.98630137, 0.98194271,\n",
      "       0.9856787 , 0.98381071, 0.98443337, 0.98381071, 0.98443337,\n",
      "       0.98318804, 0.98630137, 0.98256538, 0.9856787 , 0.98132005,\n",
      "       0.98443337, 0.98132005, 0.98630137, 0.98256538, 0.9856787 ,\n",
      "       0.98443337, 0.98443337, 0.98381071, 0.98256538, 0.98069738,\n",
      "       0.98318804, 0.98194271, 0.98630137, 0.98194271, 0.98443337,\n",
      "       0.98256538, 0.98443337, 0.98256538, 0.98256538, 0.98381071,\n",
      "       0.98194271, 0.97820672, 0.98443337, 0.97882939, 0.98505604,\n",
      "       0.98132005, 0.9856787 , 0.98256538, 0.98630137, 0.98318804,\n",
      "       0.98443337, 0.98318804, 0.98630137, 0.98194271, 0.98630137,\n",
      "       0.98256538, 0.98630137, 0.98318804, 0.9875467 , 0.98381071,\n",
      "       0.98692403, 0.98505604, 0.9856787 , 0.98381071]),\n",
      " 'split3_train_score': array([0.98225128, 0.98318543, 0.98209559, 0.98162852, 0.98209559,\n",
      "       0.9819399 , 0.98302974, 0.98225128, 0.98225128, 0.98380819,\n",
      "       0.98287405, 0.98287405, 0.99657481, 0.99688619, 0.99641912,\n",
      "       0.99641912, 0.99610774, 0.99641912, 0.99626343, 0.9967305 ,\n",
      "       0.99626343, 0.99626343, 0.99657481, 0.99657481, 0.99766464,\n",
      "       0.9982874 , 0.9984431 , 0.9982874 , 0.99859879, 0.99906586,\n",
      "       0.99922155, 0.99906586, 0.99953293, 0.99922155, 0.99953293,\n",
      "       0.99953293, 0.99906586, 0.99937724, 0.99937724, 0.99953293,\n",
      "       0.99922155, 0.99922155, 0.99953293, 0.99937724, 0.99922155,\n",
      "       0.99953293, 0.99968862, 0.99968862, 0.99875448, 0.99859879,\n",
      "       0.99953293, 0.99922155, 0.99968862, 0.99953293, 0.99984431,\n",
      "       0.99953293, 0.99984431, 0.99953293, 1.        , 1.        ,\n",
      "       0.99906586, 0.99922155, 0.99937724, 0.99937724, 0.99953293,\n",
      "       0.99937724, 0.99953293, 0.99953293, 0.99968862, 0.99984431,\n",
      "       0.99984431, 0.99984431, 0.99922155, 0.99891017, 0.99953293,\n",
      "       0.99968862, 0.99984431, 0.99984431, 0.99984431, 0.99984431,\n",
      "       0.99984431, 0.99968862, 0.99968862, 0.99968862, 0.99937724,\n",
      "       0.99937724, 0.99953293, 0.99953293, 0.99984431, 0.99968862,\n",
      "       1.        , 0.99968862, 1.        , 0.99968862, 1.        ,\n",
      "       1.        , 0.99953293, 0.99953293, 0.99968862, 0.99953293,\n",
      "       1.        , 0.99968862, 1.        , 0.99984431, 1.        ,\n",
      "       0.99968862, 1.        , 0.99984431, 0.99968862, 0.99968862,\n",
      "       1.        , 0.99984431, 1.        , 0.99984431, 1.        ,\n",
      "       0.99984431, 1.        , 0.99968862, 1.        , 0.99984431,\n",
      "       0.99968862, 0.99937724, 1.        , 0.99953293, 1.        ,\n",
      "       0.99968862, 1.        , 0.99984431, 1.        , 0.99984431,\n",
      "       1.        , 1.        , 0.99953293, 0.99922155, 0.99984431,\n",
      "       0.99937724, 0.99984431, 0.99953293, 1.        , 0.99968862,\n",
      "       1.        , 0.99984431, 1.        , 1.        ]),\n",
      " 'split4_test_score': array([0.97757009, 0.97819315, 0.97757009, 0.97757009, 0.97570093,\n",
      "       0.97694704, 0.97632399, 0.97632399, 0.97757009, 0.9788162 ,\n",
      "       0.97819315, 0.97819315, 0.98193146, 0.98504673, 0.98255452,\n",
      "       0.98255452, 0.98317757, 0.98255452, 0.98504673, 0.98629283,\n",
      "       0.98629283, 0.98629283, 0.98442368, 0.98691589, 0.98566978,\n",
      "       0.98442368, 0.98442368, 0.98566978, 0.98442368, 0.98566978,\n",
      "       0.98629283, 0.98691589, 0.98816199, 0.98753894, 0.98878505,\n",
      "       0.98816199, 0.98753894, 0.98504673, 0.98691589, 0.98691589,\n",
      "       0.98629283, 0.9894081 , 0.98753894, 0.98753894, 0.98753894,\n",
      "       0.98816199, 0.9894081 , 0.98878505, 0.98878505, 0.98691589,\n",
      "       0.98816199, 0.98629283, 0.98816199, 0.98753894, 0.98878505,\n",
      "       0.98816199, 0.98816199, 0.98816199, 0.98816199, 0.98878505,\n",
      "       0.98753894, 0.98629283, 0.98816199, 0.98753894, 0.98878505,\n",
      "       0.98816199, 0.98816199, 0.98691589, 0.98878505, 0.98878505,\n",
      "       0.98816199, 0.98753894, 0.99127726, 0.98816199, 0.98753894,\n",
      "       0.98816199, 0.99003115, 0.98816199, 0.98878505, 0.98691589,\n",
      "       0.9894081 , 0.99127726, 0.9894081 , 0.99003115, 0.98691589,\n",
      "       0.98504673, 0.98753894, 0.98442368, 0.98504673, 0.98380062,\n",
      "       0.98691589, 0.98566978, 0.98629283, 0.98629283, 0.98753894,\n",
      "       0.98753894, 0.98629283, 0.98130841, 0.98566978, 0.98255452,\n",
      "       0.98691589, 0.98442368, 0.98816199, 0.98504673, 0.98753894,\n",
      "       0.98629283, 0.98753894, 0.98816199, 0.98566978, 0.98380062,\n",
      "       0.98691589, 0.98878505, 0.98691589, 0.98629283, 0.9894081 ,\n",
      "       0.98753894, 0.99127726, 0.9894081 , 0.99003115, 0.98816199,\n",
      "       0.98691589, 0.98504673, 0.98753894, 0.98504673, 0.9894081 ,\n",
      "       0.98504673, 0.9894081 , 0.98629283, 0.9894081 , 0.98753894,\n",
      "       0.98878505, 0.98816199, 0.98442368, 0.98006231, 0.98566978,\n",
      "       0.98193146, 0.9894081 , 0.98629283, 0.98878505, 0.98691589,\n",
      "       0.98816199, 0.98504673, 0.98566978, 0.98504673]),\n",
      " 'split4_train_score': array([0.98412204, 0.9856787 , 0.98583437, 0.98630137, 0.98583437,\n",
      "       0.98583437, 0.98521171, 0.98505604, 0.98474471, 0.98521171,\n",
      "       0.98630137, 0.98599004, 0.99564134, 0.99595268, 0.99641968,\n",
      "       0.99657534, 0.99626401, 0.99735367, 0.99704234, 0.99719801,\n",
      "       0.99750934, 0.99750934, 0.99735367, 0.99735367, 0.998132  ,\n",
      "       0.99844334, 0.99828767, 0.99891034, 0.99875467, 0.999066  ,\n",
      "       0.99922167, 0.999533  , 0.999066  , 0.99922167, 0.999066  ,\n",
      "       0.99922167, 0.99844334, 0.99875467, 0.99937733, 0.999533  ,\n",
      "       0.99968867, 0.99937733, 0.99984433, 0.99968867, 0.99984433,\n",
      "       0.99968867, 0.99937733, 0.99968867, 0.99922167, 0.99937733,\n",
      "       0.99968867, 0.99968867, 0.99968867, 0.99968867, 0.999533  ,\n",
      "       0.99968867, 0.99984433, 0.99984433, 1.        , 1.        ,\n",
      "       0.999066  , 0.99937733, 0.99937733, 0.99968867, 0.99968867,\n",
      "       0.99968867, 0.99968867, 0.99968867, 0.99984433, 0.99984433,\n",
      "       1.        , 0.99984433, 0.99922167, 0.99891034, 0.999533  ,\n",
      "       0.99968867, 0.99968867, 0.99968867, 0.99984433, 0.99984433,\n",
      "       0.99984433, 0.99968867, 1.        , 0.99968867, 0.999066  ,\n",
      "       0.99891034, 0.99937733, 0.99937733, 0.99968867, 0.999533  ,\n",
      "       0.99968867, 0.99968867, 0.99968867, 0.999533  , 0.99984433,\n",
      "       0.999533  , 0.99922167, 0.999066  , 0.99968867, 0.999533  ,\n",
      "       0.99968867, 0.99968867, 0.99984433, 0.99968867, 0.99968867,\n",
      "       0.99937733, 0.99984433, 0.999533  , 0.99922167, 0.999066  ,\n",
      "       0.999533  , 0.99922167, 0.999533  , 0.999533  , 0.99968867,\n",
      "       0.99968867, 0.99968867, 0.999533  , 0.99984433, 0.99968867,\n",
      "       0.999533  , 0.999066  , 0.99984433, 0.999533  , 0.99968867,\n",
      "       0.99968867, 0.99968867, 0.99968867, 0.99968867, 0.999533  ,\n",
      "       0.99984433, 0.99968867, 0.999066  , 0.998599  , 0.99968867,\n",
      "       0.999533  , 0.99968867, 0.999533  , 0.99968867, 0.99968867,\n",
      "       0.99984433, 0.99937733, 0.99984433, 0.99968867]),\n",
      " 'std_fit_time': array([0.16410513, 0.19777399, 0.08577988, 0.16964582, 0.22021216,\n",
      "       0.21137681, 0.20374071, 0.1378607 , 0.21882928, 0.23600398,\n",
      "       0.21506785, 0.26302365, 0.25143799, 0.09510689, 0.16388513,\n",
      "       0.04061722, 0.18410827, 0.14433739, 0.07200249, 0.23056583,\n",
      "       0.18527069, 0.07886207, 0.18264207, 0.29760742, 0.14016626,\n",
      "       0.14232984, 0.1178633 , 0.12690391, 0.2673891 , 0.29531357,\n",
      "       0.24690782, 0.31998941, 0.11506458, 0.17733791, 0.11530923,\n",
      "       0.14302835, 0.11102372, 0.14125062, 0.08217483, 0.04724879,\n",
      "       0.1781807 , 0.18816694, 0.11068395, 0.40178535, 0.20612102,\n",
      "       0.11600188, 0.3951881 , 0.52144625, 0.12208515, 0.22285666,\n",
      "       0.07968782, 0.16458839, 0.18876096, 0.1737989 , 0.13231742,\n",
      "       0.19125623, 0.08486931, 0.18759085, 0.14769238, 0.13084278,\n",
      "       0.18385697, 0.08065519, 0.07476032, 0.17073558, 0.10755806,\n",
      "       0.12284619, 0.11909106, 0.15760027, 0.15496807, 0.15779941,\n",
      "       0.22512406, 0.14674696, 0.22166947, 0.17056983, 0.15495436,\n",
      "       0.13571519, 0.16522252, 0.08070987, 0.30205336, 0.14718056,\n",
      "       0.0535489 , 0.18518511, 0.11999646, 0.0554238 , 0.11050472,\n",
      "       0.06903665, 0.0914478 , 0.09311277, 0.39078527, 0.35451173,\n",
      "       0.06874596, 0.34378589, 0.20767351, 0.235805  , 0.40855732,\n",
      "       0.60247376, 0.04671466, 0.29522867, 0.0880319 , 0.25953432,\n",
      "       0.23506994, 0.33247179, 0.37516792, 0.44437719, 0.27976981,\n",
      "       0.27027939, 0.09991902, 0.31814974, 0.14547003, 0.07590851,\n",
      "       0.19326786, 0.22684171, 0.1794894 , 0.16386883, 0.24651671,\n",
      "       0.20353699, 0.19293637, 0.13459744, 0.14926483, 0.27439193,\n",
      "       0.31317161, 0.17051229, 0.10413696, 0.07832101, 0.17759004,\n",
      "       0.19134279, 0.07324087, 0.32931311, 0.24832925, 0.1169858 ,\n",
      "       0.30208924, 0.31180077, 0.17948679, 0.07606904, 0.2047899 ,\n",
      "       0.07657406, 0.1316036 , 0.19649716, 0.11262792, 0.08892979,\n",
      "       0.03912056, 0.11860114, 0.20531649, 0.09609416]),\n",
      " 'std_score_time': array([0.15912354, 0.21761369, 0.0904882 , 0.22147858, 0.20561717,\n",
      "       0.18883381, 0.23039419, 0.02609135, 0.22645928, 0.25000075,\n",
      "       0.17534837, 0.28995255, 0.10986294, 0.09612108, 0.10270577,\n",
      "       0.07554766, 0.14185221, 0.29763531, 0.0523842 , 0.12647846,\n",
      "       0.19420671, 0.18533388, 0.17148008, 0.14558918, 0.09670327,\n",
      "       0.0754986 , 0.26081524, 0.20543407, 0.25488964, 0.24930546,\n",
      "       0.13879268, 0.31842358, 0.21634459, 0.09483158, 0.2107084 ,\n",
      "       0.11141385, 0.20261007, 0.16196409, 0.18150812, 0.1953789 ,\n",
      "       0.17806767, 0.2502437 , 0.15899234, 0.23341981, 0.24678645,\n",
      "       0.18732423, 0.37025186, 0.13020619, 0.18972603, 0.20929259,\n",
      "       0.11056199, 0.11453555, 0.09503427, 0.15966701, 0.26389454,\n",
      "       0.15318126, 0.15148352, 0.09200712, 0.18156854, 0.23703568,\n",
      "       0.11925918, 0.17674059, 0.17407619, 0.14768086, 0.01354864,\n",
      "       0.10254301, 0.08637742, 0.07974958, 0.26605581, 0.17337136,\n",
      "       0.05413103, 0.1294986 , 0.07083   , 0.09847217, 0.11079625,\n",
      "       0.16711003, 0.2285518 , 0.11126929, 0.24433452, 0.18754067,\n",
      "       0.12425758, 0.12769529, 0.16380446, 0.23902468, 0.14365316,\n",
      "       0.18104195, 0.26015468, 0.18046158, 0.24881154, 0.43654825,\n",
      "       0.16664975, 0.39103226, 0.22035827, 0.15388882, 0.29525561,\n",
      "       0.12127915, 0.08029071, 0.18276958, 0.2766517 , 0.16157805,\n",
      "       0.29443803, 0.19423186, 0.31895562, 0.41197813, 0.04401245,\n",
      "       0.17242381, 0.18114538, 0.21827235, 0.17520246, 0.13454677,\n",
      "       0.17489565, 0.1153809 , 0.20406408, 0.13802627, 0.1207653 ,\n",
      "       0.27653895, 0.29123205, 0.22641224, 0.15976198, 0.24937787,\n",
      "       0.04488462, 0.2029704 , 0.17732056, 0.13243826, 0.05827818,\n",
      "       0.36168776, 0.11320851, 0.09932252, 0.13844313, 0.18408877,\n",
      "       0.19600505, 0.14218212, 0.14652159, 0.31894009, 0.09423715,\n",
      "       0.03499134, 0.15572004, 0.1026781 , 0.13972427, 0.08338607,\n",
      "       0.14498436, 0.12420603, 0.08804722, 0.24400671]),\n",
      " 'std_test_score': array([0.00468352, 0.00463013, 0.00562819, 0.00638014, 0.00659617,\n",
      "       0.00570066, 0.00604893, 0.00641456, 0.006208  , 0.00654605,\n",
      "       0.00667035, 0.00674431, 0.00340099, 0.00332604, 0.00309042,\n",
      "       0.00390607, 0.0028508 , 0.0026532 , 0.00217726, 0.0024385 ,\n",
      "       0.00270969, 0.00231495, 0.0026528 , 0.00270946, 0.00082639,\n",
      "       0.00084524, 0.00139832, 0.00171595, 0.0015965 , 0.00139822,\n",
      "       0.00145195, 0.00192826, 0.00173286, 0.00209719, 0.00188769,\n",
      "       0.00182027, 0.00154292, 0.00195298, 0.00091376, 0.00210486,\n",
      "       0.00107151, 0.00245752, 0.00115436, 0.00154462, 0.00154462,\n",
      "       0.00128035, 0.00143963, 0.0011537 , 0.0021333 , 0.00277236,\n",
      "       0.00150242, 0.00241465, 0.00152465, 0.00227563, 0.00195307,\n",
      "       0.00184663, 0.00122065, 0.00187147, 0.00221353, 0.00186323,\n",
      "       0.00150298, 0.00224014, 0.00230902, 0.00256911, 0.00164168,\n",
      "       0.00224692, 0.00130538, 0.00093036, 0.001554  , 0.0013955 ,\n",
      "       0.00150372, 0.00154462, 0.00258032, 0.0028488 , 0.0011537 ,\n",
      "       0.00195102, 0.00159316, 0.00209686, 0.00183   , 0.00220644,\n",
      "       0.00178615, 0.00224632, 0.00120613, 0.00210428, 0.00226129,\n",
      "       0.00284913, 0.00210425, 0.00126787, 0.00055901, 0.00132951,\n",
      "       0.00078635, 0.00068044, 0.00145275, 0.00144144, 0.00144102,\n",
      "       0.00173327, 0.00072501, 0.00154753, 0.00160502, 0.00169026,\n",
      "       0.00120672, 0.0018305 , 0.00150456, 0.00242814, 0.00160478,\n",
      "       0.00145206, 0.00177859, 0.00188764, 0.00198356, 0.0031341 ,\n",
      "       0.00169696, 0.00316269, 0.00063241, 0.00198242, 0.00213403,\n",
      "       0.00256271, 0.00253803, 0.00265101, 0.00240648, 0.00154367,\n",
      "       0.00264101, 0.00340879, 0.00203038, 0.00272708, 0.00159273,\n",
      "       0.00165112, 0.0018711 , 0.00178644, 0.00155415, 0.0018283 ,\n",
      "       0.0017155 , 0.0017595 , 0.00182266, 0.00298492, 0.00173479,\n",
      "       0.00212216, 0.00164109, 0.00182076, 0.00128195, 0.00152389,\n",
      "       0.00128179, 0.00091494, 0.00099826, 0.00101219]),\n",
      " 'std_train_score': array([2.61382283e-03, 2.33952980e-03, 2.04071270e-03, 2.11207736e-03,\n",
      "       2.13346058e-03, 2.17662669e-03, 1.69813126e-03, 1.72418778e-03,\n",
      "       1.44180872e-03, 1.14570573e-03, 1.54333287e-03, 1.66064263e-03,\n",
      "       1.24017184e-03, 1.11150819e-03, 1.14845235e-03, 9.00452982e-04,\n",
      "       7.47445237e-04, 8.67534226e-04, 7.84168945e-04, 8.27484604e-04,\n",
      "       7.12922541e-04, 6.60685036e-04, 7.46139616e-04, 8.08515821e-04,\n",
      "       5.15423725e-04, 3.86409734e-04, 5.30231392e-04, 5.48262054e-04,\n",
      "       4.17761423e-04, 4.61862430e-04, 4.78363575e-04, 4.35945829e-04,\n",
      "       6.11769512e-04, 4.01191968e-04, 4.01179554e-04, 3.63133656e-04,\n",
      "       2.41100802e-04, 2.32979922e-04, 2.11208866e-04, 3.01911555e-04,\n",
      "       3.05106622e-04, 1.58781376e-04, 1.81572230e-04, 1.39264645e-04,\n",
      "       2.06553349e-04, 9.84826387e-05, 1.16477004e-04, 1.93886037e-08,\n",
      "       3.46730476e-04, 3.26588367e-04, 2.11193141e-04, 2.71464537e-04,\n",
      "       2.06548234e-04, 2.67861170e-04, 1.39237549e-04, 1.81563087e-04,\n",
      "       1.16509388e-04, 1.81568074e-04, 7.62724503e-05, 1.24552390e-04,\n",
      "       1.86794661e-04, 1.16513281e-04, 2.71448971e-04, 1.52552817e-04,\n",
      "       1.81566412e-04, 2.20179600e-04, 1.58769965e-04, 1.58768064e-04,\n",
      "       1.39253806e-04, 1.52545890e-04, 1.81564748e-04, 1.70550384e-04,\n",
      "       1.52525119e-04, 2.49066615e-04, 2.28809437e-04, 1.16518457e-04,\n",
      "       1.24547544e-04, 1.58769965e-04, 6.22737721e-05, 6.22737721e-05,\n",
      "       1.16506797e-04, 1.52536986e-04, 1.52544901e-04, 1.52536986e-04,\n",
      "       2.67822857e-04, 3.49503124e-04, 2.67846522e-04, 2.33021370e-04,\n",
      "       1.24545120e-04, 1.24541487e-04, 1.16492547e-04, 6.22713506e-05,\n",
      "       1.16492547e-04, 1.16484775e-04, 7.62665142e-05, 1.81537316e-04,\n",
      "       1.81548129e-04, 2.11180284e-04, 2.88764928e-04, 3.26586055e-04,\n",
      "       3.41100768e-04, 2.06548234e-04, 2.28818011e-04, 1.24545120e-04,\n",
      "       1.86823739e-04, 1.96903965e-04, 7.62684932e-05, 1.52518187e-04,\n",
      "       2.20145327e-04, 2.49082973e-04, 2.78499483e-04, 3.11368860e-04,\n",
      "       2.28805480e-04, 1.86821316e-04, 1.39242968e-04, 1.58769965e-04,\n",
      "       1.52536986e-04, 1.52527093e-04, 7.62684932e-05, 9.84519827e-05,\n",
      "       2.41194668e-04, 2.20179606e-04, 3.32465719e-04, 3.55041690e-04,\n",
      "       2.78507611e-04, 2.28822629e-04, 1.39242968e-04, 1.24545120e-04,\n",
      "       1.39242968e-04, 1.58750953e-04, 7.62684932e-05, 1.16492547e-04,\n",
      "       2.88722070e-04, 2.32940405e-04, 3.01896056e-04, 2.52981420e-04,\n",
      "       1.81563087e-04, 1.96934622e-04, 1.52536986e-04, 1.52542923e-04,\n",
      "       1.16506797e-04, 2.11154548e-04, 7.62665142e-05, 1.16492547e-04])}\n"
     ]
    }
   ],
   "source": [
    "# Lets try to optimzie with GridSearch\n",
    "\n",
    "rfc5_params = {'n_estimators':[9,10,11,12,13,14,15,16,17,18,19,20],\n",
    "              'max_depth':[7,9,11,12,13,14,15,16,17,18,19,20]}\n",
    "\n",
    "rfc5_gridsearch = GridSearchCV(RandomForestClassifier(criterion='entropy', max_features=7, n_jobs=-1,\n",
    "                                                      random_state=0), rfc5_params, cv=5, verbose=1, scoring='recall', n_jobs=-1)\n",
    "\n",
    "rfc5_gridsearch.fit(smote_X1, smote_y1)\n",
    "\n",
    "print(rfc5_gridsearch.best_params_)\n",
    "print(rfc5_gridsearch.best_score_)\n",
    "pp.pprint(rfc5_gridsearch.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores: [0.96661219 0.97174877 0.96777959 0.97174218 0.9686989 ]\n",
      "Mean Cross-Validation Score: 0.9693163268727902\n",
      "[[3247  112]\n",
      " [  18   53]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98      3359\n",
      "           1       0.32      0.75      0.45        71\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      3430\n",
      "   macro avg       0.66      0.86      0.71      3430\n",
      "weighted avg       0.98      0.96      0.97      3430\n",
      "\n",
      "AUC Score 0.9519663380700997\n"
     ]
    }
   ],
   "source": [
    "rfc5 = RandomForestClassifier(n_estimators=17, criterion='entropy', max_features=7, n_jobs=-1, random_state=0, max_depth=13)\n",
    "\n",
    "# Fitting the model\n",
    "rfct5 = rfc5.fit(smote_X1, smote_y1)\n",
    "\n",
    "\n",
    "# Checking cross-validation values\n",
    "print('Cross-Validation Scores:', cross_val_score(rfct5, smote_X1, smote_y1, cv=5))\n",
    "print('Mean Cross-Validation Score:', np.mean(cross_val_score(rfct5, smote_X1, smote_y1, cv=5)))\n",
    "\n",
    "# Constructing the confusion matrix\n",
    "predictions_rfct5 = rfct5.predict(X1_test)\n",
    "predictions_proba_rfct5 = rfct5.predict_proba(X1_test)\n",
    "print(confusion_matrix(y1_test, predictions_rfct5))\n",
    "print(classification_report(y1_test, predictions_rfct5))\n",
    "print('AUC Score', roc_auc_score(y1_test, predictions_proba_rfct5[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END\n",
    "Experiment with random forest ends here. Conclusion is it performs even worse than Logistic Regression with SMOTE\n",
    "\n",
    "I wanna try Logistic Regression with some SMOTE tinkering before I conclude logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversampling the data with SMOTE\n",
    "sm = SMOTE(sampling_strategy=0.6, random_state=7, k_neighbors=8)\n",
    "smote_X1, smote_y1 = sm.fit_sample(X1_train, y1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores: [0.92108335 0.9266869  0.92411861 0.93017282 0.92128007]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Cross-Validation Score: 0.9246683508272676\n",
      "[[3154  205]\n",
      " [  10   61]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.94      0.97      3359\n",
      "           1       0.23      0.86      0.36        71\n",
      "\n",
      "   micro avg       0.94      0.94      0.94      3430\n",
      "   macro avg       0.61      0.90      0.66      3430\n",
      "weighted avg       0.98      0.94      0.95      3430\n",
      "\n",
      "AUC Score 0.9673863364767348\n"
     ]
    }
   ],
   "source": [
    "# Specifying the optimal hyperparameters\n",
    "log1 = LogisticRegression(penalty='l2', C=0.2)\n",
    "\n",
    "# Fitting the model\n",
    "logt1 = log1.fit(smote_X1, smote_y1)\n",
    "\n",
    "\n",
    "# Checking cross-validation values\n",
    "print('Cross-Validation Scores:', cross_val_score(logt1, smote_X1, smote_y1, cv=5))\n",
    "print('Mean Cross-Validation Score:', np.mean(cross_val_score(logt1, smote_X1, smote_y1, cv=5)))\n",
    "\n",
    "# Constructing the confusion matrix\n",
    "predictions_logt1 = logt1.predict(X1_test)\n",
    "predictions_proba_logt1 = logt1.predict_proba(X1_test)\n",
    "print(confusion_matrix(y1_test, predictions_logt1))\n",
    "print(classification_report(y1_test, predictions_logt1))\n",
    "print('AUC Score', roc_auc_score(y1_test, predictions_proba_logt1[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3600 candidates, totalling 18000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 160 tasks      | elapsed:    1.0s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-680-5a35448fb5f4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mbbc1_gridsearch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbbc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbbc1_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'recall'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mbbc1_gridsearch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msmote_X1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msmote_y1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbbc1_gridsearch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    720\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    724\u001b[0m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1189\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1191\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    709\u001b[0m                                \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m                                in product(candidate_params,\n\u001b[1;32m--> 711\u001b[1;33m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    712\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    713\u001b[0m                 \u001b[0mall_candidate_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    929\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 930\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    931\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    831\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    832\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 833\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    834\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    520\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    425\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 427\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    428\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bbc = BalancedBaggingClassifier(base_estimator=LogisticRegression(penalty='l2', C=0.2))\n",
    "\n",
    "bbc1_params = {'n_estimators':[1,3,5,7,9,11], 'max_samples':[500,700,900,1100,1300],\n",
    "              'max_features':[1,3,5,7,9,11], 'bootstrap':['True', 'False'],\n",
    "              'replacement':['True', 'False'], \n",
    "              'sampling_strategy':['majority', 'not minority', 'all', 'auto', 'not majority']}\n",
    "\n",
    "bbc1_gridsearch = GridSearchCV(bbc, bbc1_params, cv=5, verbose=1, scoring='recall', n_jobs=-1)\n",
    "\n",
    "bbc1_gridsearch.fit(smote_X1, smote_y1)\n",
    "\n",
    "print(bbc1_gridsearch.best_params_)\n",
    "print(bbc1_gridsearch.best_score_)\n",
    "pp.pprint(bbc1_gridsearch.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores: [0.88209199 0.89259865 0.89003035 0.89070528 0.88764307]\n",
      "Mean Cross-Validation Score: 0.8886138683831895\n",
      "[[2804  555]\n",
      " [   3   68]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.83      0.91      3359\n",
      "           1       0.11      0.96      0.20        71\n",
      "\n",
      "   micro avg       0.84      0.84      0.84      3430\n",
      "   macro avg       0.55      0.90      0.55      3430\n",
      "weighted avg       0.98      0.84      0.89      3430\n",
      "\n",
      "AUC Score 0.9614950794376259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "bbc2 = BalancedBaggingClassifier(base_estimator=LogisticRegression(penalty='l2', C=0.2), bootstrap=False,\n",
    "                                 max_features=1, max_samples=500, n_estimators=9, replacement=True,\n",
    "                                 sampling_strategy='majority', random_state=0)\n",
    "\n",
    "# Fitting the model\n",
    "bbc2 = bbc2.fit(smote_X1, smote_y1)\n",
    "\n",
    "\n",
    "# Checking cross-validation values\n",
    "print('Cross-Validation Scores:', cross_val_score(bbc2, smote_X1, smote_y1, cv=5))\n",
    "print('Mean Cross-Validation Score:', np.mean(cross_val_score(bbc2, smote_X1, smote_y1, cv=5)))\n",
    "\n",
    "# Constructing the confusion matrix\n",
    "predictions_bbc2 = bbc2.predict(X1_test)\n",
    "predictions_proba_bbc2 = bbc2.predict_proba(X1_test)\n",
    "print(confusion_matrix(y1_test, predictions_bbc2))\n",
    "print(classification_report(y1_test, predictions_bbc2))\n",
    "print('AUC Score', roc_auc_score(y1_test, predictions_proba_bbc2[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 180 candidates, totalling 900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "[Parallel(n_jobs=1)]: Done 900 out of 900 | elapsed:  4.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_features': 3, 'max_samples': 500, 'n_estimators': 7}\n",
      "0.9245215521303327\n",
      "{'mean_fit_time': array([0.01297593, 0.09909425, 0.06201358, 0.26459398, 0.24459038,\n",
      "       0.06062608, 0.01115108, 0.11558266, 0.23583269, 0.28156009,\n",
      "       0.2401794 , 0.07909021, 0.01077857, 0.132723  , 0.25717335,\n",
      "       0.25640135, 0.24466658, 0.05926943, 0.01136017, 0.11204786,\n",
      "       0.25628514, 0.2632247 , 0.24612594, 0.06260533, 0.01216927,\n",
      "       0.11512589, 0.25472636, 0.26225452, 0.24551535, 0.06281366,\n",
      "       0.01873837, 0.12982554, 0.28662448, 0.30810084, 0.27911448,\n",
      "       0.10131569, 0.01955786, 0.12465582, 0.27829952, 0.284551  ,\n",
      "       0.10723   , 0.09795108, 0.01916256, 0.12565908, 0.27913957,\n",
      "       0.28761411, 0.28418374, 0.09795847, 0.01980352, 0.14258709,\n",
      "       0.0852829 , 0.291675  , 0.28292184, 0.11197886, 0.02244968,\n",
      "       0.12398996, 0.28638954, 0.29269652, 0.28583283, 0.11317201,\n",
      "       0.02430782, 0.13976445, 0.1063293 , 0.12878213, 0.33209662,\n",
      "       0.1760004 , 0.02673659, 0.15194817, 0.31418629, 0.33351564,\n",
      "       0.3402298 , 0.17643213, 0.02573404, 0.14542718, 0.10422587,\n",
      "       0.33374281, 0.33992682, 0.17509212, 0.02473702, 0.13878884,\n",
      "       0.12107878, 0.33565369, 0.33292556, 0.17178922, 0.03014622,\n",
      "       0.15966163, 0.31231937, 0.33616738, 0.34641752, 0.23495469,\n",
      "       0.04028764, 0.17186317, 0.15084081, 0.18827581, 0.21221733,\n",
      "       0.23710628, 0.03630686, 0.16941838, 0.14582729, 0.1897933 ,\n",
      "       0.22466855, 0.25048132, 0.04278183, 0.19961667, 0.33211904,\n",
      "       0.18628612, 0.23481393, 0.25097489, 0.03989305, 0.17752309,\n",
      "       0.15816875, 0.20268307, 0.22684145, 0.26212764, 0.0347187 ,\n",
      "       0.1677053 , 0.155758  , 0.19552393, 0.23752022, 0.25831385,\n",
      "       0.05983415, 0.19372907, 0.37358479, 0.43385687, 0.46572204,\n",
      "       0.32922697, 0.05304866, 0.1914547 , 0.38977871, 0.4460844 ,\n",
      "       0.29917359, 0.33528776, 0.06184502, 0.19171033, 0.38913879,\n",
      "       0.44654222, 0.48094387, 0.32968254, 0.06601782, 0.19728165,\n",
      "       0.37907639, 0.44832134, 0.49590349, 0.37710938, 0.07101254,\n",
      "       0.19000015, 0.40418258, 0.50597081, 0.34504523, 0.36082888,\n",
      "       0.07639832, 0.21276908, 0.42952886, 0.54072762, 0.53718162,\n",
      "       0.42802172, 0.07860212, 0.21814332, 0.44875937, 0.51158161,\n",
      "       0.5545661 , 0.4363338 , 0.08179679, 0.22832799, 0.45415435,\n",
      "       0.52120762, 0.61280332, 0.45549846, 0.08617115, 0.23262653,\n",
      "       0.45369053, 0.52940345, 0.56502161, 0.46031351, 0.08357782,\n",
      "       0.23291783, 0.46340818, 0.34161367, 0.58288584, 0.48502979]),\n",
      " 'mean_score_time': array([0.00199232, 0.00740066, 0.21814985, 0.0165566 , 0.0175602 ,\n",
      "       0.01647921, 0.00139666, 0.0074573 , 0.19637523, 0.01935558,\n",
      "       0.0177711 , 0.02081065, 0.00178127, 0.00821157, 0.19610176,\n",
      "       0.01633844, 0.01795135, 0.01577234, 0.00158963, 0.00738182,\n",
      "       0.01296573, 0.01657271, 0.01716223, 0.01604476, 0.00179029,\n",
      "       0.00738621, 0.01257429, 0.01658254, 0.0177474 , 0.01677175,\n",
      "       0.00224009, 0.00778637, 0.0127666 , 0.01821518, 0.01797171,\n",
      "       0.01606402, 0.00232587, 0.00811534, 0.01296635, 0.01675534,\n",
      "       0.01697454, 0.01656809, 0.0023839 , 0.00775743, 0.01250248,\n",
      "       0.01736903, 0.01716323, 0.01592274, 0.00249472, 0.01064124,\n",
      "       0.21428285, 0.01612139, 0.01697946, 0.01695638, 0.00199876,\n",
      "       0.00777783, 0.01278768, 0.01615739, 0.01795888, 0.01615758,\n",
      "       0.0025815 , 0.00760236, 0.19726086, 0.21432562, 0.01716619,\n",
      "       0.01608372, 0.00251546, 0.00823827, 0.01217961, 0.01476412,\n",
      "       0.01709595, 0.01655884, 0.0023159 , 0.00756822, 0.01249237,\n",
      "       0.0189219 , 0.01754379, 0.01624169, 0.00234413, 0.00763564,\n",
      "       0.21856155, 0.01558318, 0.016852  , 0.01596947, 0.00233054,\n",
      "       0.01077361, 0.19680357, 0.01616926, 0.01775432, 0.01698647,\n",
      "       0.00280123, 0.00791078, 0.21457849, 0.21524677, 0.20168395,\n",
      "       0.01597505, 0.00239625, 0.00795822, 0.01179495, 0.21619945,\n",
      "       0.20464573, 0.01757798, 0.00216727, 0.00817122, 0.01276608,\n",
      "       0.21433511, 0.20107532, 0.01656265, 0.00223513, 0.00844917,\n",
      "       0.21521006, 0.21565738, 0.19958863, 0.01655436, 0.00247774,\n",
      "       0.00788231, 0.21421533, 0.21741705, 0.20165277, 0.01616473,\n",
      "       0.00204983, 0.00833659, 0.19657335, 0.01559162, 0.01652308,\n",
      "       0.01633587, 0.00217576, 0.00789475, 0.01197524, 0.01537519,\n",
      "       0.1980381 , 0.01604018, 0.00239701, 0.00810485, 0.01238146,\n",
      "       0.01455107, 0.01738448, 0.01616273, 0.00225863, 0.00815349,\n",
      "       0.19624357, 0.01466603, 0.01795192, 0.01800265, 0.00220933,\n",
      "       0.00807328, 0.01217403, 0.01835356, 0.01662087, 0.01616731,\n",
      "       0.00243797, 0.00781164, 0.01278768, 0.01854959, 0.01759181,\n",
      "       0.01778584, 0.00229874, 0.00763087, 0.01255927, 0.01638222,\n",
      "       0.01716042, 0.017765  , 0.00256305, 0.00776682, 0.01317558,\n",
      "       0.01782441, 0.01897173, 0.01817565, 0.00240355, 0.00818005,\n",
      "       0.01338196, 0.01815057, 0.01852036, 0.0185452 , 0.00221667,\n",
      "       0.00823865, 0.01317701, 0.01676078, 0.01820683, 0.01829896]),\n",
      " 'mean_test_score': array([0.60981761, 0.80494991, 0.86922445, 0.83010995, 0.83646761,\n",
      "       0.87520214, 0.51137043, 0.68754584, 0.77606049, 0.83410992,\n",
      "       0.85452755, 0.85153788, 0.45283708, 0.81306043, 0.85216285,\n",
      "       0.85091696, 0.85564834, 0.87781672, 0.4954194 , 0.82974253,\n",
      "       0.7346993 , 0.75052693, 0.87445619, 0.88803028, 0.43742248,\n",
      "       0.71890435, 0.87271059, 0.87645074, 0.86037703, 0.87034505,\n",
      "       0.87832045, 0.89936361, 0.89998648, 0.92452155, 0.90895461,\n",
      "       0.9138119 , 0.87420795, 0.89500484, 0.91206873, 0.91468385,\n",
      "       0.91144551, 0.9221562 , 0.84406459, 0.90496928, 0.90721248,\n",
      "       0.90447121, 0.91057382, 0.90982696, 0.87208713, 0.89351065,\n",
      "       0.90571619, 0.91680097, 0.90932877, 0.90708596, 0.83285543,\n",
      "       0.88529292, 0.90895578, 0.91094746, 0.91144533, 0.90185576,\n",
      "       0.8845453 , 0.91169288, 0.90285162, 0.90596546, 0.89687508,\n",
      "       0.90322723, 0.88653445, 0.90907966, 0.91655187, 0.91468432,\n",
      "       0.91020032, 0.90845663, 0.90297577, 0.91144565, 0.90820732,\n",
      "       0.90546887, 0.91044976, 0.91107346, 0.89351082, 0.91206927,\n",
      "       0.90546863, 0.90596708, 0.91393701, 0.90721226, 0.88391988,\n",
      "       0.91381178, 0.91991454, 0.90882972, 0.91518198, 0.90708684,\n",
      "       0.89762134, 0.90858114, 0.90335107, 0.90658861, 0.91406114,\n",
      "       0.90845655, 0.88803211, 0.90795739, 0.91132177, 0.90783336,\n",
      "       0.909577  , 0.90721154, 0.90783604, 0.90932778, 0.90932806,\n",
      "       0.90883005, 0.91256686, 0.91343871, 0.91667611, 0.9129403 ,\n",
      "       0.91244233, 0.91231759, 0.91007626, 0.91530654, 0.90334873,\n",
      "       0.91543084, 0.91493288, 0.91493348, 0.91368821, 0.91194347,\n",
      "       0.90036017, 0.90397231, 0.90609068, 0.91044991, 0.90883039,\n",
      "       0.90559293, 0.91555494, 0.91505782, 0.91107212, 0.91244203,\n",
      "       0.90945333, 0.90858096, 0.90260257, 0.90310109, 0.91107247,\n",
      "       0.91244186, 0.91505779, 0.91431047, 0.91431057, 0.91231773,\n",
      "       0.91356315, 0.91356337, 0.91555574, 0.91543125, 0.91069879,\n",
      "       0.9144351 , 0.91667696, 0.9139371 , 0.91443519, 0.9143106 ,\n",
      "       0.89101972, 0.91605461, 0.91343904, 0.91219352, 0.90907989,\n",
      "       0.90895553, 0.91381348, 0.90496931, 0.9094532 , 0.91007575,\n",
      "       0.9130652 , 0.91281613, 0.9019794 , 0.91692565, 0.91418614,\n",
      "       0.90858118, 0.91281578, 0.91617859, 0.91318893, 0.91069875,\n",
      "       0.91306458, 0.9155558 , 0.91480872, 0.91431058, 0.91331407,\n",
      "       0.91343825, 0.91368767, 0.91617832, 0.91717515, 0.91393678]),\n",
      " 'mean_train_score': array([0.61189941, 0.8064533 , 0.87137167, 0.81940892, 0.8382429 ,\n",
      "       0.8778181 , 0.50851086, 0.6883115 , 0.77390996, 0.83397525,\n",
      "       0.85626936, 0.85714225, 0.44806639, 0.80912725, 0.85458951,\n",
      "       0.85222086, 0.86044244, 0.87380121, 0.49434844, 0.82803149,\n",
      "       0.74194586, 0.75055747, 0.87417433, 0.88410756, 0.43309065,\n",
      "       0.717802  , 0.87479737, 0.8780036 , 0.85745672, 0.87053118,\n",
      "       0.87669589, 0.89665651, 0.90004997, 0.92141077, 0.90926596,\n",
      "       0.91166412, 0.87473429, 0.89693605, 0.90951546, 0.91350142,\n",
      "       0.91038768, 0.92178411, 0.84699334, 0.90615392, 0.91076039,\n",
      "       0.90229189, 0.91178858, 0.91181967, 0.86832089, 0.89674978,\n",
      "       0.90609023, 0.91664593, 0.90873747, 0.90808299, 0.83615355,\n",
      "       0.88856065, 0.90739738, 0.91269114, 0.90914232, 0.90048594,\n",
      "       0.88550848, 0.90549977, 0.90506291, 0.90562376, 0.89986238,\n",
      "       0.90596555, 0.88874751, 0.90823922, 0.91617941, 0.9134385 ,\n",
      "       0.91116581, 0.90590346, 0.90198057, 0.91035674, 0.9062465 ,\n",
      "       0.90683727, 0.91113458, 0.9135943 , 0.8916743 , 0.91013857,\n",
      "       0.90957759, 0.90668165, 0.91396793, 0.90988907, 0.8867852 ,\n",
      "       0.91253523, 0.92128553, 0.90917312, 0.91543175, 0.90795852,\n",
      "       0.89674915, 0.9102008 , 0.90770897, 0.90830123, 0.91555615,\n",
      "       0.90951569, 0.8892444 , 0.90898638, 0.91396776, 0.90960905,\n",
      "       0.91203771, 0.90774039, 0.90552952, 0.90777216, 0.90795871,\n",
      "       0.90702463, 0.91272233, 0.91374976, 0.9132831 , 0.91119715,\n",
      "       0.9144663 , 0.91365693, 0.9098578 , 0.9154627 , 0.90350617,\n",
      "       0.91403033, 0.91088536, 0.91512016, 0.9156494 , 0.9126294 ,\n",
      "       0.89992476, 0.90665085, 0.90571669, 0.90954617, 0.91016941,\n",
      "       0.90668173, 0.91804742, 0.91359437, 0.91063656, 0.91219331,\n",
      "       0.9097023 , 0.91007599, 0.90129576, 0.90590362, 0.91141477,\n",
      "       0.91505803, 0.91521356, 0.91496445, 0.91362533, 0.91269167,\n",
      "       0.91069869, 0.91536918, 0.91580512, 0.9144352 , 0.91309604,\n",
      "       0.9151514 , 0.91580506, 0.91424841, 0.91564935, 0.91611647,\n",
      "       0.89154941, 0.91421767, 0.91119637, 0.91253563, 0.90917302,\n",
      "       0.90858128, 0.91166389, 0.90537409, 0.91007579, 0.90923522,\n",
      "       0.9133765 , 0.91297148, 0.90431598, 0.91733093, 0.91483976,\n",
      "       0.90851905, 0.91331417, 0.91571181, 0.91543199, 0.91294045,\n",
      "       0.91368799, 0.91592992, 0.91574282, 0.91471539, 0.91144625,\n",
      "       0.91281623, 0.91508904, 0.91736208, 0.91714413, 0.91487107]),\n",
      " 'param_max_features': masked_array(data=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                   1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3,\n",
      "                   3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "                   3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "                   5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "                   7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "                   7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 9, 9, 9, 9, 9, 9,\n",
      "                   9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "                   9, 9, 9, 9, 9, 9, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
      "                   11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
      "                   11, 11, 11, 11, 11, 11, 11],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object),\n",
      " 'param_max_samples': masked_array(data=[500, 500, 500, 500, 500, 500, 700, 700, 700, 700, 700,\n",
      "                   700, 900, 900, 900, 900, 900, 900, 1100, 1100, 1100,\n",
      "                   1100, 1100, 1100, 1300, 1300, 1300, 1300, 1300, 1300,\n",
      "                   500, 500, 500, 500, 500, 500, 700, 700, 700, 700, 700,\n",
      "                   700, 900, 900, 900, 900, 900, 900, 1100, 1100, 1100,\n",
      "                   1100, 1100, 1100, 1300, 1300, 1300, 1300, 1300, 1300,\n",
      "                   500, 500, 500, 500, 500, 500, 700, 700, 700, 700, 700,\n",
      "                   700, 900, 900, 900, 900, 900, 900, 1100, 1100, 1100,\n",
      "                   1100, 1100, 1100, 1300, 1300, 1300, 1300, 1300, 1300,\n",
      "                   500, 500, 500, 500, 500, 500, 700, 700, 700, 700, 700,\n",
      "                   700, 900, 900, 900, 900, 900, 900, 1100, 1100, 1100,\n",
      "                   1100, 1100, 1100, 1300, 1300, 1300, 1300, 1300, 1300,\n",
      "                   500, 500, 500, 500, 500, 500, 700, 700, 700, 700, 700,\n",
      "                   700, 900, 900, 900, 900, 900, 900, 1100, 1100, 1100,\n",
      "                   1100, 1100, 1100, 1300, 1300, 1300, 1300, 1300, 1300,\n",
      "                   500, 500, 500, 500, 500, 500, 700, 700, 700, 700, 700,\n",
      "                   700, 900, 900, 900, 900, 900, 900, 1100, 1100, 1100,\n",
      "                   1100, 1100, 1100, 1300, 1300, 1300, 1300, 1300, 1300],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object),\n",
      " 'param_n_estimators': masked_array(data=[1, 3, 5, 7, 9, 11, 1, 3, 5, 7, 9, 11, 1, 3, 5, 7, 9,\n",
      "                   11, 1, 3, 5, 7, 9, 11, 1, 3, 5, 7, 9, 11, 1, 3, 5, 7,\n",
      "                   9, 11, 1, 3, 5, 7, 9, 11, 1, 3, 5, 7, 9, 11, 1, 3, 5,\n",
      "                   7, 9, 11, 1, 3, 5, 7, 9, 11, 1, 3, 5, 7, 9, 11, 1, 3,\n",
      "                   5, 7, 9, 11, 1, 3, 5, 7, 9, 11, 1, 3, 5, 7, 9, 11, 1,\n",
      "                   3, 5, 7, 9, 11, 1, 3, 5, 7, 9, 11, 1, 3, 5, 7, 9, 11,\n",
      "                   1, 3, 5, 7, 9, 11, 1, 3, 5, 7, 9, 11, 1, 3, 5, 7, 9,\n",
      "                   11, 1, 3, 5, 7, 9, 11, 1, 3, 5, 7, 9, 11, 1, 3, 5, 7,\n",
      "                   9, 11, 1, 3, 5, 7, 9, 11, 1, 3, 5, 7, 9, 11, 1, 3, 5,\n",
      "                   7, 9, 11, 1, 3, 5, 7, 9, 11, 1, 3, 5, 7, 9, 11, 1, 3,\n",
      "                   5, 7, 9, 11, 1, 3, 5, 7, 9, 11],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object),\n",
      " 'params': [{'max_features': 1, 'max_samples': 500, 'n_estimators': 1},\n",
      "            {'max_features': 1, 'max_samples': 500, 'n_estimators': 3},\n",
      "            {'max_features': 1, 'max_samples': 500, 'n_estimators': 5},\n",
      "            {'max_features': 1, 'max_samples': 500, 'n_estimators': 7},\n",
      "            {'max_features': 1, 'max_samples': 500, 'n_estimators': 9},\n",
      "            {'max_features': 1, 'max_samples': 500, 'n_estimators': 11},\n",
      "            {'max_features': 1, 'max_samples': 700, 'n_estimators': 1},\n",
      "            {'max_features': 1, 'max_samples': 700, 'n_estimators': 3},\n",
      "            {'max_features': 1, 'max_samples': 700, 'n_estimators': 5},\n",
      "            {'max_features': 1, 'max_samples': 700, 'n_estimators': 7},\n",
      "            {'max_features': 1, 'max_samples': 700, 'n_estimators': 9},\n",
      "            {'max_features': 1, 'max_samples': 700, 'n_estimators': 11},\n",
      "            {'max_features': 1, 'max_samples': 900, 'n_estimators': 1},\n",
      "            {'max_features': 1, 'max_samples': 900, 'n_estimators': 3},\n",
      "            {'max_features': 1, 'max_samples': 900, 'n_estimators': 5},\n",
      "            {'max_features': 1, 'max_samples': 900, 'n_estimators': 7},\n",
      "            {'max_features': 1, 'max_samples': 900, 'n_estimators': 9},\n",
      "            {'max_features': 1, 'max_samples': 900, 'n_estimators': 11},\n",
      "            {'max_features': 1, 'max_samples': 1100, 'n_estimators': 1},\n",
      "            {'max_features': 1, 'max_samples': 1100, 'n_estimators': 3},\n",
      "            {'max_features': 1, 'max_samples': 1100, 'n_estimators': 5},\n",
      "            {'max_features': 1, 'max_samples': 1100, 'n_estimators': 7},\n",
      "            {'max_features': 1, 'max_samples': 1100, 'n_estimators': 9},\n",
      "            {'max_features': 1, 'max_samples': 1100, 'n_estimators': 11},\n",
      "            {'max_features': 1, 'max_samples': 1300, 'n_estimators': 1},\n",
      "            {'max_features': 1, 'max_samples': 1300, 'n_estimators': 3},\n",
      "            {'max_features': 1, 'max_samples': 1300, 'n_estimators': 5},\n",
      "            {'max_features': 1, 'max_samples': 1300, 'n_estimators': 7},\n",
      "            {'max_features': 1, 'max_samples': 1300, 'n_estimators': 9},\n",
      "            {'max_features': 1, 'max_samples': 1300, 'n_estimators': 11},\n",
      "            {'max_features': 3, 'max_samples': 500, 'n_estimators': 1},\n",
      "            {'max_features': 3, 'max_samples': 500, 'n_estimators': 3},\n",
      "            {'max_features': 3, 'max_samples': 500, 'n_estimators': 5},\n",
      "            {'max_features': 3, 'max_samples': 500, 'n_estimators': 7},\n",
      "            {'max_features': 3, 'max_samples': 500, 'n_estimators': 9},\n",
      "            {'max_features': 3, 'max_samples': 500, 'n_estimators': 11},\n",
      "            {'max_features': 3, 'max_samples': 700, 'n_estimators': 1},\n",
      "            {'max_features': 3, 'max_samples': 700, 'n_estimators': 3},\n",
      "            {'max_features': 3, 'max_samples': 700, 'n_estimators': 5},\n",
      "            {'max_features': 3, 'max_samples': 700, 'n_estimators': 7},\n",
      "            {'max_features': 3, 'max_samples': 700, 'n_estimators': 9},\n",
      "            {'max_features': 3, 'max_samples': 700, 'n_estimators': 11},\n",
      "            {'max_features': 3, 'max_samples': 900, 'n_estimators': 1},\n",
      "            {'max_features': 3, 'max_samples': 900, 'n_estimators': 3},\n",
      "            {'max_features': 3, 'max_samples': 900, 'n_estimators': 5},\n",
      "            {'max_features': 3, 'max_samples': 900, 'n_estimators': 7},\n",
      "            {'max_features': 3, 'max_samples': 900, 'n_estimators': 9},\n",
      "            {'max_features': 3, 'max_samples': 900, 'n_estimators': 11},\n",
      "            {'max_features': 3, 'max_samples': 1100, 'n_estimators': 1},\n",
      "            {'max_features': 3, 'max_samples': 1100, 'n_estimators': 3},\n",
      "            {'max_features': 3, 'max_samples': 1100, 'n_estimators': 5},\n",
      "            {'max_features': 3, 'max_samples': 1100, 'n_estimators': 7},\n",
      "            {'max_features': 3, 'max_samples': 1100, 'n_estimators': 9},\n",
      "            {'max_features': 3, 'max_samples': 1100, 'n_estimators': 11},\n",
      "            {'max_features': 3, 'max_samples': 1300, 'n_estimators': 1},\n",
      "            {'max_features': 3, 'max_samples': 1300, 'n_estimators': 3},\n",
      "            {'max_features': 3, 'max_samples': 1300, 'n_estimators': 5},\n",
      "            {'max_features': 3, 'max_samples': 1300, 'n_estimators': 7},\n",
      "            {'max_features': 3, 'max_samples': 1300, 'n_estimators': 9},\n",
      "            {'max_features': 3, 'max_samples': 1300, 'n_estimators': 11},\n",
      "            {'max_features': 5, 'max_samples': 500, 'n_estimators': 1},\n",
      "            {'max_features': 5, 'max_samples': 500, 'n_estimators': 3},\n",
      "            {'max_features': 5, 'max_samples': 500, 'n_estimators': 5},\n",
      "            {'max_features': 5, 'max_samples': 500, 'n_estimators': 7},\n",
      "            {'max_features': 5, 'max_samples': 500, 'n_estimators': 9},\n",
      "            {'max_features': 5, 'max_samples': 500, 'n_estimators': 11},\n",
      "            {'max_features': 5, 'max_samples': 700, 'n_estimators': 1},\n",
      "            {'max_features': 5, 'max_samples': 700, 'n_estimators': 3},\n",
      "            {'max_features': 5, 'max_samples': 700, 'n_estimators': 5},\n",
      "            {'max_features': 5, 'max_samples': 700, 'n_estimators': 7},\n",
      "            {'max_features': 5, 'max_samples': 700, 'n_estimators': 9},\n",
      "            {'max_features': 5, 'max_samples': 700, 'n_estimators': 11},\n",
      "            {'max_features': 5, 'max_samples': 900, 'n_estimators': 1},\n",
      "            {'max_features': 5, 'max_samples': 900, 'n_estimators': 3},\n",
      "            {'max_features': 5, 'max_samples': 900, 'n_estimators': 5},\n",
      "            {'max_features': 5, 'max_samples': 900, 'n_estimators': 7},\n",
      "            {'max_features': 5, 'max_samples': 900, 'n_estimators': 9},\n",
      "            {'max_features': 5, 'max_samples': 900, 'n_estimators': 11},\n",
      "            {'max_features': 5, 'max_samples': 1100, 'n_estimators': 1},\n",
      "            {'max_features': 5, 'max_samples': 1100, 'n_estimators': 3},\n",
      "            {'max_features': 5, 'max_samples': 1100, 'n_estimators': 5},\n",
      "            {'max_features': 5, 'max_samples': 1100, 'n_estimators': 7},\n",
      "            {'max_features': 5, 'max_samples': 1100, 'n_estimators': 9},\n",
      "            {'max_features': 5, 'max_samples': 1100, 'n_estimators': 11},\n",
      "            {'max_features': 5, 'max_samples': 1300, 'n_estimators': 1},\n",
      "            {'max_features': 5, 'max_samples': 1300, 'n_estimators': 3},\n",
      "            {'max_features': 5, 'max_samples': 1300, 'n_estimators': 5},\n",
      "            {'max_features': 5, 'max_samples': 1300, 'n_estimators': 7},\n",
      "            {'max_features': 5, 'max_samples': 1300, 'n_estimators': 9},\n",
      "            {'max_features': 5, 'max_samples': 1300, 'n_estimators': 11},\n",
      "            {'max_features': 7, 'max_samples': 500, 'n_estimators': 1},\n",
      "            {'max_features': 7, 'max_samples': 500, 'n_estimators': 3},\n",
      "            {'max_features': 7, 'max_samples': 500, 'n_estimators': 5},\n",
      "            {'max_features': 7, 'max_samples': 500, 'n_estimators': 7},\n",
      "            {'max_features': 7, 'max_samples': 500, 'n_estimators': 9},\n",
      "            {'max_features': 7, 'max_samples': 500, 'n_estimators': 11},\n",
      "            {'max_features': 7, 'max_samples': 700, 'n_estimators': 1},\n",
      "            {'max_features': 7, 'max_samples': 700, 'n_estimators': 3},\n",
      "            {'max_features': 7, 'max_samples': 700, 'n_estimators': 5},\n",
      "            {'max_features': 7, 'max_samples': 700, 'n_estimators': 7},\n",
      "            {'max_features': 7, 'max_samples': 700, 'n_estimators': 9},\n",
      "            {'max_features': 7, 'max_samples': 700, 'n_estimators': 11},\n",
      "            {'max_features': 7, 'max_samples': 900, 'n_estimators': 1},\n",
      "            {'max_features': 7, 'max_samples': 900, 'n_estimators': 3},\n",
      "            {'max_features': 7, 'max_samples': 900, 'n_estimators': 5},\n",
      "            {'max_features': 7, 'max_samples': 900, 'n_estimators': 7},\n",
      "            {'max_features': 7, 'max_samples': 900, 'n_estimators': 9},\n",
      "            {'max_features': 7, 'max_samples': 900, 'n_estimators': 11},\n",
      "            {'max_features': 7, 'max_samples': 1100, 'n_estimators': 1},\n",
      "            {'max_features': 7, 'max_samples': 1100, 'n_estimators': 3},\n",
      "            {'max_features': 7, 'max_samples': 1100, 'n_estimators': 5},\n",
      "            {'max_features': 7, 'max_samples': 1100, 'n_estimators': 7},\n",
      "            {'max_features': 7, 'max_samples': 1100, 'n_estimators': 9},\n",
      "            {'max_features': 7, 'max_samples': 1100, 'n_estimators': 11},\n",
      "            {'max_features': 7, 'max_samples': 1300, 'n_estimators': 1},\n",
      "            {'max_features': 7, 'max_samples': 1300, 'n_estimators': 3},\n",
      "            {'max_features': 7, 'max_samples': 1300, 'n_estimators': 5},\n",
      "            {'max_features': 7, 'max_samples': 1300, 'n_estimators': 7},\n",
      "            {'max_features': 7, 'max_samples': 1300, 'n_estimators': 9},\n",
      "            {'max_features': 7, 'max_samples': 1300, 'n_estimators': 11},\n",
      "            {'max_features': 9, 'max_samples': 500, 'n_estimators': 1},\n",
      "            {'max_features': 9, 'max_samples': 500, 'n_estimators': 3},\n",
      "            {'max_features': 9, 'max_samples': 500, 'n_estimators': 5},\n",
      "            {'max_features': 9, 'max_samples': 500, 'n_estimators': 7},\n",
      "            {'max_features': 9, 'max_samples': 500, 'n_estimators': 9},\n",
      "            {'max_features': 9, 'max_samples': 500, 'n_estimators': 11},\n",
      "            {'max_features': 9, 'max_samples': 700, 'n_estimators': 1},\n",
      "            {'max_features': 9, 'max_samples': 700, 'n_estimators': 3},\n",
      "            {'max_features': 9, 'max_samples': 700, 'n_estimators': 5},\n",
      "            {'max_features': 9, 'max_samples': 700, 'n_estimators': 7},\n",
      "            {'max_features': 9, 'max_samples': 700, 'n_estimators': 9},\n",
      "            {'max_features': 9, 'max_samples': 700, 'n_estimators': 11},\n",
      "            {'max_features': 9, 'max_samples': 900, 'n_estimators': 1},\n",
      "            {'max_features': 9, 'max_samples': 900, 'n_estimators': 3},\n",
      "            {'max_features': 9, 'max_samples': 900, 'n_estimators': 5},\n",
      "            {'max_features': 9, 'max_samples': 900, 'n_estimators': 7},\n",
      "            {'max_features': 9, 'max_samples': 900, 'n_estimators': 9},\n",
      "            {'max_features': 9, 'max_samples': 900, 'n_estimators': 11},\n",
      "            {'max_features': 9, 'max_samples': 1100, 'n_estimators': 1},\n",
      "            {'max_features': 9, 'max_samples': 1100, 'n_estimators': 3},\n",
      "            {'max_features': 9, 'max_samples': 1100, 'n_estimators': 5},\n",
      "            {'max_features': 9, 'max_samples': 1100, 'n_estimators': 7},\n",
      "            {'max_features': 9, 'max_samples': 1100, 'n_estimators': 9},\n",
      "            {'max_features': 9, 'max_samples': 1100, 'n_estimators': 11},\n",
      "            {'max_features': 9, 'max_samples': 1300, 'n_estimators': 1},\n",
      "            {'max_features': 9, 'max_samples': 1300, 'n_estimators': 3},\n",
      "            {'max_features': 9, 'max_samples': 1300, 'n_estimators': 5},\n",
      "            {'max_features': 9, 'max_samples': 1300, 'n_estimators': 7},\n",
      "            {'max_features': 9, 'max_samples': 1300, 'n_estimators': 9},\n",
      "            {'max_features': 9, 'max_samples': 1300, 'n_estimators': 11},\n",
      "            {'max_features': 11, 'max_samples': 500, 'n_estimators': 1},\n",
      "            {'max_features': 11, 'max_samples': 500, 'n_estimators': 3},\n",
      "            {'max_features': 11, 'max_samples': 500, 'n_estimators': 5},\n",
      "            {'max_features': 11, 'max_samples': 500, 'n_estimators': 7},\n",
      "            {'max_features': 11, 'max_samples': 500, 'n_estimators': 9},\n",
      "            {'max_features': 11, 'max_samples': 500, 'n_estimators': 11},\n",
      "            {'max_features': 11, 'max_samples': 700, 'n_estimators': 1},\n",
      "            {'max_features': 11, 'max_samples': 700, 'n_estimators': 3},\n",
      "            {'max_features': 11, 'max_samples': 700, 'n_estimators': 5},\n",
      "            {'max_features': 11, 'max_samples': 700, 'n_estimators': 7},\n",
      "            {'max_features': 11, 'max_samples': 700, 'n_estimators': 9},\n",
      "            {'max_features': 11, 'max_samples': 700, 'n_estimators': 11},\n",
      "            {'max_features': 11, 'max_samples': 900, 'n_estimators': 1},\n",
      "            {'max_features': 11, 'max_samples': 900, 'n_estimators': 3},\n",
      "            {'max_features': 11, 'max_samples': 900, 'n_estimators': 5},\n",
      "            {'max_features': 11, 'max_samples': 900, 'n_estimators': 7},\n",
      "            {'max_features': 11, 'max_samples': 900, 'n_estimators': 9},\n",
      "            {'max_features': 11, 'max_samples': 900, 'n_estimators': 11},\n",
      "            {'max_features': 11, 'max_samples': 1100, 'n_estimators': 1},\n",
      "            {'max_features': 11, 'max_samples': 1100, 'n_estimators': 3},\n",
      "            {'max_features': 11, 'max_samples': 1100, 'n_estimators': 5},\n",
      "            {'max_features': 11, 'max_samples': 1100, 'n_estimators': 7},\n",
      "            {'max_features': 11, 'max_samples': 1100, 'n_estimators': 9},\n",
      "            {'max_features': 11, 'max_samples': 1100, 'n_estimators': 11},\n",
      "            {'max_features': 11, 'max_samples': 1300, 'n_estimators': 1},\n",
      "            {'max_features': 11, 'max_samples': 1300, 'n_estimators': 3},\n",
      "            {'max_features': 11, 'max_samples': 1300, 'n_estimators': 5},\n",
      "            {'max_features': 11, 'max_samples': 1300, 'n_estimators': 7},\n",
      "            {'max_features': 11, 'max_samples': 1300, 'n_estimators': 9},\n",
      "            {'max_features': 11, 'max_samples': 1300, 'n_estimators': 11}],\n",
      " 'rank_test_score': array([176, 170, 156, 167, 164, 150, 177, 175, 171, 165, 159, 161, 179,\n",
      "       169, 160, 162, 158, 148, 178, 168, 173, 172, 151, 142, 180, 174,\n",
      "       153, 149, 157, 155, 147, 134, 133,   1,  93,  39, 152, 137,  63,\n",
      "        26,  67,   2, 163, 120, 106, 121,  76,  82, 154, 139, 115,   6,\n",
      "        86, 110, 166, 144,  91,  73,  68, 131, 145,  65, 128, 114, 136,\n",
      "       125, 143,  90,   9,  25,  79, 100, 127,  66, 102, 117,  78,  70,\n",
      "       138,  62, 118, 113,  36, 107, 146,  40,   3,  96,  19, 109, 135,\n",
      "        98, 123, 111,  34, 101, 141, 103,  69, 105,  83, 108, 104,  88,\n",
      "        87,  95,  55,  46,   8,  52,  56,  60,  80,  18, 124,  17,  23,\n",
      "        22,  41,  64, 132, 122, 112,  77,  94, 116,  15,  20,  72,  57,\n",
      "        84,  99, 129, 126,  71,  58,  21,  32,  31,  59,  44,  43,  14,\n",
      "        16,  74,  28,   7,  35,  27,  29, 140,  12,  45,  61,  89,  92,\n",
      "        38, 119,  85,  81,  50,  53, 130,   5,  33,  97,  54,  10,  49,\n",
      "        75,  51,  13,  24,  30,  48,  47,  42,  11,   4,  37]),\n",
      " 'split0_test_score': array([0.28704857, 0.78518057, 0.88729763, 0.91344956, 0.78518057,\n",
      "       0.88418431, 0.56475716, 0.87297634, 0.54234122, 0.81070984,\n",
      "       0.90535492, 0.84931507, 0.50498132, 0.89788294, 0.87795766,\n",
      "       0.85927771, 0.78206725, 0.88107098, 0.91033624, 0.83997509,\n",
      "       0.86674969, 0.82503113, 0.90971357, 0.88044832, 0.85118306,\n",
      "       0.86737235, 0.8362391 , 0.84308842, 0.90348692, 0.84184309,\n",
      "       0.85242839, 0.89103362, 0.90846824, 0.9358655 , 0.89663761,\n",
      "       0.89601494, 0.90971357, 0.90348692, 0.9252802 , 0.91407223,\n",
      "       0.90161893, 0.93648817, 0.83374844, 0.93648817, 0.92029888,\n",
      "       0.91531756, 0.92278954, 0.90784558, 0.84993773, 0.91407223,\n",
      "       0.91158157, 0.92714819, 0.92403487, 0.90909091, 0.91220423,\n",
      "       0.93088418, 0.90099626, 0.91220423, 0.9234122 , 0.91843088,\n",
      "       0.87110834, 0.9252802 , 0.92278954, 0.93026152, 0.89975093,\n",
      "       0.91407223, 0.91469489, 0.89227895, 0.93399751, 0.89663761,\n",
      "       0.91469489, 0.90473225, 0.87671233, 0.92590286, 0.90909091,\n",
      "       0.9109589 , 0.91843088, 0.91905355, 0.9128269 , 0.91905355,\n",
      "       0.91531756, 0.91843088, 0.9234122 , 0.91407223, 0.87795766,\n",
      "       0.91033624, 0.93399751, 0.92714819, 0.91843088, 0.91531756,\n",
      "       0.89290162, 0.90909091, 0.89227895, 0.91843088, 0.9252802 ,\n",
      "       0.89975093, 0.88231631, 0.91220423, 0.91531756, 0.90722291,\n",
      "       0.90846824, 0.90597758, 0.8860523 , 0.92154421, 0.9252802 ,\n",
      "       0.90722291, 0.90099626, 0.91967621, 0.91843088, 0.91843088,\n",
      "       0.92092154, 0.92714819, 0.91905355, 0.91469489, 0.92278954,\n",
      "       0.9128269 , 0.91594022, 0.9234122 , 0.91967621, 0.90286426,\n",
      "       0.91780822, 0.89290162, 0.90535492, 0.90722291, 0.92216687,\n",
      "       0.91158157, 0.91220423, 0.91718555, 0.91780822, 0.91344956,\n",
      "       0.91843088, 0.90971357, 0.91407223, 0.9109589 , 0.92216687,\n",
      "       0.91594022, 0.91967621, 0.92029888, 0.92590286, 0.92278954,\n",
      "       0.91594022, 0.92154421, 0.91905355, 0.92029888, 0.91344956,\n",
      "       0.91531756, 0.9234122 , 0.91843088, 0.92029888, 0.92092154,\n",
      "       0.86674969, 0.92963885, 0.91407223, 0.91843088, 0.91531756,\n",
      "       0.91531756, 0.92278954, 0.91220423, 0.92278954, 0.92029888,\n",
      "       0.93088418, 0.92029888, 0.89290162, 0.91531756, 0.91843088,\n",
      "       0.92029888, 0.91843088, 0.9128269 , 0.91531756, 0.91220423,\n",
      "       0.9234122 , 0.92714819, 0.91718555, 0.91531756, 0.91407223,\n",
      "       0.92029888, 0.91344956, 0.92590286, 0.91967621, 0.92029888]),\n",
      " 'split0_train_score': array([0.29581193, 0.79822513, 0.88042971, 0.90144792, 0.80320722,\n",
      "       0.88883699, 0.58150397, 0.88245368, 0.55114433, 0.82344699,\n",
      "       0.90954383, 0.87108828, 0.50910789, 0.88961544, 0.88525611,\n",
      "       0.86517204, 0.79168613, 0.8788728 , 0.90860968, 0.84088432,\n",
      "       0.87015413, 0.83605792, 0.90471742, 0.87700452, 0.84104001,\n",
      "       0.86999844, 0.8494473 , 0.86050132, 0.90129223, 0.84757901,\n",
      "       0.83808189, 0.88836992, 0.89911256, 0.91997509, 0.89023821,\n",
      "       0.89179511, 0.89989102, 0.90487311, 0.91452592, 0.90783123,\n",
      "       0.89272925, 0.93383154, 0.84399813, 0.92900514, 0.92075354,\n",
      "       0.90767554, 0.91763973, 0.90751985, 0.84321968, 0.91312471,\n",
      "       0.91047797, 0.91701697, 0.92215476, 0.90487311, 0.90627433,\n",
      "       0.93243033, 0.8919508 , 0.90300483, 0.91499299, 0.91078935,\n",
      "       0.87762728, 0.91872957, 0.91686128, 0.92651409, 0.8969329 ,\n",
      "       0.91125642, 0.90892106, 0.88323213, 0.92168768, 0.89864549,\n",
      "       0.90814261, 0.90160361, 0.87466916, 0.92215476, 0.90565156,\n",
      "       0.90518449, 0.91032228, 0.91717266, 0.90954383, 0.91701697,\n",
      "       0.91125642, 0.9148373 , 0.91763973, 0.91203487, 0.88665733,\n",
      "       0.89895687, 0.92884945, 0.9181068 , 0.91639421, 0.9067414 ,\n",
      "       0.89506461, 0.90238206, 0.89490892, 0.91219057, 0.91763973,\n",
      "       0.90316052, 0.87202242, 0.91405885, 0.91047797, 0.90658571,\n",
      "       0.90814261, 0.90238206, 0.8788728 , 0.9181068 , 0.92013078,\n",
      "       0.90066947, 0.89257356, 0.9132804 , 0.92059785, 0.91234626,\n",
      "       0.91623852, 0.91935233, 0.91530437, 0.91234626, 0.91686128,\n",
      "       0.90316052, 0.90565156, 0.9181068 , 0.91468161, 0.90596295,\n",
      "       0.90985521, 0.89957964, 0.90191499, 0.90565156, 0.9181068 ,\n",
      "       0.90316052, 0.91499299, 0.90938814, 0.91530437, 0.90860968,\n",
      "       0.9115678 , 0.90518449, 0.90471742, 0.91359178, 0.91670559,\n",
      "       0.91296902, 0.91141211, 0.91530437, 0.91686128, 0.91763973,\n",
      "       0.91296902, 0.91748404, 0.91686128, 0.91499299, 0.90876537,\n",
      "       0.91452592, 0.91452592, 0.91359178, 0.91888526, 0.91795111,\n",
      "       0.86595049, 0.92324459, 0.90705278, 0.91203487, 0.90736416,\n",
      "       0.90845399, 0.91872957, 0.90534018, 0.91888526, 0.91312471,\n",
      "       0.92106492, 0.91172349, 0.89413047, 0.91110073, 0.91187918,\n",
      "       0.91250195, 0.91296902, 0.9082983 , 0.91670559, 0.91016659,\n",
      "       0.91732835, 0.91997509, 0.9115678 , 0.91296902, 0.9050288 ,\n",
      "       0.91172349, 0.90783123, 0.91904095, 0.91623852, 0.91732835]),\n",
      " 'split1_test_score': array([0.89165629, 0.85305106, 0.81942715, 0.76650062, 0.79638854,\n",
      "       0.88293898, 0.24221669, 0.80261519, 0.73349938, 0.88231631,\n",
      "       0.86052304, 0.89726027, 0.30323786, 0.875467  , 0.84184309,\n",
      "       0.78580324, 0.85927771, 0.83250311, 0.        , 0.81506849,\n",
      "       0.74346202, 0.90535492, 0.87982565, 0.86176837, 0.54109589,\n",
      "       0.55230386, 0.89663761, 0.88978829, 0.82440847, 0.88293898,\n",
      "       0.91718555, 0.87422167, 0.85678705, 0.8985056 , 0.90348692,\n",
      "       0.90971357, 0.86861768, 0.87297634, 0.88542964, 0.9234122 ,\n",
      "       0.91843088, 0.91158157, 0.8630137 , 0.89601494, 0.89352428,\n",
      "       0.89352428, 0.90161893, 0.90722291, 0.88356164, 0.87110834,\n",
      "       0.90099626, 0.89788294, 0.91220423, 0.89352428, 0.54420922,\n",
      "       0.91033624, 0.9003736 , 0.88854296, 0.90099626, 0.90224159,\n",
      "       0.87795766, 0.89975093, 0.87048568, 0.8985056 , 0.88854296,\n",
      "       0.89165629, 0.86239103, 0.91780822, 0.92403487, 0.90161893,\n",
      "       0.89975093, 0.89103362, 0.93150685, 0.91843088, 0.89041096,\n",
      "       0.89103362, 0.91220423, 0.91407223, 0.87297634, 0.92590286,\n",
      "       0.91158157, 0.88293898, 0.90348692, 0.89726027, 0.875467  ,\n",
      "       0.88169365, 0.90722291, 0.89414695, 0.90784558, 0.90286426,\n",
      "       0.90597758, 0.91780822, 0.88978829, 0.89788294, 0.90286426,\n",
      "       0.9109589 , 0.85678705, 0.88293898, 0.90348692, 0.90410959,\n",
      "       0.90286426, 0.90348692, 0.9252802 , 0.9003736 , 0.89663761,\n",
      "       0.9003736 , 0.91158157, 0.89601494, 0.92029888, 0.9109589 ,\n",
      "       0.90971357, 0.90846824, 0.89726027, 0.91033624, 0.90161893,\n",
      "       0.90224159, 0.9109589 , 0.90099626, 0.91407223, 0.90473225,\n",
      "       0.8860523 , 0.90099626, 0.89476961, 0.90846824, 0.89663761,\n",
      "       0.9003736 , 0.91344956, 0.90909091, 0.9003736 , 0.90597758,\n",
      "       0.9003736 , 0.89601494, 0.88978829, 0.89352428, 0.90099626,\n",
      "       0.90909091, 0.90535492, 0.90473225, 0.91407223, 0.90597758,\n",
      "       0.91158157, 0.90784558, 0.90784558, 0.91220423, 0.91656289,\n",
      "       0.90971357, 0.90909091, 0.90846824, 0.90722291, 0.90660025,\n",
      "       0.92029888, 0.90846824, 0.90473225, 0.89539228, 0.90473225,\n",
      "       0.90161893, 0.90722291, 0.90286426, 0.89788294, 0.90348692,\n",
      "       0.89788294, 0.90473225, 0.89975093, 0.91905355, 0.91531756,\n",
      "       0.8985056 , 0.90722291, 0.91531756, 0.90660025, 0.90971357,\n",
      "       0.9003736 , 0.91033624, 0.91220423, 0.91531756, 0.92465753,\n",
      "       0.90846824, 0.90846824, 0.90660025, 0.91656289, 0.91469489]),\n",
      " 'split1_train_score': array([0.89444185, 0.85536354, 0.82936323, 0.74855986, 0.79620115,\n",
      "       0.89771135, 0.24957185, 0.80242877, 0.73470341, 0.88743578,\n",
      "       0.85863304, 0.89599875, 0.28164409, 0.88120816, 0.86361513,\n",
      "       0.77891951, 0.86454928, 0.83060875, 0.        , 0.81005761,\n",
      "       0.76163786, 0.9100109 , 0.89288494, 0.85987856, 0.52950335,\n",
      "       0.54429394, 0.90035809, 0.88728009, 0.82733925, 0.8886813 ,\n",
      "       0.92246614, 0.88805854, 0.87357932, 0.9082983 , 0.91141211,\n",
      "       0.91187918, 0.87622606, 0.88416628, 0.89662152, 0.92495719,\n",
      "       0.92807099, 0.91701697, 0.86314806, 0.90596295, 0.90549587,\n",
      "       0.89584306, 0.91063366, 0.91437023, 0.89101666, 0.88089678,\n",
      "       0.91032228, 0.91125642, 0.92044216, 0.90316052, 0.55207847,\n",
      "       0.91343609, 0.90954383, 0.89241787, 0.90487311, 0.90985521,\n",
      "       0.88369921, 0.90113654, 0.88198661, 0.90783123, 0.89817842,\n",
      "       0.90362759, 0.86221392, 0.92729254, 0.92900514, 0.91032228,\n",
      "       0.90767554, 0.89677721, 0.93678966, 0.92651409, 0.89163942,\n",
      "       0.89631014, 0.91795111, 0.92417873, 0.8625253 , 0.92713685,\n",
      "       0.91935233, 0.89086097, 0.90643002, 0.90518449, 0.88369921,\n",
      "       0.88836992, 0.92059785, 0.89802273, 0.91234626, 0.91032228,\n",
      "       0.91841818, 0.92168768, 0.90471742, 0.90565156, 0.91514868,\n",
      "       0.9213763 , 0.86890861, 0.89163942, 0.9115678 , 0.91016659,\n",
      "       0.91437023, 0.90876537, 0.92495719, 0.9067414 , 0.89802273,\n",
      "       0.90409466, 0.92075354, 0.90471742, 0.91343609, 0.91748404,\n",
      "       0.91561576, 0.9230889 , 0.90425035, 0.91795111, 0.90331621,\n",
      "       0.90643002, 0.91561576, 0.9100109 , 0.91748404, 0.91281333,\n",
      "       0.88899268, 0.90923245, 0.9017593 , 0.90969952, 0.90565156,\n",
      "       0.9115678 , 0.92417873, 0.91779542, 0.90814261, 0.91078935,\n",
      "       0.90892106, 0.90783123, 0.89335202, 0.89911256, 0.9082983 ,\n",
      "       0.91997509, 0.91795111, 0.91265764, 0.91904095, 0.91468161,\n",
      "       0.91763973, 0.91639421, 0.91078935, 0.91608283, 0.9198194 ,\n",
      "       0.91841818, 0.91374747, 0.91390316, 0.91468161, 0.91421454,\n",
      "       0.92526857, 0.91577145, 0.90814261, 0.90814261, 0.90767554,\n",
      "       0.90938814, 0.90985521, 0.91187918, 0.90331621, 0.90860968,\n",
      "       0.90938814, 0.91343609, 0.90627433, 0.91826249, 0.92075354,\n",
      "       0.90580726, 0.91110073, 0.91935233, 0.91405885, 0.91748404,\n",
      "       0.91047797, 0.91997509, 0.9181068 , 0.91686128, 0.92729254,\n",
      "       0.91577145, 0.91561576, 0.91234626, 0.91857388, 0.91623852]),\n",
      " 'split2_test_score': array([0.84806974, 0.69053549, 0.86986301, 0.88729763, 0.92216687,\n",
      "       0.85491905, 0.71731009, 0.88356164, 0.84682441, 0.8879203 ,\n",
      "       0.73225405, 0.77334994, 0.25280199, 0.71731009, 0.8611457 ,\n",
      "       0.83312578, 0.875467  , 0.8860523 , 0.66127024, 0.93897883,\n",
      "       0.89601494, 0.29078456, 0.82129514, 0.89975093, 0.31569116,\n",
      "       0.85865504, 0.8381071 , 0.9128269 , 0.88916563, 0.85616438,\n",
      "       0.91033624, 0.92963885, 0.90722291, 0.9501868 , 0.90597758,\n",
      "       0.92714819, 0.83001245, 0.88854296, 0.91344956, 0.91843088,\n",
      "       0.91905355, 0.9358655 , 0.88293898, 0.93150685, 0.89975093,\n",
      "       0.91905355, 0.90660025, 0.91843088, 0.85242839, 0.90722291,\n",
      "       0.89476961, 0.92590286, 0.90971357, 0.9003736 , 0.88854296,\n",
      "       0.87920299, 0.90597758, 0.92465753, 0.91594022, 0.88729763,\n",
      "       0.91718555, 0.91158157, 0.90161893, 0.90224159, 0.89414695,\n",
      "       0.90909091, 0.89352428, 0.94271482, 0.91469489, 0.93026152,\n",
      "       0.91780822, 0.91158157, 0.8985056 , 0.91033624, 0.93648817,\n",
      "       0.91220423, 0.91531756, 0.91967621, 0.91220423, 0.90846824,\n",
      "       0.89975093, 0.92590286, 0.91344956, 0.92652553, 0.8630137 ,\n",
      "       0.92590286, 0.91407223, 0.90909091, 0.9234122 , 0.90348692,\n",
      "       0.89103362, 0.91344956, 0.90473225, 0.90660025, 0.91718555,\n",
      "       0.91469489, 0.91407223, 0.91033624, 0.91843088, 0.90473225,\n",
      "       0.91407223, 0.9109589 , 0.93399751, 0.90909091, 0.90784558,\n",
      "       0.91843088, 0.92403487, 0.91469489, 0.89041096, 0.92839352,\n",
      "       0.91718555, 0.90535492, 0.91344956, 0.92029888, 0.873599  ,\n",
      "       0.91718555, 0.92154421, 0.92216687, 0.91905355, 0.92465753,\n",
      "       0.85367372, 0.91033624, 0.92403487, 0.90971357, 0.91344956,\n",
      "       0.91407223, 0.91718555, 0.91967621, 0.91531756, 0.92092154,\n",
      "       0.91967621, 0.91718555, 0.91905355, 0.91220423, 0.92092154,\n",
      "       0.91594022, 0.92029888, 0.92029888, 0.89290162, 0.92777086,\n",
      "       0.91158157, 0.91967621, 0.91967621, 0.92092154, 0.90224159,\n",
      "       0.92465753, 0.92216687, 0.92777086, 0.91469489, 0.91843088,\n",
      "       0.88729763, 0.93462017, 0.90846824, 0.92652553, 0.9252802 ,\n",
      "       0.9234122 , 0.93150685, 0.90286426, 0.90971357, 0.91469489,\n",
      "       0.92154421, 0.91531756, 0.92652553, 0.91905355, 0.91469489,\n",
      "       0.91407223, 0.91967621, 0.92777086, 0.92403487, 0.91469489,\n",
      "       0.91843088, 0.92278954, 0.91843088, 0.9252802 , 0.91718555,\n",
      "       0.92403487, 0.92652553, 0.92278954, 0.92590286, 0.91594022]),\n",
      " 'split2_train_score': array([0.83387825, 0.69624786, 0.87280087, 0.89179511, 0.92153199,\n",
      "       0.85240542, 0.70263117, 0.8805854 , 0.83839327, 0.8871244 ,\n",
      "       0.72707458, 0.77658415, 0.25938035, 0.71119415, 0.85442939,\n",
      "       0.83901604, 0.88074109, 0.88836992, 0.65825938, 0.93569983,\n",
      "       0.88790285, 0.28958431, 0.81628522, 0.8952203 , 0.33317764,\n",
      "       0.84929161, 0.83387825, 0.89942395, 0.88821423, 0.8494473 ,\n",
      "       0.89662152, 0.91904095, 0.9050288 , 0.94379573, 0.90565156,\n",
      "       0.92620271, 0.83403394, 0.88525611, 0.9115678 , 0.92231045,\n",
      "       0.91935233, 0.92682547, 0.87139966, 0.93025066, 0.89895687,\n",
      "       0.91078935, 0.90783123, 0.91779542, 0.84072863, 0.90767554,\n",
      "       0.8821423 , 0.92355597, 0.90253775, 0.90129223, 0.88759147,\n",
      "       0.87980694, 0.9002024 , 0.92480149, 0.91374747, 0.88463335,\n",
      "       0.91094504, 0.89677721, 0.90160361, 0.89319633, 0.89568737,\n",
      "       0.90767554, 0.89631014, 0.93211895, 0.91203487, 0.92293321,\n",
      "       0.91281333, 0.90191499, 0.88572318, 0.90207068, 0.92838238,\n",
      "       0.91265764, 0.91452592, 0.91468161, 0.91312471, 0.90627433,\n",
      "       0.90284914, 0.91732835, 0.91016659, 0.9246458 , 0.86672894,\n",
      "       0.91919664, 0.90907676, 0.9050288 , 0.92013078, 0.90549587,\n",
      "       0.87871711, 0.9050288 , 0.90565156, 0.90892106, 0.91888526,\n",
      "       0.9067414 , 0.91405885, 0.91203487, 0.91265764, 0.90860968,\n",
      "       0.91390316, 0.90643002, 0.9296279 , 0.89724428, 0.8984898 ,\n",
      "       0.91530437, 0.91265764, 0.90907676, 0.8805854 , 0.91499299,\n",
      "       0.9115678 , 0.90253775, 0.90596295, 0.91359178, 0.86641756,\n",
      "       0.91047797, 0.9067414 , 0.92044216, 0.91717266, 0.91857388,\n",
      "       0.85256111, 0.90004671, 0.91670559, 0.89428616, 0.90892106,\n",
      "       0.90316052, 0.91390316, 0.91187918, 0.91032228, 0.91608283,\n",
      "       0.91623852, 0.91374747, 0.9148373 , 0.91016659, 0.91592714,\n",
      "       0.91717266, 0.91468161, 0.91125642, 0.88790285, 0.91872957,\n",
      "       0.90378328, 0.91265764, 0.91405885, 0.91670559, 0.89895687,\n",
      "       0.91763973, 0.91530437, 0.92417873, 0.91110073, 0.91187918,\n",
      "       0.88074109, 0.92713685, 0.89724428, 0.91701697, 0.92090923,\n",
      "       0.91452592, 0.92106492, 0.89101666, 0.90409466, 0.90767554,\n",
      "       0.91296902, 0.90954383, 0.9148373 , 0.91468161, 0.90362759,\n",
      "       0.90611864, 0.9100109 , 0.92122061, 0.9198194 , 0.90658571,\n",
      "       0.91172349, 0.9148373 , 0.91343609, 0.91888526, 0.90985521,\n",
      "       0.91499299, 0.92277752, 0.91717266, 0.91732835, 0.91172349]),\n",
      " 'split3_test_score': array([0.16998755, 0.93088418, 0.87671233, 0.87484433, 0.84122042,\n",
      "       0.87982565, 0.84620174, 0.        , 0.87484433, 0.69302615,\n",
      "       0.87795766, 0.86924035, 0.84557908, 0.69302615, 0.81755915,\n",
      "       0.8611457 , 0.87484433, 0.90660025, 0.90535492, 0.78331258,\n",
      "       0.77272727, 0.873599  , 0.86550436, 0.90473225, 0.17496887,\n",
      "       0.5373599 , 0.90224159, 0.83312578, 0.8985056 , 0.88169365,\n",
      "       0.80448319, 0.91594022, 0.92465753, 0.94645081, 0.92278954,\n",
      "       0.92403487, 0.85491905, 0.9128269 , 0.91718555, 0.91531756,\n",
      "       0.91594022, 0.92777086, 0.83374844, 0.8879203 , 0.89601494,\n",
      "       0.89975093, 0.91531756, 0.90722291, 0.90410959, 0.89352428,\n",
      "       0.91718555, 0.92465753, 0.90099626, 0.9252802 , 0.9003736 ,\n",
      "       0.83250311, 0.90722291, 0.91843088, 0.91780822, 0.90099626,\n",
      "       0.85678705, 0.93960149, 0.91531756, 0.90597758, 0.88418431,\n",
      "       0.88293898, 0.90597758, 0.89788294, 0.91531756, 0.92029888,\n",
      "       0.91220423, 0.91843088, 0.91594022, 0.90784558, 0.90909091,\n",
      "       0.89103362, 0.90286426, 0.88916563, 0.88916563, 0.8985056 ,\n",
      "       0.88916563, 0.88667497, 0.91407223, 0.88916563, 0.90971357,\n",
      "       0.93150685, 0.93150685, 0.91718555, 0.91967621, 0.90660025,\n",
      "       0.89601494, 0.90597758, 0.90348692, 0.90535492, 0.91780822,\n",
      "       0.91220423, 0.87671233, 0.93150685, 0.90597758, 0.92029888,\n",
      "       0.92092154, 0.90535492, 0.87422167, 0.91967621, 0.91656289,\n",
      "       0.91469489, 0.91407223, 0.91843088, 0.93711083, 0.90909091,\n",
      "       0.90909091, 0.91407223, 0.90660025, 0.92154421, 0.92901619,\n",
      "       0.93026152, 0.91967621, 0.9128269 , 0.90473225, 0.92278954,\n",
      "       0.93088418, 0.91718555, 0.9003736 , 0.90909091, 0.90784558,\n",
      "       0.89290162, 0.93212951, 0.91594022, 0.91407223, 0.91594022,\n",
      "       0.90410959, 0.91531756, 0.90286426, 0.90224159, 0.90535492,\n",
      "       0.91780822, 0.91656289, 0.91531756, 0.91905355, 0.90597758,\n",
      "       0.91656289, 0.90971357, 0.92029888, 0.91656289, 0.90971357,\n",
      "       0.9128269 , 0.91594022, 0.90722291, 0.91469489, 0.91344956,\n",
      "       0.89476961, 0.90224159, 0.91531756, 0.90784558, 0.89788294,\n",
      "       0.89726027, 0.89227895, 0.90597758, 0.90909091, 0.90971357,\n",
      "       0.90784558, 0.9109589 , 0.90846824, 0.92216687, 0.9109589 ,\n",
      "       0.90846824, 0.91344956, 0.91656289, 0.92029888, 0.90846824,\n",
      "       0.91905355, 0.91407223, 0.91469489, 0.90971357, 0.90909091,\n",
      "       0.91531756, 0.91344956, 0.92029888, 0.91407223, 0.9128269 ]),\n",
      " 'split3_train_score': array([0.18480461, 0.93025066, 0.8756033 , 0.87264518, 0.84088432,\n",
      "       0.87856142, 0.83496808, 0.        , 0.86361513, 0.68706212,\n",
      "       0.87669313, 0.86532773, 0.84228554, 0.68534953, 0.8117702 ,\n",
      "       0.85084851, 0.8740464 , 0.89708859, 0.90487311, 0.78468006,\n",
      "       0.78141056, 0.87015413, 0.86563911, 0.90284914, 0.16799004,\n",
      "       0.53666511, 0.90798692, 0.82858477, 0.89304064, 0.87575899,\n",
      "       0.81488401, 0.9115678 , 0.92729254, 0.94177176, 0.91546007,\n",
      "       0.92168768, 0.85536354, 0.9115678 , 0.91374747, 0.91047797,\n",
      "       0.91203487, 0.9279153 , 0.84757901, 0.89864549, 0.89864549,\n",
      "       0.90113654, 0.91296902, 0.90798692, 0.89210649, 0.90129223,\n",
      "       0.91514868, 0.91530437, 0.90238206, 0.91623852, 0.90611864,\n",
      "       0.84244123, 0.90207068, 0.91904095, 0.91779542, 0.90300483,\n",
      "       0.86003425, 0.93585552, 0.92013078, 0.90767554, 0.88883699,\n",
      "       0.89101666, 0.90253775, 0.90098085, 0.91997509, 0.91281333,\n",
      "       0.91717266, 0.91872957, 0.91857388, 0.90767554, 0.91203487,\n",
      "       0.89895687, 0.89989102, 0.89304064, 0.89319633, 0.90051378,\n",
      "       0.89864549, 0.89163942, 0.91561576, 0.89506461, 0.9034719 ,\n",
      "       0.92495719, 0.93258602, 0.91935233, 0.91888526, 0.9050288 ,\n",
      "       0.89054959, 0.92044216, 0.90689709, 0.90860968, 0.91187918,\n",
      "       0.91265764, 0.87498054, 0.92480149, 0.90969952, 0.91748404,\n",
      "       0.91468161, 0.90362759, 0.87466916, 0.9198194 , 0.91701697,\n",
      "       0.91110073, 0.91530437, 0.91514868, 0.93865795, 0.90767554,\n",
      "       0.91405885, 0.91577145, 0.90689709, 0.9181068 , 0.93149619,\n",
      "       0.93398723, 0.90969952, 0.91094504, 0.90954383, 0.91888526,\n",
      "       0.93196326, 0.9165499 , 0.89973533, 0.91343609, 0.90860968,\n",
      "       0.89926825, 0.93009497, 0.91219057, 0.91265764, 0.91374747,\n",
      "       0.90113654, 0.91390316, 0.90689709, 0.90144792, 0.90144792,\n",
      "       0.91390316, 0.91546007, 0.91888526, 0.92168768, 0.90534018,\n",
      "       0.90767554, 0.91141211, 0.91888526, 0.91094504, 0.91717266,\n",
      "       0.91234626, 0.9148373 , 0.90736416, 0.91250195, 0.91686128,\n",
      "       0.89506461, 0.90534018, 0.9148373 , 0.90720847, 0.90160361,\n",
      "       0.8984898 , 0.89444185, 0.90767554, 0.90814261, 0.90596295,\n",
      "       0.90985521, 0.90938814, 0.91390316, 0.92542426, 0.91670559,\n",
      "       0.90736416, 0.91717266, 0.91437023, 0.92480149, 0.91296902,\n",
      "       0.91872957, 0.91437023, 0.91623852, 0.90954383, 0.91125642,\n",
      "       0.9148373 , 0.91374747, 0.92153199, 0.91717266, 0.91359178]),\n",
      " 'split4_test_score': array([0.85233645, 0.76510903, 0.89283489, 0.70841121, 0.83738318,\n",
      "       0.8741433 , 0.18629283, 0.87850467, 0.88286604, 0.89657321,\n",
      "       0.89657321, 0.86853583, 0.3576324 , 0.88161994, 0.8623053 ,\n",
      "       0.9152648 , 0.88660436, 0.88286604, 0.        , 0.77133956,\n",
      "       0.39439252, 0.85794393, 0.89595016, 0.89345794, 0.30404984,\n",
      "       0.7788162 , 0.89034268, 0.90342679, 0.78629283, 0.88909657,\n",
      "       0.90716511, 0.88598131, 0.90280374, 0.89158879, 0.91588785,\n",
      "       0.91214953, 0.90778816, 0.89719626, 0.91900312, 0.90218069,\n",
      "       0.90218069, 0.89906542, 0.80685358, 0.8728972 , 0.92647975,\n",
      "       0.89470405, 0.90654206, 0.90841121, 0.87040498, 0.88161994,\n",
      "       0.90404984, 0.90841121, 0.89968847, 0.90716511, 0.91900312,\n",
      "       0.87352025, 0.93021807, 0.91090343, 0.89906542, 0.90031153,\n",
      "       0.89968847, 0.88224299, 0.90404984, 0.89283489, 0.91775701,\n",
      "       0.91838006, 0.85607477, 0.89470405, 0.89470405, 0.92461059,\n",
      "       0.90654206, 0.9165109 , 0.89221184, 0.89470405, 0.89595016,\n",
      "       0.92211838, 0.90342679, 0.91339564, 0.88037383, 0.90841121,\n",
      "       0.91152648, 0.91588785, 0.9152648 , 0.90903427, 0.89345794,\n",
      "       0.91962617, 0.91277259, 0.89657321, 0.90654206, 0.90716511,\n",
      "       0.90218069, 0.89657321, 0.92647975, 0.9046729 , 0.90716511,\n",
      "       0.9046729 , 0.91028037, 0.90280374, 0.91339564, 0.90280374,\n",
      "       0.90155763, 0.91028037, 0.91962617, 0.89595016, 0.90031153,\n",
      "       0.90342679, 0.91214953, 0.91838006, 0.91713396, 0.89781931,\n",
      "       0.90529595, 0.90654206, 0.91401869, 0.90965732, 0.88971963,\n",
      "       0.91464174, 0.90654206, 0.9152648 , 0.91090343, 0.9046729 ,\n",
      "       0.91339564, 0.89844237, 0.905919  , 0.91775701, 0.90404984,\n",
      "       0.90903427, 0.90280374, 0.91339564, 0.90778816, 0.905919  ,\n",
      "       0.9046729 , 0.9046729 , 0.88722741, 0.89657321, 0.905919  ,\n",
      "       0.90342679, 0.91339564, 0.91090343, 0.91962617, 0.89906542,\n",
      "       0.91214953, 0.90903427, 0.91090343, 0.90716511, 0.91152648,\n",
      "       0.90965732, 0.91277259, 0.90778816, 0.9152648 , 0.91214953,\n",
      "       0.88598131, 0.90529595, 0.92461059, 0.91277259, 0.90218069,\n",
      "       0.90716511, 0.9152648 , 0.90093458, 0.90778816, 0.90218069,\n",
      "       0.90716511, 0.91277259, 0.88224299, 0.90903427, 0.91152648,\n",
      "       0.90155763, 0.90529595, 0.90841121, 0.89968847, 0.90841121,\n",
      "       0.90404984, 0.90342679, 0.91152648, 0.905919  , 0.90155763,\n",
      "       0.89906542, 0.90654206, 0.90529595, 0.90965732, 0.905919  ]),\n",
      " 'split4_train_score': array([0.8505604 , 0.75217933, 0.89866127, 0.68259651, 0.82938979,\n",
      "       0.87157534, 0.1738792 , 0.87608966, 0.88169365, 0.88480697,\n",
      "       0.90940224, 0.87671233, 0.34791407, 0.87826899, 0.85787671,\n",
      "       0.92714819, 0.89118929, 0.874066  , 0.        , 0.76883562,\n",
      "       0.40862391, 0.84698007, 0.89134496, 0.88558531, 0.29374222,\n",
      "       0.7887609 , 0.88231631, 0.9142279 , 0.77739726, 0.89118929,\n",
      "       0.9114259 , 0.87624533, 0.89523661, 0.89321295, 0.92356787,\n",
      "       0.90675592, 0.90815691, 0.89881694, 0.91111457, 0.90193026,\n",
      "       0.89975093, 0.90333126, 0.80884184, 0.86690535, 0.92995019,\n",
      "       0.89601494, 0.90986924, 0.9114259 , 0.874533  , 0.88075965,\n",
      "       0.9123599 , 0.91609589, 0.89617061, 0.91485056, 0.92870486,\n",
      "       0.87468867, 0.93321918, 0.92419054, 0.89430262, 0.89414695,\n",
      "       0.89523661, 0.875     , 0.90473225, 0.89290162, 0.91967621,\n",
      "       0.91625156, 0.87375467, 0.89757161, 0.89819427, 0.92247821,\n",
      "       0.91002491, 0.91049191, 0.89414695, 0.89336862, 0.89352428,\n",
      "       0.92107721, 0.91298257, 0.91889788, 0.87998132, 0.89975093,\n",
      "       0.91578456, 0.91874222, 0.91998755, 0.91251557, 0.89336862,\n",
      "       0.93119552, 0.91531756, 0.90535492, 0.90940224, 0.91220423,\n",
      "       0.90099626, 0.90146326, 0.92636986, 0.90613325, 0.9142279 ,\n",
      "       0.90364259, 0.91625156, 0.90239726, 0.92543587, 0.90519925,\n",
      "       0.90909091, 0.91749689, 0.91952055, 0.89694894, 0.90613325,\n",
      "       0.90395392, 0.92232254, 0.92652553, 0.91313823, 0.90348692,\n",
      "       0.91485056, 0.90753425, 0.91687422, 0.91531756, 0.8994396 ,\n",
      "       0.91609589, 0.91671856, 0.91609589, 0.91936488, 0.90691158,\n",
      "       0.91625156, 0.90784558, 0.90846824, 0.92465753, 0.90955791,\n",
      "       0.91625156, 0.90706725, 0.91671856, 0.90675592, 0.91173724,\n",
      "       0.91064757, 0.90971357, 0.88667497, 0.90519925, 0.91469489,\n",
      "       0.91127024, 0.91656289, 0.91671856, 0.92263387, 0.90706725,\n",
      "       0.9114259 , 0.91889788, 0.91843088, 0.91344956, 0.92076588,\n",
      "       0.9128269 , 0.92061021, 0.91220423, 0.92107721, 0.91967621,\n",
      "       0.89072229, 0.89959527, 0.92870486, 0.91827522, 0.90831258,\n",
      "       0.91204857, 0.9142279 , 0.9109589 , 0.91594022, 0.91080324,\n",
      "       0.91360523, 0.92076588, 0.89243462, 0.91718555, 0.92123288,\n",
      "       0.91080324, 0.91531756, 0.91531756, 0.9017746 , 0.91749689,\n",
      "       0.91018057, 0.91049191, 0.91936488, 0.91531756, 0.90379826,\n",
      "       0.90675592, 0.91547323, 0.91671856, 0.91640722, 0.91547323]),\n",
      " 'std_fit_time': array([2.59619507e-03, 1.41145760e-01, 5.11247234e-03, 3.99122539e-01,\n",
      "       3.62454275e-01, 4.03632247e-03, 3.86580092e-04, 1.71091734e-01,\n",
      "       3.63566541e-01, 4.37024522e-01, 3.61699365e-01, 4.20461767e-02,\n",
      "       9.95564454e-04, 2.02628762e-01, 3.57553131e-01, 3.88823823e-01,\n",
      "       3.66901627e-01, 2.65088584e-03, 5.26693699e-04, 1.66381987e-01,\n",
      "       3.97761884e-01, 3.96906164e-01, 3.65699194e-01, 4.44092228e-03,\n",
      "       2.98360733e-03, 1.65858410e-01, 3.92578249e-01, 3.98101103e-01,\n",
      "       3.63216497e-01, 5.17558466e-03, 3.94060493e-03, 1.65063708e-01,\n",
      "       4.01629760e-01, 4.00978103e-01, 3.62169046e-01, 4.61744143e-03,\n",
      "       5.19582342e-03, 1.64941311e-01, 3.89531402e-01, 3.90752621e-01,\n",
      "       1.41917694e-02, 4.69007770e-03, 6.40805601e-03, 1.65335756e-01,\n",
      "       3.92854881e-01, 3.95621888e-01, 3.57911371e-01, 7.07930642e-03,\n",
      "       5.47568817e-03, 2.02499598e-01, 3.92402350e-03, 3.90287374e-01,\n",
      "       3.63254948e-01, 7.51997573e-03, 8.41140004e-03, 1.65486703e-01,\n",
      "       3.84139527e-01, 3.88435717e-01, 3.57794734e-01, 8.03975949e-03,\n",
      "       3.93147458e-03, 1.65283674e-01, 2.42307162e-02, 1.19599022e-02,\n",
      "       3.67862011e-01, 8.38965338e-03, 4.95579237e-03, 1.68274596e-01,\n",
      "       4.02217767e-01, 4.05120909e-01, 3.52440965e-01, 1.48299385e-02,\n",
      "       6.03258213e-03, 1.68200821e-01, 1.80219601e-02, 3.84807904e-01,\n",
      "       3.64294205e-01, 1.18269216e-02, 6.66035354e-03, 1.62435177e-01,\n",
      "       2.01785985e-02, 3.79718348e-01, 3.60362957e-01, 1.27314178e-02,\n",
      "       8.92473819e-03, 1.61590954e-01, 3.82983143e-01, 3.90491911e-01,\n",
      "       3.64365401e-01, 8.87769540e-02, 1.25118645e-02, 1.92809801e-01,\n",
      "       3.54108790e-02, 3.59140146e-02, 2.69101368e-02, 1.88127849e-02,\n",
      "       1.04509735e-02, 1.74742674e-01, 3.19917158e-02, 3.48959272e-02,\n",
      "       3.02081090e-02, 1.29934004e-02, 1.56463111e-02, 1.85965517e-01,\n",
      "       3.81875242e-01, 2.82967528e-02, 2.64054966e-02, 1.32186469e-02,\n",
      "       9.95366644e-03, 1.70936946e-01, 4.07299570e-02, 2.65103134e-02,\n",
      "       2.54903734e-02, 6.75069742e-03, 4.73026039e-03, 1.67272082e-01,\n",
      "       4.52893770e-02, 3.43652289e-02, 9.93153062e-03, 1.39802727e-02,\n",
      "       6.70441369e-03, 1.69548312e-01, 3.81341876e-01, 3.86604992e-01,\n",
      "       3.37760220e-01, 1.07627504e-02, 1.38775191e-02, 1.58637046e-01,\n",
      "       3.95972091e-01, 3.92689062e-01, 1.94485908e-02, 7.96293676e-03,\n",
      "       1.59329995e-02, 1.63030810e-01, 3.97577591e-01, 3.90889903e-01,\n",
      "       3.35597405e-01, 2.35255179e-02, 5.83935885e-03, 1.72983944e-01,\n",
      "       3.86165191e-01, 3.95636335e-01, 3.40728728e-01, 2.99902055e-02,\n",
      "       7.35611771e-03, 1.61190233e-01, 3.97858771e-01, 3.78228726e-01,\n",
      "       7.73566238e-02, 7.12407980e-03, 8.13554097e-03, 1.61918827e-01,\n",
      "       3.93097643e-01, 4.21762998e-01, 3.31263894e-01, 8.46279916e-03,\n",
      "       1.02874307e-02, 1.63262850e-01, 4.14046063e-01, 3.94742411e-01,\n",
      "       3.26549487e-01, 1.44208697e-02, 1.00447348e-02, 1.62596159e-01,\n",
      "       3.81292084e-01, 3.89008052e-01, 3.48566794e-01, 8.17907350e-03,\n",
      "       6.13672668e-03, 1.65723041e-01, 3.94607010e-01, 3.96583085e-01,\n",
      "       3.21666600e-01, 6.47903942e-03, 6.47842697e-03, 1.59382750e-01,\n",
      "       4.04562444e-01, 7.35181947e-02, 3.17639638e-01, 6.91346010e-03]),\n",
      " 'std_score_time': array([7.05267788e-06, 4.89582218e-04, 4.08863566e-01, 1.35295841e-03,\n",
      "       1.50393868e-03, 9.20742727e-04, 4.88908651e-04, 5.08171143e-04,\n",
      "       3.66818684e-01, 5.88357614e-03, 7.62479131e-04, 7.29763951e-03,\n",
      "       3.98614030e-04, 1.47622629e-03, 3.68263616e-01, 4.82791008e-04,\n",
      "       6.30072828e-04, 3.88855630e-04, 4.86370994e-04, 4.87727185e-04,\n",
      "       1.09184009e-03, 1.86559359e-03, 7.43284359e-04, 6.54724587e-04,\n",
      "       4.03902888e-04, 5.16245520e-04, 4.95017389e-04, 1.50305704e-03,\n",
      "       1.33059291e-03, 7.30074150e-04, 3.85253163e-04, 3.88463333e-04,\n",
      "       1.33441719e-03, 2.50568726e-03, 1.64050786e-03, 4.61562276e-04,\n",
      "       2.96406388e-04, 1.64771313e-04, 1.09310225e-03, 1.46500647e-03,\n",
      "       8.96481536e-04, 4.65703044e-04, 4.66574217e-04, 3.08710467e-04,\n",
      "       1.10659688e-03, 1.48439822e-03, 7.45357803e-04, 8.97588841e-04,\n",
      "       4.20347780e-04, 5.65337388e-03, 4.03627728e-01, 1.40100086e-03,\n",
      "       1.10428171e-03, 6.12389645e-04, 3.29516903e-05, 4.13315495e-04,\n",
      "       1.55780365e-03, 1.32219882e-03, 1.08689416e-03, 7.51452977e-04,\n",
      "       4.27758509e-04, 3.77019874e-04, 3.70831319e-01, 3.96557558e-01,\n",
      "       7.45844783e-04, 4.51888665e-04, 4.44788952e-04, 8.89118030e-04,\n",
      "       1.44493665e-03, 1.59457867e-03, 1.22225718e-03, 5.06247737e-04,\n",
      "       3.69194895e-04, 4.47290401e-04, 6.18960170e-04, 4.28046953e-03,\n",
      "       1.18626531e-03, 3.80760893e-04, 4.13919621e-04, 5.70698333e-04,\n",
      "       4.13159677e-01, 1.35414250e-03, 1.20124474e-03, 6.45087020e-04,\n",
      "       4.32530599e-04, 6.09410913e-03, 3.68151795e-01, 1.15250617e-03,\n",
      "       1.32345762e-03, 6.32377991e-04, 4.06992439e-04, 2.94714544e-04,\n",
      "       4.07341175e-01, 4.00601411e-01, 3.68265970e-01, 6.47167695e-04,\n",
      "       4.66555898e-04, 1.10224434e-04, 7.39510123e-04, 4.03934918e-01,\n",
      "       3.75261103e-01, 1.30832567e-03, 3.03348060e-04, 3.82950312e-04,\n",
      "       1.16363691e-03, 3.99220439e-01, 3.69734410e-01, 4.79965164e-04,\n",
      "       3.71451843e-04, 5.18824370e-04, 4.07363772e-01, 4.02055803e-01,\n",
      "       3.65596917e-01, 7.76056950e-04, 4.61835794e-04, 5.60320354e-04,\n",
      "       4.06521771e-01, 4.05030994e-01, 3.69364590e-01, 7.53734482e-04,\n",
      "       1.04465608e-04, 2.34560720e-04, 3.69990713e-01, 2.06022571e-03,\n",
      "       1.36806702e-03, 4.30766296e-04, 3.65452969e-04, 4.57329370e-04,\n",
      "       1.08222373e-03, 1.96563502e-03, 3.63264514e-01, 1.64580440e-04,\n",
      "       3.80293953e-04, 2.30204585e-04, 1.00282111e-03, 1.18469719e-03,\n",
      "       1.83371620e-03, 4.11835025e-04, 3.62756845e-04, 4.15361854e-04,\n",
      "       3.68222782e-01, 1.07166226e-03, 1.07434804e-03, 2.07584799e-03,\n",
      "       3.94950364e-04, 1.58146866e-04, 1.15568960e-03, 2.56848348e-03,\n",
      "       1.71906217e-03, 4.12731872e-04, 4.41059308e-04, 3.95124706e-04,\n",
      "       1.82664819e-03, 2.99855058e-03, 1.67180944e-03, 6.87414729e-04,\n",
      "       2.23016775e-04, 3.72367886e-04, 8.02474531e-04, 2.85402087e-03,\n",
      "       1.59824862e-03, 4.05308594e-04, 3.87156137e-04, 4.02522380e-04,\n",
      "       1.45740000e-03, 2.02595752e-03, 1.08235290e-03, 4.46534345e-04,\n",
      "       3.91829874e-04, 3.22819975e-04, 2.05553360e-03, 2.55452571e-03,\n",
      "       1.86331272e-03, 8.15212796e-04, 4.08900934e-04, 3.87384268e-04,\n",
      "       1.59977495e-03, 1.59127934e-03, 2.07503580e-03, 4.22525957e-04]),\n",
      " 'std_test_score': array([0.31389145, 0.08153791, 0.02615548, 0.07882993, 0.04817905,\n",
      "       0.01071928, 0.25902276, 0.34501831, 0.12854051, 0.07690376,\n",
      "       0.06306951, 0.04198134, 0.21373579, 0.08873123, 0.02075004,\n",
      "       0.04212073, 0.03780835, 0.02443742, 0.4143807 , 0.05969866,\n",
      "       0.17930965, 0.23134361, 0.03046491, 0.01544702, 0.23809661,\n",
      "       0.14552114, 0.02926874, 0.0323075 , 0.04673728, 0.0181797 ,\n",
      "       0.04356496, 0.02035735, 0.02283556, 0.02461497, 0.00927292,\n",
      "       0.01112401, 0.03080547, 0.01357525, 0.01385976, 0.00703265,\n",
      "       0.00786483, 0.0146272 , 0.02633479, 0.02488789, 0.01349691,\n",
      "       0.01065675, 0.00753642, 0.00432505, 0.02018434, 0.01583772,\n",
      "       0.0078847 , 0.01166159, 0.00879825, 0.01062105, 0.14470953,\n",
      "       0.03366974, 0.01096209, 0.01222786, 0.00965739, 0.00989963,\n",
      "       0.02139722, 0.01986512, 0.01790875, 0.01290165, 0.01167992,\n",
      "       0.0136164 , 0.02336862, 0.0190889 , 0.01298106, 0.01318454,\n",
      "       0.0063983 , 0.00991534, 0.01901511, 0.01050924, 0.01592748,\n",
      "       0.01240479, 0.00628212, 0.01124279, 0.01634492, 0.00949194,\n",
      "       0.00969055, 0.0176282 , 0.00634082, 0.01302417, 0.01612663,\n",
      "       0.01753224, 0.01076062, 0.01242   , 0.00673726, 0.00444502,\n",
      "       0.00563703, 0.00721161, 0.01298669, 0.00665205, 0.00803262,\n",
      "       0.00546514, 0.0214951 , 0.01568938, 0.00567037, 0.00639567,\n",
      "       0.00720392, 0.00290873, 0.02337622, 0.01015371, 0.01050127,\n",
      "       0.00678045, 0.00733269, 0.00887072, 0.01499273, 0.01016168,\n",
      "       0.00572958, 0.00799634, 0.00753493, 0.00491562, 0.02054561,\n",
      "       0.00899325, 0.00553905, 0.00803801, 0.00552727, 0.00966015,\n",
      "       0.02753415, 0.00868429, 0.00983338, 0.00374454, 0.00862238,\n",
      "       0.0078475 , 0.00954518, 0.00360583, 0.00628749, 0.00582228,\n",
      "       0.00798725, 0.00766687, 0.0126717 , 0.00747929, 0.00872814,\n",
      "       0.00539909, 0.00544043, 0.00593474, 0.01134552, 0.01099518,\n",
      "       0.00221364, 0.00581546, 0.00515372, 0.00517392, 0.00479896,\n",
      "       0.00553161, 0.00545621, 0.00805459, 0.00417696, 0.00501114,\n",
      "       0.01731935, 0.01336667, 0.00677087, 0.01044897, 0.00993226,\n",
      "       0.00941605, 0.01343815, 0.00396246, 0.00793402, 0.00680776,\n",
      "       0.01167938, 0.00511989, 0.01497652, 0.00450255, 0.00272195,\n",
      "       0.0079792 , 0.00577618, 0.00643174, 0.00892579, 0.00242548,\n",
      "       0.00910029, 0.0085235 , 0.00269588, 0.0065407 , 0.00774932,\n",
      "       0.00887835, 0.00697476, 0.00854882, 0.00545786, 0.00470248]),\n",
      " 'std_train_score': array([0.30606704, 0.08108835, 0.02285357, 0.0877969 , 0.04476033,\n",
      "       0.01550797, 0.2563641 , 0.34546101, 0.12246152, 0.07740935,\n",
      "       0.06749116, 0.04157864, 0.21562193, 0.09095758, 0.02394254,\n",
      "       0.04760499, 0.03545918, 0.02300404, 0.41370773, 0.0591003 ,\n",
      "       0.17366034, 0.23187611, 0.03162846, 0.01492931, 0.23469002,\n",
      "       0.14724405, 0.02873476, 0.03035007, 0.04785472, 0.01873417,\n",
      "       0.04244966, 0.01601636, 0.01727246, 0.01941531, 0.01115918,\n",
      "       0.01208931, 0.02750964, 0.01077136, 0.00657324, 0.00876376,\n",
      "       0.01281201, 0.01069   , 0.0215476 , 0.02323658, 0.01250536,\n",
      "       0.00605826, 0.00335582, 0.00388712, 0.02240913, 0.01352883,\n",
      "       0.01209984, 0.00397581, 0.01052404, 0.00621226, 0.142634  ,\n",
      "       0.03143055, 0.01407203, 0.01283781, 0.00858786, 0.00991742,\n",
      "       0.01706446, 0.02060027, 0.01349692, 0.01234328, 0.01042109,\n",
      "       0.00855166, 0.01779412, 0.01857475, 0.01048704, 0.00895322,\n",
      "       0.00350485, 0.00778778, 0.02262119, 0.01236687, 0.01340953,\n",
      "       0.00908081, 0.00613936, 0.0107387 , 0.01880503, 0.01050453,\n",
      "       0.00776754, 0.01266402, 0.004974  , 0.00970734, 0.01211264,\n",
      "       0.01621863, 0.008603  , 0.00824029, 0.00402018, 0.00281932,\n",
      "       0.01306914, 0.00895612, 0.01025584, 0.00233783, 0.00248494,\n",
      "       0.00682824, 0.02125437, 0.01122029, 0.00582048, 0.00428747,\n",
      "       0.00282015, 0.0053588 , 0.02373545, 0.00980797, 0.00918616,\n",
      "       0.00535856, 0.0106706 , 0.00733236, 0.01879666, 0.00504189,\n",
      "       0.00162339, 0.0075762 , 0.00518217, 0.00229857, 0.02169764,\n",
      "       0.01087273, 0.00452491, 0.00404307, 0.00339722, 0.00550711,\n",
      "       0.02740244, 0.00631981, 0.00623246, 0.00991508, 0.00418994,\n",
      "       0.0062484 , 0.00812164, 0.00316273, 0.00307412, 0.00255207,\n",
      "       0.00492094, 0.00338302, 0.01003321, 0.00536188, 0.00579938,\n",
      "       0.00312071, 0.00219589, 0.00274357, 0.01302   , 0.00548777,\n",
      "       0.00470794, 0.00286307, 0.0030234 , 0.00206558, 0.00823795,\n",
      "       0.00247091, 0.00245547, 0.0054906 , 0.00378062, 0.00276277,\n",
      "       0.01960687, 0.01042525, 0.01040153, 0.0044934 , 0.00634491,\n",
      "       0.00547478, 0.0094313 , 0.00754904, 0.00628015, 0.00249473,\n",
      "       0.00418657, 0.00417409, 0.00950114, 0.00473762, 0.00653917,\n",
      "       0.00266573, 0.00263938, 0.00448365, 0.00770803, 0.00423044,\n",
      "       0.0036093 , 0.00363112, 0.00288809, 0.00322955, 0.00840652,\n",
      "       0.003331  , 0.00477521, 0.00302705, 0.00082971, 0.00199071])}\n"
     ]
    }
   ],
   "source": [
    "# Optimizing for roc_auc\n",
    "bc1 = BaggingClassifier(base_estimator=LogisticRegression(penalty='l2', C=0.2), n_jobs=-1)\n",
    "\n",
    "bc1_params = {'n_estimators':[1,3,5,7,9,11], 'max_samples':[500,700,900,1100,1300],\n",
    "              'max_features':[1,3,5,7,9,11]}\n",
    "\n",
    "bc1_gridsearch = GridSearchCV(bc1, bc1_params, cv=5, verbose=1, scoring='recall')\n",
    "\n",
    "bc1_gridsearch.fit(smote_X1, smote_y1)\n",
    "\n",
    "print(bc1_gridsearch.best_params_)\n",
    "print(bc1_gridsearch.best_score_)\n",
    "pp.pprint(bc1_gridsearch.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversampling the data with SMOTE\n",
    "sm = SMOTE(sampling_strategy='auto', random_state=7)\n",
    "smote_X1, smote_y1 = sm.fit_sample(X1_train, y1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores: [0.91781845 0.92304819 0.92379529 0.92769058 0.92488789]\n",
      "Mean Cross-Validation Score: 0.9234480820676382\n",
      "[[2924  435]\n",
      " [   4   67]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.87      0.93      3359\n",
      "           1       0.13      0.94      0.23        71\n",
      "\n",
      "   micro avg       0.87      0.87      0.87      3430\n",
      "   macro avg       0.57      0.91      0.58      3430\n",
      "weighted avg       0.98      0.87      0.92      3430\n",
      "\n",
      "AUC Score 0.9602916696367548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kengw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "bc1 = BalancedBaggingClassifier(base_estimator=LogisticRegression(penalty='l2', C=0.2),\n",
    "                                 max_features=3, max_samples=500, n_estimators=7, random_state=0)\n",
    "\n",
    "# Fitting the model\n",
    "bc1 = bc1.fit(smote_X1, smote_y1)\n",
    "\n",
    "\n",
    "# Checking cross-validation values\n",
    "print('Cross-Validation Scores:', cross_val_score(bc1, smote_X1, smote_y1, cv=5))\n",
    "print('Mean Cross-Validation Score:', np.mean(cross_val_score(bc1, smote_X1, smote_y1, cv=5)))\n",
    "\n",
    "# Constructing the confusion matrix\n",
    "predictions_bc1 = bc1.predict(X1_test)\n",
    "predictions_proba_bc1 = bc1.predict_proba(X1_test)\n",
    "print(confusion_matrix(y1_test, predictions_bc1))\n",
    "print(classification_report(y1_test, predictions_bc1))\n",
    "print('AUC Score', roc_auc_score(y1_test, predictions_proba_bc1[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
